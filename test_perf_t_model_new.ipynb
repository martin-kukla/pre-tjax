{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a00dc8-db80-4b67-a0e6-98a3e06f1c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_MEM_FRACTION=0.95\n",
      "Vocabulary size: 38561\n",
      "Number of params: 63883425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 10:47:31.892189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result (Array(10.57429, dtype=float32), (Array(7.813416e-05, dtype=float32), Array(0.9998828, dtype=float32)))\n",
      "The end\n"
     ]
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_MEM_FRACTION=0.95\n",
    "\n",
    "import jax\n",
    "from jax import jit\n",
    "from t_model_new import *\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, lax \n",
    "from jax import random\n",
    "\n",
    "LAYERS = 6\n",
    "model_vocab_size = 38561\n",
    "START_TOK = model_vocab_size - 2\n",
    "END_TOK = model_vocab_size - 1\n",
    "EMB_DIM=512\n",
    "FFN_DIM=2048\n",
    "NUM_HEADS = 8\n",
    "params = init_transformer(model_vocab_size, EMB_DIM, LAYERS, NUM_HEADS, FFN_DIM, random.PRNGKey(0))\n",
    "\n",
    "print(f'Vocabulary size: {model_vocab_size}')\n",
    "num_params = sum([jnp.size(p_leaf) for p_leaf in jax.tree_util.tree_leaves(params)])\n",
    "print(f'Number of params: {num_params}')\n",
    "\n",
    "def avg_cross_entropy_loss(y_labels, x_logits): # y_labels: batch_len x seq_len, x_logits: batch_len x seq_len x vocab_size\n",
    "    y_labels_1d = jnp.reshape(y_labels, -1)\n",
    "    x_logits_2d = jnp.reshape(x_logits, (y_labels.size, -1))\n",
    "    elements_loss = log_softmax(x_logits_2d)[(jnp.arange(y_labels.size), y_labels_1d)]\n",
    "    elements_loss = jnp.where(y_labels_1d != 0, elements_loss, jnp.nan) # account for padding tokens\n",
    "    result = -jnp.nanmean(elements_loss) \n",
    "    return result, jnp.count_nonzero(y_labels)\n",
    "    \n",
    "def accuracy(y_labels, x_logits):\n",
    "    return jnp.nanmean(jnp.where(y_labels!=0, y_labels == jnp.argmax(x_logits, axis=-1), jnp.nan))\n",
    "    \n",
    "def loss(params, x, y, key, train):  # inputs: batch_size x seq_len\n",
    "    y_lens = jnp.count_nonzero(y, axis=1)\n",
    "    # It's possible that there are no padding tokens, and we will go out of boundary, hence the use of \"drop\" mode\n",
    "    y = y.at[jnp.arange(y.shape[0]), y_lens].set(END_TOK, mode=\"drop\") \n",
    "    y = y.at[:,0].set(jnp.where(x[:,0]!=END_TOK, x[:,0], 0)) # Account for possible empty sequences (which are used for in-complete batches)\n",
    "    \n",
    "    start_toks = jnp.full((y.shape[0], 1), START_TOK) \n",
    "    shifted_y = jnp.concatenate((start_toks, y[:,:-1]), axis=1) \n",
    "    \n",
    "    # TODO: write it without copying memory? is it possible? \n",
    "    logits = batched_forward(params, x, shifted_y, key, train) \n",
    "    loss_val, tokens_count = avg_cross_entropy_loss(y, logits)\n",
    "    acc = accuracy(y, logits) \n",
    "    #return loss_val, (loss_val, acc, tokens_count/jnp.size(y)) # TODO: this is wrapping, but we could make use of jax.value_and_grad instead\n",
    "    return loss_val, (acc, tokens_count/jnp.size(y)) # TODO: this is wrapping, but we could make use of jax.value_and_grad instead\n",
    "\n",
    "batch_size = 512\n",
    "seq_len = 50\n",
    "test_x  = random.randint(random.PRNGKey(0), (batch_size, seq_len), 0, model_vocab_size)\n",
    "test_y  = random.randint(random.PRNGKey(0), (batch_size, seq_len), 0, model_vocab_size)\n",
    "\n",
    "import time\n",
    "from jax import grad, value_and_grad\n",
    "from functools import partial\n",
    "loss_train = partial(loss, train=True)\n",
    "grad_loss_train = grad(loss_train, has_aux=True)\n",
    "value_and_grad_loss_train = value_and_grad(loss_train, has_aux=True)\n",
    "\n",
    "with jax.profiler.trace(\"/lego/storage/output/\"):\n",
    "    #result = jit(loss_train)(params, test_x, test_y, random.PRNGKey(0)) # No spike at the end\n",
    "    #result = jit(grad_loss_train)(params, test_x, test_y, random.PRNGKey(0))[1] # Spike at the end\n",
    "    result = jit(value_and_grad_loss_train)(params, test_x, test_y, random.PRNGKey(0))[0] # Spike at the end\n",
    "    result = jax.block_until_ready(result)\n",
    "    time.sleep(30)\n",
    "    print(f'result {result}')\n",
    "    time.sleep(30)\n",
    "    print(f'The end')\n",
    "    #print(jit(loss_train)(params, test_x, test_y, random.PRNGKey(0))[0])\n",
    "    #print(len(jit(grad_loss_train)(params, test_x, test_y, random.PRNGKey(0))[0])) # Requires massive amount of memory at the end??\n",
    "\n",
    "#jitted_test_proj_fwd = jit(test_proj_fwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82af98a8-df8b-4c4a-a00d-47d040479f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result 94.72313690185547\n"
     ]
    }
   ],
   "source": [
    "# TODO: Check whether grad gives spike at the end on a toy example\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "from jax import random\n",
    "\n",
    "model_vocab_size = 38561\n",
    "EMB_DIM = 512\n",
    "batch_size = 125 #250\n",
    "seq_len = 100\n",
    "test_x  = random.normal(random.PRNGKey(0), (batch_size*seq_len, EMB_DIM))\n",
    "test_y = random.randint(random.PRNGKey(0), (batch_size*seq_len, ), 0, model_vocab_size)\n",
    "params = random.normal(random.PRNGKey(0), (EMB_DIM, model_vocab_size))\n",
    "\n",
    "def test_matmul(params, x): \n",
    "    return jnp.matmul(x, params)\n",
    "\n",
    "from t_model_new import log_softmax\n",
    "\n",
    "def test_avg_cross_entropy_loss(y_labels_1d, x_logits_2d):\n",
    "    elements_loss = log_softmax(x_logits_2d)[(jnp.arange(y_labels_1d.size), y_labels_1d)]\n",
    "    return -jnp.nanmean(elements_loss) \n",
    "\n",
    "def test_loss(params, x, y):\n",
    "    x_logits = test_matmul(params, x)\n",
    "    return test_avg_cross_entropy_loss(y, x_logits)\n",
    "\n",
    "import time\n",
    "from jax import make_jaxpr\n",
    "from jax import grad, value_and_grad\n",
    "\n",
    "#print(make_jaxpr(test_matmul)(params, test_x))\n",
    "#print(make_jaxpr(test_loss)(params, test_x, test_y))\n",
    "#print(make_jaxpr(grad(test_loss))(params, test_x, test_y))\n",
    "#raise Exception(\"end\")\n",
    "\n",
    "with jax.profiler.trace(\"/lego/storage/output/\"):\n",
    "    #result = jit(test_matmul)(params, test_x) # No spike\n",
    "    #result = jit(test_loss)(params, test_x, test_y) # No spike\n",
    "    result = value_and_grad(test_loss)(params, test_x, test_y)[0] # Run on reduced batch_size\n",
    "    #result = jit(value_and_grad(test_loss))(params, test_x, test_y)[0] # Spike (not as big as full model)\n",
    "    result = jax.block_until_ready(result)\n",
    "    #time.sleep(30)\n",
    "    print(f'result {result}')\n",
    "    #time.sleep(30)\n",
    "    #print(f'The end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61496a0c-2d2f-4939-a2a9-9afde0451bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 38561)\n"
     ]
    }
   ],
   "source": [
    "# TODO: investigate why GEMM takes 2x matrix.size space\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "from jax import random\n",
    "\n",
    "model_vocab_size = 38561\n",
    "EMB_DIM = 512\n",
    "batch_size = 250\n",
    "seq_len = 100\n",
    "test_x  = random.normal(random.PRNGKey(0), (batch_size*seq_len, EMB_DIM))\n",
    "params = random.normal(random.PRNGKey(0), (EMB_DIM, model_vocab_size))\n",
    "\n",
    "def test_matmul(params, x): \n",
    "    return jnp.matmul(x, params)\n",
    "\n",
    "with jax.profiler.trace(\"/lego/storage/output/\"):\n",
    "    print(jit(test_matmul)(params, test_x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9094b441-2d0b-44bb-bdeb-17c6127d4968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
