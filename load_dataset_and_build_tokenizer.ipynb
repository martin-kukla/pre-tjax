{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e79aa66f-b4c8-4de0-ab38-cf73f96c22de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "IterableDatasetDict.map() got an unexpected keyword argument 'num_proc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# LOAD DATASET \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenized_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_wmt14_dataset\n\u001b[0;32m----> 4\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_wmt14_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstreaming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 5.4mln rows\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mnext\u001b[39m(x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/efs/notebooks/mkukla/tmp/tokenized_dataset.py:11\u001b[0m, in \u001b[0;36mload_wmt14_dataset\u001b[0;34m(streaming)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading dataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwmt14\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mde-en\u001b[39m\u001b[38;5;124m\"\u001b[39m, streaming\u001b[38;5;241m=\u001b[39mstreaming, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/efs/notebooks/mkukla/tmp/datasets/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# 5.4mln rows (set cache to permanent space)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtranslation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtranslation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mde\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranslation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "\u001b[0;31mTypeError\u001b[0m: IterableDatasetDict.map() got an unexpected keyword argument 'num_proc'"
     ]
    }
   ],
   "source": [
    "# LOAD DATASET \n",
    "\n",
    "# TODO: reuse tokenized_dataset.py:load_wmt14_load_dataset?\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"wmt14\", \"de-en\", streaming=True) # 5.4mln rows\n",
    "ds = ds.map(lambda item: {'x':item['translation'][\"en\"], 'y':item['translation'][\"de\"]}, remove_columns=[\"translation\"], batched=False)\n",
    "\n",
    "next(x for x in ds[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad1249-05d9-4827-828b-6e06f9d364a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build BPE-ified word vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4508785it [03:48, 19701.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vocab) 3946607\n",
      "Establish 35000 BPE merges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 29272/35000 [9:59:39<2:57:07,  1.86s/it]"
     ]
    }
   ],
   "source": [
    "# BUILD/LOAD (test) TOKENIZER\n",
    "# Load functionality is just for testing (for efficient runs, use load_dataset_and_load_tokenizer.py instead )\n",
    "from tokenizer import *\n",
    "import itertools\n",
    "\n",
    "TOK_DS_SIZE = 1000\n",
    "BPE_NUM_MERGES = 1000\n",
    "\n",
    "#(tokenize, detokenize), state = build_w_tokenizer(itertools.islice(ds[\"train\"], TOK_DS_SIZE))\n",
    "#(tokenize, detokenize), state = build_bpe_tokenizer(itertools.islice(ds[\"train\"], TOK_DS_SIZE), BPE_NUM_MERGES)\n",
    "\n",
    "\n",
    "# building\n",
    "#(tokenize, detokenize), state = build_bpe_tokenizer(itertools.islice(ds[\"train\"], 100), 10) #debugging\n",
    "#(tokenize, detokenize), state = build_bpe_tokenizer(itertools.islice(ds[\"train\"], 100000), 10000)\n",
    "\n",
    "#(tokenize, detokenize), state = build_bpe_tokenizer(ds[\"train\"], 10000) # Runs in under 2hours\n",
    "#save_tokenizer(state, \"bpe_tokenizer_ds_train_all_merges_10k.pickle\") # 13k vocab size\n",
    "\n",
    "(tokenize, detokenize), state = build_bpe_tokenizer(ds[\"train\"], 35000) # XXX: TODO add time\n",
    "save_tokenizer(state, \"bpe_tokenizer_ds_train_all_merges_35k.pickle\") # XXX: TODO add size\n",
    "\n",
    "# loading\n",
    "#(tokenize, detokenize), state = load_bpe_tokenizer(\"bpe_tokenizer_ds_100k_merges_10k_fixed.pickle\")\n",
    "#(tokenize, detokenize), state = load_bpe_tokenizer(\"bpe_tokenizer_ds_train_all_merges_10k.pickle\")\n",
    "\n",
    "for item in itertools.islice(ds[\"train\"], 1):\n",
    "    #print(f'x {item[\"x\"]} tokenized {tokenize([item[\"x\"]])} detokenized {detokenize(tokenize([item[\"x\"]]))}')\n",
    "    x_and_y = [item[\"x\"], item[\"y\"]]\n",
    "    print(f'x_and_y {x_and_y} tokenized {tokenize(x_and_y)} detokenized {detokenize(tokenize(x_and_y))}')\n",
    "\n",
    "vocab_map = state[0][0] # we need this line for t.ipynb notebook\n",
    "print(len(vocab_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd29aeae-cb64-4de1-82a3-3addf463ddea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 226.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x ([3588, 3691, 12118, 3613, 3586], [3667, 4531, 19, 3715, 3588], [3585, 10333, 3578, 3626, 3829])\n",
      "y ([5476, 10343, 3606, 6056, 11623], [3638, 5706, 3567, 3610, 3979], [3869, 3725, 4824, 5435, 6539])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data collator\n",
    "#%load_ext line_profiler\n",
    "from tqdm import tqdm\n",
    "    \n",
    "def get_batched_examples(batch_size, max_len, split=\"train\"):\n",
    "    def encode_batch(batch, max_len):\n",
    "        batch_str = [ x for x, _ in batch] + [ y for _, y in batch]\n",
    "        pad = lambda toks: toks + [0] * (max_len - len(toks))\n",
    "        encoded = [pad(str)[:max_len] for str in tokenize(batch_str)]\n",
    "        return zip(encoded[:int(len(encoded)/2)], encoded[int(len(encoded)/2):])\n",
    "    \n",
    "    batch = []\n",
    "    for item in ds[split]:\n",
    "        batch.append((item['x'],item['y']))\n",
    "        if len(batch) == batch_size:\n",
    "            yield zip(*encode_batch(batch, max_len))\n",
    "            batch = []\n",
    "    if len(batch)>0:\n",
    "        yield zip(*encode_batch(batch))\n",
    "\n",
    "for x, y in tqdm(itertools.islice(get_batched_examples(3, 5), 1)):\n",
    "    print(f'x {x}')\n",
    "    print(f'y {y}')\n",
    "\n",
    "#%lprun -f batched_apply_bpe_merges4 hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f2bab-5354-45df-bbb2-2d71389f4ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
