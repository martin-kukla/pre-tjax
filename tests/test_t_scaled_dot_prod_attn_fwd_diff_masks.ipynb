{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b67f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRITON_INTERPRET\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9b9fa417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repro ready\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "#from model_triton import t_scaled_dot_prod_attn_fwd3 #, t_scaled_dot_prod_attn_fwd3_t\n",
    "\n",
    "#BS, H, N, D = 2, 2, 512, 64\n",
    "#BS, H, N, D = 1, 1, 16, 16\n",
    "#BS, H, N, D = 1, 1, 512, 64\n",
    "#BS, H, N, D = 1, 1, 512, 16 #bug\n",
    "#BS, H, N, D = 1, 1, 128, 16 #not bug\n",
    "#BS, H, N, D = 1, 1, 256, 16 #bug\n",
    "BS, H, N, D = 1, 1, 32, 16 #bug\n",
    "# BS, H, N, D = 1, 1, 16, 16\n",
    "# qkv = torch.randn((BS, H, 3, N, D), device=\"cuda\")\n",
    "qkv = torch.randn((1, H, 3, N, D), device=\"cuda\").expand(BS, H, 3, N, D)\n",
    "mask = torch.tril(torch.ones((N,N), dtype=torch.bool, device=\"cuda\")).unsqueeze(0).repeat(BS, 1, 1)\n",
    "train=False #True\n",
    "p_gen_aux = 42\n",
    "N_RUNS = 1 #10\n",
    "\n",
    "# qkv[:,:,2, :, :] = 1 # set V to 1s, so I can interepet output easily\n",
    "# qkv[:,:,:, :, :] = 1 # set V to 1s, so I can interepet output easily\n",
    "\n",
    "# full mask\n",
    "mask[0, 0:, :]=0 # This mask is not supported \n",
    "\n",
    "import math\n",
    "from model_triton import t_log_softmax_fwd, t_dropout_fwd\n",
    "def t_softmax_attn_fwd(q, k, mask, train, p_gen_aux=None):\n",
    "    D = q.shape[-1]\n",
    "    attn = torch.matmul(q, torch.transpose(k, -2, -1))\n",
    "    attn = attn / math.sqrt(D)\n",
    "    attn = torch.where(torch.unsqueeze(mask,dim=1), attn, torch.full_like(attn, -1e9)) # Note, instead of usign -jnp.inf, which results in NaNs (NIT: probably better to use jax.numpy.finfo)\n",
    "    #print(attn)\n",
    "    sa = torch.exp(t_log_softmax_fwd(attn))\n",
    "    #print(sa)\n",
    "    sa = t_dropout_fwd(sa, train, p_gen_aux)\n",
    "    #print(sa)\n",
    "    return sa\n",
    "\n",
    "def t_scaled_dot_prod_attn_fwd3(qkv, mask, train=True, p_gen_aux=None): # inputs: BS x H x 3 x N x D, mask: BS x N(q) x N(k)\n",
    "    q, k, v = torch.unbind(qkv, dim=2) # BS x H x N x D\n",
    "    softmaxed_attn = t_softmax_attn_fwd(q, k, mask, train, p_gen_aux)\n",
    "    return torch.matmul(softmaxed_attn, v), [softmaxed_attn] # output: BS x H x N x D\n",
    "\n",
    "result, _ = t_scaled_dot_prod_attn_fwd3(qkv, mask, train, p_gen_aux)\n",
    "# print(result)\n",
    "\n",
    "# result2, _ = t_scaled_dot_prod_attn_fwd3_t(qkv, mask, train, p_gen_aux)\n",
    "\n",
    "# assert torch.allclose(result, result2, rtol=5e-2, atol=1e-3), (result.shape, result2.shape, result[:2, :2, -4:, -4:], result2[:2, :2, -4:, -4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8dc450e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "from model_triton import gelu_k, T_DROPOUT_RATE, dropout_k\n",
    "\n",
    "# This will work for moderate size of D: it's tiling along N dimension of Q, and along N dimension of K_T&V.\n",
    "# It doesn't tile along D dimension.\n",
    "# Different program per BS_H item (reshape of BS and H in one dim, and one program per this dim)\n",
    "@triton.jit\n",
    "def t_scaled_dot_prod_attn_fwd3_k(q_ptr, k_t_ptr, v_ptr, mask_ptr, output_ptr, acts0_ptr, acts1_ptr,\n",
    "                q_stride0, q_stride1, q_stride2, k_t_stride0, k_t_stride1, k_t_stride2,\n",
    "                v_stride0, v_stride1, v_stride2, mask_stride0, mask_stride1,\n",
    "                output_stride0, output_stride1, output_stride2, acts0_stride0, acts1_stride0,\n",
    "                train, p_gen_aux,\n",
    "                BS_H, N:tl.constexpr, D,\n",
    "                BLOCK_SIZE_Q_N: tl.constexpr, BLOCK_SIZE_K_T_N: tl.constexpr, BLOCK_SIZE_D: tl.constexpr,\n",
    "                Q_N_BLCKS: tl.constexpr,\n",
    "                #num_stages: tl.constexpr\n",
    "                ):\n",
    "    # Matching PyTorch's fp32 dtype ( see https://github.com/triton-lang/triton/issues/4574)\n",
    "    ASM: tl.constexpr = \"cvt.rna.tf32.f32 $0, $1;\"\n",
    "    \n",
    "    sqrt_D = tl.sqrt(D.to(tl.float32)) # TODO T: extract from this method?\n",
    "    bs_h_start = tl.program_id(0)\n",
    "    bs_h_step = tl.num_programs(0)\n",
    "    for bs_h_pid in tl.range(bs_h_start, BS_H, bs_h_step): #, num_stages):\n",
    "        bs_h_q_ptr = q_ptr + bs_h_pid * q_stride0\n",
    "        bs_h_k_t_ptr = k_t_ptr + bs_h_pid * k_t_stride0    \n",
    "        bs_h_v_ptr = v_ptr + bs_h_pid * v_stride0        \n",
    "        bs_h_output_ptr = output_ptr + bs_h_pid * output_stride0\n",
    "        bs_h_acts0_ptr = acts0_ptr + bs_h_pid * acts0_stride0\n",
    "        bs_h_acts1_ptr = acts1_ptr + bs_h_pid * acts1_stride0     \n",
    "        \n",
    "        for q_n_step in tl.static_range(0, Q_N_BLCKS):          \n",
    "            q_n_offsets = q_n_step * BLOCK_SIZE_Q_N + tl.arange(0, BLOCK_SIZE_Q_N)            \n",
    "            d_offsets = tl.arange(0, BLOCK_SIZE_D)\n",
    "            # TODO T: Do I need modulo n, modulo m operations? \n",
    "            q_n_offsets_mod = q_n_offsets % N\n",
    "            d_offsets_mod = d_offsets %D\n",
    "            \n",
    "            # Load Q blck once\n",
    "            q_blck_ptr = bs_h_q_ptr + q_n_offsets_mod[:,None] * q_stride1 + d_offsets_mod[None, :] * q_stride2\n",
    "            q_blck_mask = (q_n_offsets[:,None] < N) & (d_offsets[None, :] < D)\n",
    "            q_blck = tl.load(q_blck_ptr, mask=q_blck_mask, other=0.0) \n",
    "            \n",
    "            # Use FlashAttention naming: ms - logits' max, ls - logit's sumexp\n",
    "            ms = tl.full((BLOCK_SIZE_Q_N, 1), -1e9, tl.float32)\n",
    "            ls = tl.zeros_like(ms)\n",
    "            output = tl.zeros((BLOCK_SIZE_Q_N, BLOCK_SIZE_D), dtype=tl.float32)\n",
    "            \n",
    "            # ASSUMES CAUSAL MASK FOR NOW\n",
    "            # This is somehow limited suppport for now. I only tested this for BLOCK_SIZE_Q_N = 2x BLOCK_SIZE_K_T_N\n",
    "            #k_t_n_step_end = min(tl.cdiv(N, BLOCK_SIZE_K_T_N), (q_n_step+1) *tl.cdiv(BLOCK_SIZE_Q_N, BLOCK_SIZE_K_T_N))\n",
    "            k_t_n_step_end = tl.cdiv(N, BLOCK_SIZE_K_T_N)\n",
    "            \n",
    "            for k_t_n_step in range(0, k_t_n_step_end):\n",
    "                k_t_n_offsets = k_t_n_step * BLOCK_SIZE_K_T_N + tl.arange(0, BLOCK_SIZE_K_T_N) \n",
    "                k_t_n_offsets_mod = k_t_n_offsets % N\n",
    "                \n",
    "                # Q * K^T\n",
    "                k_blck_ptr = bs_h_k_t_ptr + d_offsets_mod[:,None] * k_t_stride1 + k_t_n_offsets_mod[None, :] * k_t_stride2\n",
    "                k_blck_mask = (d_offsets[:, None] < D) & (k_t_n_offsets[None, :] < N)\n",
    "                k_blck = tl.load(k_blck_ptr, mask=k_blck_mask, other=0.0)\n",
    "                # Matching PyTorch's fp32 dtype ( see https://github.com/triton-lang/triton/issues/4574)\n",
    "                #q_blck = tl.inline_asm_elementwise(ASM, \"=r, r\", [q_blck], dtype=tl.float32, is_pure=True, pack=1)\n",
    "                #k_blck = tl.inline_asm_elementwise(ASM, \"=r, r\", [k_blck], dtype=tl.float32, is_pure=True, pack=1)\n",
    "                acc = tl.dot(q_blck, k_blck)\n",
    "\n",
    "                # /sqrt(D) + Mask + Softmax + Dropout (with keeping updating ms&ls following FlashAttention's paper)\n",
    "                acc = acc / sqrt_D\n",
    "                mask_blck_ptr = mask_ptr + q_n_offsets_mod[:,None] * mask_stride0 + k_t_n_offsets_mod[None, :] * mask_stride1\n",
    "                mask_mask = (q_n_offsets[:,None] <N) & (k_t_n_offsets[None, :]<N)\n",
    "                mask_blck = tl.load(mask_blck_ptr, mask=mask_mask, other= 0.0)\n",
    "#                 print(\"k_t_n_step, mask_blck\", k_t_n_step, mask_blck)\n",
    "                acc = tl.where(mask_blck, acc, -1e9)\n",
    "                blck_ms = tl.max(acc, axis=1, keep_dims=True)\n",
    "#                 print(\"blck_ms\", blck_ms)\n",
    "                n_ms = tl.maximum(ms, blck_ms)\n",
    "#                 print(\"n_ms\", n_ms)\n",
    "                acc = tl.exp(acc - blck_ms)\n",
    "#                 print(\"acc\", acc)\n",
    "                blck_ls = tl.sum(acc, axis=1, keep_dims=True)   \n",
    "#                 print(\"blck_ls\", blck_ls)\n",
    "                n_ls = tl.exp(ms - n_ms) * ls + tl.exp(blck_ms - n_ms) * blck_ls\n",
    "#                 print(\"n_ls\", n_ls)\n",
    "                #print(acc)\n",
    "                # TODO T: confirm that this is different enough seed per row (assumes that D_PID always equals to 0)\n",
    "                acc = dropout_k(acc, train, p_gen_aux+bs_h_pid, q_n_offsets[:,None] + k_t_n_offsets[None, :])\n",
    "#                 print(\"acc\", acc)\n",
    "                #print(acc)\n",
    "                \n",
    "                # * V\n",
    "                v_blck_ptr = bs_h_v_ptr + k_t_n_offsets_mod[:,None] * v_stride1 + d_offsets_mod[None, :] * v_stride2\n",
    "                v_blck_mask = (k_t_n_offsets[:, None] < N) & (d_offsets[None, :]<D)\n",
    "                v_blck = tl.load(v_blck_ptr, mask=v_blck_mask, other=0.0)\n",
    "                #print(\"\\n\")\n",
    "                #print(\"q_n_step, k_t_n_step, v_blck\", q_n_step, k_t_n_step, v_blck)\n",
    "                #v_blck = tl.inline_asm_elementwise(ASM, \"=r, r\", [v_blck], dtype=tl.float32, is_pure=True, pack=1)\n",
    "                \n",
    "                output = 1/n_ls * (ls * tl.exp(ms - n_ms) * output + tl.exp(blck_ms - n_ms) * tl.dot(acc, v_blck))\n",
    "#                 print(\"output\", output)\n",
    "                ms, ls = n_ms, n_ls\n",
    "                \n",
    "            tl.store(bs_h_acts0_ptr + q_n_offsets, ms.reshape(BLOCK_SIZE_Q_N), mask=q_n_offsets<N)\n",
    "            tl.store(bs_h_acts1_ptr + q_n_offsets, ls.reshape(BLOCK_SIZE_Q_N), mask=q_n_offsets<N)  \n",
    "            output_blck_ptr = bs_h_output_ptr + q_n_offsets[:,None] * output_stride1 + d_offsets[None, :] * output_stride2\n",
    "            output_mask = (q_n_offsets[:,None] <N) & (d_offsets[None, :]<D)\n",
    "            tl.store(output_blck_ptr, output, mask=output_mask)\n",
    "    \n",
    "\n",
    "def t_scaled_dot_prod_attn_fwd3_t(qkv:torch.Tensor, mask:torch.Tensor, train=True, p_gen_aux=None):\n",
    "    q, k, v = torch.unbind(qkv, dim=2) # BS x H x N x D\n",
    "    BS, H, N, D = q.shape\n",
    "    \n",
    "    q = q.reshape(BS*H, N, D)\n",
    "    k = k.reshape(BS*H, N, D)\n",
    "    v = v.reshape(BS*H, N, D)\n",
    "    mask = mask[0] # Asumme mask being the same across rows. TODO XXX: make that assumption throughput the code\n",
    "    \n",
    "    output = torch.zeros_like(q)\n",
    "    acts0 = torch.empty((BS*H, N), device=q.device)\n",
    "    acts1 = torch.empty_like(acts0)\n",
    "    \n",
    "    # TODO T: check if some matrices are contiguous?\n",
    "    grid = (min(BS*H, 80),)\n",
    "\n",
    "    # Tuned params given num_warps=8, and BS, H, N, D = 8, 12, 512, 64\n",
    "    num_warps = 8\n",
    "    num_stages = 2 # TODO T: I don't think this helps\n",
    "    BLOCK_SIZE_Q_N = 16 #128\n",
    "    BLOCK_SIZE_K_T_N = 16 #64\n",
    "    BLOCK_SIZE_D = triton.next_power_of_2(D)\n",
    "    \n",
    "    # We enforce causal masking for now, but the assert below cost too much perf\n",
    "    #assert torch.allclose(mask, torch.tril(torch.ones((N, N), device=mask.device, dtype=torch.bool))), \"Assumes causal mask\"\n",
    "    assert BLOCK_SIZE_Q_N >= BLOCK_SIZE_K_T_N, \"The Q block size needs to be bigger/equal than the K block size. Due to the limited support for levarging causal mask\"\n",
    "    \n",
    "    assert N % BLOCK_SIZE_Q_N==0, \"Given the limited support, N has be dividable by the Q block size\"\n",
    "    assert N % BLOCK_SIZE_K_T_N==0, \"Given the limited support, N has be dividable by the K block size\"    \n",
    "\n",
    "#     print(v.shape, v[0,:,:4])\n",
    "    \n",
    "    if not train:\n",
    "        p_gen_aux = 0 # Need to mock some value for triton to compile the kernel without errors\n",
    "    k_t = torch.transpose(k, -2, -1)\n",
    "    t_scaled_dot_prod_attn_fwd3_k[grid](\n",
    "        q, k_t, v, mask, output, acts0, acts1,\n",
    "        q.stride(0), q.stride(1), q.stride(2), k_t.stride(0), k_t.stride(1), k_t.stride(2), \n",
    "        v.stride(0), v.stride(1), v.stride(2),\n",
    "        mask.stride(0), mask.stride(1), output.stride(0), output.stride(1), output.stride(2),\n",
    "        acts0.stride(0), acts1.stride(0),\n",
    "        train, p_gen_aux,\n",
    "        BS*H, N, D,\n",
    "        BLOCK_SIZE_Q_N=BLOCK_SIZE_Q_N, BLOCK_SIZE_K_T_N = BLOCK_SIZE_K_T_N, BLOCK_SIZE_D=BLOCK_SIZE_D,\n",
    "        Q_N_BLCKS = triton.cdiv(N, BLOCK_SIZE_Q_N), # TODO T: Is there a way to do that cdiv inside kernel?\n",
    "        num_warps=num_warps, num_stages=num_stages)\n",
    "    \n",
    "    output = output.reshape(BS, H, N, D)\n",
    "    return output, [acts0.reshape(BS, H, N), acts1.reshape(BS, H, N), output]\n",
    "\n",
    "\n",
    "result2, _ = t_scaled_dot_prod_attn_fwd3_t(qkv, mask, train, p_gen_aux)\n",
    "\n",
    "assert torch.allclose(result, result2, rtol=5e-2, atol=1e-3), (result.shape, result2.shape, result[:2, :2, -4:, -4:], result2[:2, :2, -4:, -4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "22a387b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9723e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03]]]], device='cuda:0')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "03fe180d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9721e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9721e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9721e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9721e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9721e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9721e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9721e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9721e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9721e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9721e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9721e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9721e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03],\n",
       "          [-1.3309e-01,  2.6458e-01,  2.4963e-02, -1.3783e-01,  7.1695e-02,\n",
       "           -2.3369e-01, -1.9722e-04, -3.0952e-01, -3.9327e-02,  1.6426e-01,\n",
       "           -1.4887e-01, -1.8342e-02,  1.4528e-01,  8.2294e-02,  3.3704e-01,\n",
       "           -4.2749e-03]]]], device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d3aed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
