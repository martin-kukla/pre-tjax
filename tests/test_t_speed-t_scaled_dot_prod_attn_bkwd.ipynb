{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58fce75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"TRITON_INTERPRET\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e3f19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JIT total 0.004426002502441406\n",
      "Naive total 0.015705108642578125\n",
      "result torch.Size([1, 96, 512, 64]) torch.Size([1, 96, 512, 64]) torch.Size([1, 96, 512, 64])\n",
      "result torch.Size([1, 96, 512, 64]) tensor([[[[-4.9232e-02,  4.7414e-03, -1.1770e-01,  1.3947e-01],\n",
      "          [-1.7000e-03, -1.9814e-02, -4.7261e-02,  1.7857e-02],\n",
      "          [ 5.8569e-02, -1.4252e-02,  1.2132e-01, -3.2367e-02],\n",
      "          [-3.8414e-02,  4.3345e-02, -7.3621e-02, -2.2131e-02]],\n",
      "\n",
      "         [[ 4.9302e-02,  6.7095e-02, -1.1078e-04,  3.5220e-02],\n",
      "          [ 3.3030e-02,  2.5750e-03, -1.2704e-01, -7.3808e-02],\n",
      "          [ 6.3743e-02,  1.0924e-01,  4.5872e-02, -8.7987e-03],\n",
      "          [ 5.4502e-02, -1.4580e-02, -1.8846e-02, -4.9602e-03]]]],\n",
      "       device='cuda:0')\n",
      "result torch.Size([1, 96, 512, 64]) tensor([[[[-3.2750e-05,  9.7217e-04,  1.2469e-03, -1.1621e-04],\n",
      "          [-2.1688e-03, -9.2686e-04, -2.8752e-03,  1.1310e-03],\n",
      "          [-3.6520e-03,  6.1378e-05,  1.8289e-03,  2.8583e-03],\n",
      "          [-2.8767e-04,  7.4431e-05,  4.4391e-04,  3.6858e-04]],\n",
      "\n",
      "         [[ 1.7765e-03, -1.3949e-03, -2.3074e-03,  1.8880e-03],\n",
      "          [ 7.2168e-04,  2.3478e-02, -8.6419e-03,  1.2350e-02],\n",
      "          [-4.9107e-04,  2.8907e-03,  2.6992e-03,  3.9213e-03],\n",
      "          [-3.3801e-04, -1.1710e-02,  2.8322e-03, -6.4094e-03]]]],\n",
      "       device='cuda:0')\n",
      "result torch.Size([1, 96, 512, 64]) tensor([[[[ 0.0037,  0.0044,  0.0051,  0.0053],\n",
      "          [-0.0030, -0.0020, -0.0013,  0.0066],\n",
      "          [ 0.0005,  0.0041,  0.0056,  0.0004],\n",
      "          [-0.0020,  0.0040,  0.0024,  0.0009]],\n",
      "\n",
      "         [[-0.0011, -0.0007,  0.0030,  0.0006],\n",
      "          [-0.0007,  0.0056, -0.0102,  0.0053],\n",
      "          [ 0.0004,  0.0031,  0.0013,  0.0013],\n",
      "          [-0.0003,  0.0016, -0.0022,  0.0012]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model_triton import t_scaled_dot_prod_attn_fwd, t_scaled_dot_prod_attn_bkwd3\n",
    "\n",
    "#BS, H, N, D = 1, 1, 64, 128\n",
    "#BS, H, N, D = 1, 1, 64, 64\n",
    "\n",
    "BS, H, N, D = 1, 96, 512, 64\n",
    "#BS, H, N, D = 8, 12, 512, 64\n",
    "#BS, H, N, D = 1, 1, 512, 64\n",
    "#BS, H, N, D = 2, 2, 128, 64 # -> out of resources for tiling along Q_N, but not along Q_N and K_T_N\n",
    "#BS, H, N, D = 2, 2, 128, 32\n",
    "#BS, H, N, D = 2, 2, 64, 32\n",
    "#BS, H, N, D = 2, 2, 32, 16\n",
    "#BS, H, N, D = 2, 2, 32, 32\n",
    "#BS, H, N, D = 2, 2, 16, 16\n",
    "\n",
    "dloss_dx = torch.randn((BS, H, N, D), device=\"cuda\")\n",
    "qkv = torch.randn((BS, H, 3, N, D), device=\"cuda\")\n",
    "#acts = [torch.randn((BS, H, N, N), device=\"cuda\")]\n",
    "mask = torch.tril(torch.ones((N,N), dtype=torch.bool, device=\"cuda\")).unsqueeze(0).expand(BS, N, N)\n",
    "#mask = torch.ones((N,N), dtype=torch.bool, device=\"cuda\").unsqueeze(0).expand(BS, N, N)\n",
    "#train=True\n",
    "train=False #True\n",
    "p_gen_aux = 42\n",
    "N_RUNS = 1 #10\n",
    "\n",
    "from model_triton import t_scaled_dot_prod_attn_fwd3\n",
    "old_acts = t_scaled_dot_prod_attn_fwd3(qkv, mask, train, p_gen_aux)[1]\n",
    "\n",
    "def fn_naive(qkv):\n",
    "    return t_scaled_dot_prod_attn_bkwd3(dloss_dx, old_acts, qkv,  mask, train, p_gen_aux)\n",
    "fn_jit = torch.compile(fn_naive)\n",
    "#burn it\n",
    "fn_jit(qkv) \n",
    "#fn_jit(dloss_dx, qkv) \n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "for _ in range(N_RUNS):\n",
    "    result = fn_jit(qkv)\n",
    "    #result = fn_jit(dloss_dx, qkv)\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(f'JIT total', total)\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "for _ in range(N_RUNS):\n",
    "    result = fn_naive(qkv)\n",
    "    #result = fn_naive(dloss_dx, qkv)\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(f'Naive total', total)\n",
    "\n",
    "print(f'result', result[0].shape, result[1].shape, result[2].shape)\n",
    "print('result', result[0].shape, result[0][:2,:2,-4:, -4:])\n",
    "print('result', result[1].shape, result[1][:2,:2,-4:, -4:])\n",
    "#print('result', result[2].shape, result[2][:2,:2,:4, :4])\n",
    "print('result', result[2].shape, result[2][:2,:2,-4:, -4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b65edd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loc(callsite(\"/efs/notebooks/mkukla/pre-tjax/model_triton.py\":1438:7 at \"/tmp/ipykernel_12532/2621160587.py\":102:74)): error: operation scheduled before its operands\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0.005916118621826172\n",
      "result2 torch.Size([1, 96, 512, 64]) tensor([[[[-4.9226e-02,  4.7485e-03, -1.1770e-01,  1.3940e-01],\n",
      "          [-1.6826e-03, -1.9838e-02, -4.7215e-02,  1.7882e-02],\n",
      "          [ 5.8574e-02, -1.4250e-02,  1.2127e-01, -3.2352e-02],\n",
      "          [-3.8434e-02,  4.3308e-02, -7.3618e-02, -2.2128e-02]],\n",
      "\n",
      "         [[ 4.9304e-02,  6.7095e-02, -1.3039e-04,  3.5181e-02],\n",
      "          [ 3.3015e-02,  2.5725e-03, -1.2698e-01, -7.3759e-02],\n",
      "          [ 6.3670e-02,  1.0921e-01,  4.5921e-02, -8.7934e-03],\n",
      "          [ 5.4475e-02, -1.4566e-02, -1.8852e-02, -4.9302e-03]]]],\n",
      "       device='cuda:0')\n",
      "result2 torch.Size([1, 96, 512, 64]) tensor([[[[-3.2711e-05,  9.7113e-04,  1.2450e-03, -1.1634e-04],\n",
      "          [-2.1691e-03, -9.2687e-04, -2.8752e-03,  1.1312e-03],\n",
      "          [-3.6514e-03,  6.1223e-05,  1.8280e-03,  2.8576e-03],\n",
      "          [-2.8752e-04,  7.4393e-05,  4.4367e-04,  3.6839e-04]],\n",
      "\n",
      "         [[ 1.7765e-03, -1.3946e-03, -2.3073e-03,  1.8882e-03],\n",
      "          [ 7.2131e-04,  2.3466e-02, -8.6388e-03,  1.2343e-02],\n",
      "          [-4.9116e-04,  2.8876e-03,  2.6999e-03,  3.9196e-03],\n",
      "          [-3.3783e-04, -1.1704e-02,  2.8307e-03, -6.4059e-03]]]],\n",
      "       device='cuda:0')\n",
      "result2 torch.Size([1, 96, 512, 64]) tensor([[[[ 0.0037,  0.0044,  0.0051,  0.0053],\n",
      "          [-0.0030, -0.0020, -0.0013,  0.0066],\n",
      "          [ 0.0005,  0.0041,  0.0056,  0.0004],\n",
      "          [-0.0020,  0.0040,  0.0024,  0.0009]],\n",
      "\n",
      "         [[-0.0011, -0.0007,  0.0029,  0.0006],\n",
      "          [-0.0007,  0.0057, -0.0102,  0.0053],\n",
      "          [ 0.0004,  0.0031,  0.0013,  0.0013],\n",
      "          [-0.0003,  0.0016, -0.0022,  0.0012]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "from model_triton import gelu_k, T_DROPOUT_RATE, dropout_k, dropout_bkwd2_k\n",
    "\n",
    "# This will work for moderate size of D: it's tiling along N dimension of Q, and along N dimension of K_T.\n",
    "# It doesn't tile along D dimension.\n",
    "# Different program per BS_H item (reshape of BS and H in one dim, and one program per this dim)\n",
    "@triton.jit\n",
    "def t_scaled_dot_prod_attn_bkwd3_k(dloss_dx_ptr, q_ptr, k_t_ptr, v_ptr, output_ptr, mask_ptr, acts0_ptr, acts1_ptr,\n",
    "                dloss_dq_ptr, dloss_dk_ptr, dloss_dv_ptr,\n",
    "                dloss_dx_stride0, dloss_dx_stride1, dloss_dx_stride2,\n",
    "                q_stride0, q_stride1, q_stride2, k_t_stride0, k_t_stride1, k_t_stride2,\n",
    "                v_stride0, v_stride1, v_stride2, output_stride0, output_stride1, output_stride2, \n",
    "                mask_stride0, mask_stride1, acts0_stride0, acts1_stride0,\n",
    "                dloss_dq_stride0, dloss_dq_stride1, dloss_dq_stride2,                                                                      \n",
    "                dloss_dk_stride0, dloss_dk_stride1, dloss_dk_stride2,                                   \n",
    "                dloss_dv_stride0, dloss_dv_stride1, dloss_dv_stride2,\n",
    "                train, p_gen_aux,\n",
    "                BS_H, N, D,\n",
    "                BLOCK_SIZE_Q_N: tl.constexpr, BLOCK_SIZE_K_T_N: tl.constexpr, BLOCK_SIZE_D: tl.constexpr,\n",
    "                num_stages: tl.constexpr\n",
    "                ):\n",
    "    # Matching PyTorch's fp32 dtype ( see https://github.com/triton-lang/triton/issues/4574)\n",
    "    ASM: tl.constexpr = \"cvt.rna.tf32.f32 $0, $1;\"\n",
    "    \n",
    "    sqrt_D = tl.sqrt(D.to(tl.float32)) # TODO T: extract from this method?\n",
    "    bs_h_start = tl.program_id(0)\n",
    "    bs_h_step = tl.num_programs(0)\n",
    "    for bs_h_pid in tl.range(bs_h_start, BS_H, bs_h_step, num_stages):\n",
    "        bs_h_dloss_dx_ptr = dloss_dx_ptr + bs_h_pid * dloss_dx_stride0      \n",
    "        bs_h_q_ptr = q_ptr + bs_h_pid * q_stride0\n",
    "        bs_h_k_t_ptr = k_t_ptr + bs_h_pid * k_t_stride0    \n",
    "        bs_h_v_ptr = v_ptr + bs_h_pid * v_stride0        \n",
    "        bs_h_output_ptr = output_ptr + bs_h_pid * output_stride0        \n",
    "        bs_h_acts0_ptr = acts0_ptr + bs_h_pid * acts0_stride0           \n",
    "        bs_h_acts1_ptr = acts1_ptr + bs_h_pid * acts1_stride0                   \n",
    "        bs_h_dloss_dq_ptr = dloss_dq_ptr + bs_h_pid * dloss_dq_stride0\n",
    "        bs_h_dloss_dk_ptr = dloss_dk_ptr + bs_h_pid * dloss_dk_stride0\n",
    "        bs_h_dloss_dv_ptr = dloss_dv_ptr + bs_h_pid * dloss_dv_stride0\n",
    "\n",
    "        d_offsets = tl.arange(0, BLOCK_SIZE_D)\n",
    "        d_offsets_mod = d_offsets %D\n",
    "        \n",
    "        for k_t_n_step in range(0, tl.cdiv(N, BLOCK_SIZE_K_T_N)):\n",
    "            \n",
    "            # Load K_T and V blcks\n",
    "            k_t_n_offsets = k_t_n_step * BLOCK_SIZE_K_T_N + tl.arange(0, BLOCK_SIZE_K_T_N) \n",
    "            k_t_n_offsets_mod = k_t_n_offsets % N\n",
    "            k_blck_ptr = bs_h_k_t_ptr + d_offsets_mod[:,None] * k_t_stride1 + k_t_n_offsets_mod[None, :] * k_t_stride2\n",
    "            k_blck_mask = (d_offsets[:, None] < D) & (k_t_n_offsets[None, :] < N)\n",
    "            k_blck = tl.load(k_blck_ptr, mask=k_blck_mask, other=0.0) # TODO T: rename to k_t_blck!\n",
    "            k_blck = tl.inline_asm_elementwise(ASM, \"=r, r\", [k_blck], dtype=tl.float32, is_pure=True, pack=1)\n",
    "            v_blck_ptr = bs_h_v_ptr + k_t_n_offsets_mod[:,None] * v_stride1 + d_offsets_mod[None, :] * v_stride2\n",
    "            v_blck_mask = (k_t_n_offsets[:, None] < N) & (d_offsets[None, :]<D)\n",
    "            v_blck = tl.load(v_blck_ptr, mask=v_blck_mask, other=0.0)\n",
    "            v_blck = tl.inline_asm_elementwise(ASM, \"=r, r\", [v_blck], dtype=tl.float32, is_pure=True, pack=1)\n",
    "            \n",
    "            dloss_dk_blck = tl.zeros((BLOCK_SIZE_K_T_N, BLOCK_SIZE_D), dtype=tl.float32)\n",
    "            dloss_dv_blck = tl.zeros((BLOCK_SIZE_K_T_N, BLOCK_SIZE_D), dtype=tl.float32)   \n",
    "            \n",
    "            # ASSUMES CAUSAL MASK FOR NOW \n",
    "            # This is somehow limited suppport for now. I only tested this for\n",
    "            # a) BLOCK_SIZE_K_T_N = BLOCK_SIZE_Q_N and BLOCK_SIZE_K_T_N = 2x BLOCK_SIZE_Q_N\n",
    "            q_n_step_start = max(0, k_t_n_step * tl.cdiv(BLOCK_SIZE_K_T_N,BLOCK_SIZE_Q_N))\n",
    "            \n",
    "            for q_n_step in range(q_n_step_start, tl.cdiv(N, BLOCK_SIZE_Q_N)):          \n",
    "                q_n_offsets = q_n_step * BLOCK_SIZE_Q_N + tl.arange(0, BLOCK_SIZE_Q_N)            \n",
    "                q_n_offsets_mod = q_n_offsets % N # TODO T: Do I need modulo n, modulo m operations? \n",
    "\n",
    "                # Load Q blck\n",
    "                q_blck_ptr = bs_h_q_ptr + q_n_offsets_mod[:,None] * q_stride1 + d_offsets_mod[None, :] * q_stride2\n",
    "                q_blck_mask = (q_n_offsets[:,None] < N) & (d_offsets[None, :] < D)\n",
    "                q_blck = tl.load(q_blck_ptr, mask=q_blck_mask, other=0.0)\n",
    "                q_blck = tl.inline_asm_elementwise(ASM, \"=r, r\", [q_blck], dtype=tl.float32, is_pure=True, pack=1)\n",
    "\n",
    "                # Load softmax logits' max and sumexp\n",
    "                attn_max = tl.load(bs_h_acts0_ptr+ q_n_offsets_mod, mask=q_n_offsets < N, other=-1e9)\n",
    "                attn_max = attn_max.expand_dims(1)\n",
    "                attn_logits_sumexp = tl.load(bs_h_acts1_ptr+ q_n_offsets_mod, mask=q_n_offsets < N, other=0.0)            \n",
    "                attn_logits_sumexp = attn_logits_sumexp.expand_dims(1)\n",
    "\n",
    "                # Precompute row-wise sum of dloss_dx * output\n",
    "                dloss_dx_blck_ptr = bs_h_dloss_dx_ptr + q_n_offsets_mod[:,None] * dloss_dx_stride1 + d_offsets_mod[None, :] * dloss_dx_stride2\n",
    "                dloss_dx_blck_mask = (q_n_offsets[:, None] < N) & (d_offsets[None, :]<D)\n",
    "                input_dloss_dx_blck = tl.load(dloss_dx_blck_ptr, mask=dloss_dx_blck_mask, other=0.0)\n",
    "                input_dloss_dx_blck = tl.inline_asm_elementwise(ASM, \"=r, r\", [input_dloss_dx_blck], dtype=tl.float32, is_pure=True, pack=1)\n",
    "                output_blck_ptr = bs_h_output_ptr + q_n_offsets_mod[:,None] * output_stride1 + d_offsets_mod[None, :] * output_stride2\n",
    "                output_blck_mask = (q_n_offsets[:, None] < N) & (d_offsets[None, :]<D)\n",
    "                output_blck = tl.load(output_blck_ptr, mask=output_blck_mask, other=0.0)\n",
    "                output_blck = tl.inline_asm_elementwise(ASM, \"=r, r\", [output_blck], dtype=tl.float32, is_pure=True, pack=1)\n",
    "                rowise_dloss_dx_output_sum = tl.sum(input_dloss_dx_blck * output_blck, axis=1, keep_dims=True)\n",
    "\n",
    "                # Compute \"Q * K^T / sqrt(D) + Mask + Softmax + Dropout\", and backpropagate\n",
    "                attn = tl.dot(q_blck, k_blck) / sqrt_D\n",
    "                mask_blck_ptr = mask_ptr + q_n_offsets_mod[:,None] * mask_stride0 + k_t_n_offsets_mod[None, :] * mask_stride1\n",
    "                mask_mask = (q_n_offsets[:,None] <N) & (k_t_n_offsets[None, :]<N)\n",
    "                mask_blck = tl.load(mask_blck_ptr, mask=mask_mask, other= 0.0)\n",
    "                attn = tl.where(mask_blck, attn, -1e9)\n",
    "                sa_pre_dropout = tl.exp(attn - attn_max)/attn_logits_sumexp\n",
    "                # TODO T: confirm that this is different enough seed per row (assumes that D_PID always equals to 0)\n",
    "                sa = dropout_k(sa_pre_dropout, train, p_gen_aux+bs_h_pid, q_n_offsets[:,None] + k_t_n_offsets[None, :])\n",
    "\n",
    "                # Propagate back\n",
    "                # dloss_dv=torch.einsum(f'cd, ce -> ed', dloss_dx, sa), dloss_dx is Q_N x D, sa is Q_N x K_T_N\n",
    "                dloss_dv_blck = tl.dot(tl.trans(sa), input_dloss_dx_blck, dloss_dv_blck)\n",
    "                # dloss_dx = torch.einsum(f'cd, ed -> ce', dloss_dx, v)\n",
    "                dloss_dx_blck = tl.dot(input_dloss_dx_blck, tl.trans(v_blck)) \n",
    "                dloss_dx_blck = dloss_dx_blck * sa + rowise_dloss_dx_output_sum * -sa_pre_dropout\n",
    "                # TODO T: For small speedup, replace the above with: (we need an access to zs though)\n",
    "                #dloss_dx_blck = sa_pre_dropout * ( dloss_dx_blck * zs - rowise_dloss_dx_output_sum)\n",
    "                dloss_dx_blck = tl.where(mask_blck, dloss_dx_blck, 0) # Q_N x K_T_N\n",
    "                # dloss_dq = torch.matmul(dloss_dx, k/math.sqrt(D))\n",
    "                dloss_dq_blck = tl.dot(dloss_dx_blck, tl.trans(k_blck)/sqrt_D) # TODO T: rename k_blck into k_t_blck!\n",
    "                dloss_dq_mask = (q_n_offsets[:,None] <N) & (d_offsets[None, :]<D)\n",
    "                dloss_dq_blck_ptr = bs_h_dloss_dq_ptr + q_n_offsets[:,None] * dloss_dq_stride1 + d_offsets[None, :] * dloss_dq_stride2                \n",
    "                tl.atomic_add(dloss_dq_blck_ptr, dloss_dq_blck, mask=dloss_dq_mask)                \n",
    "                # dloss_dk = torch.einsum('abcd, abce->abde', dloss_dx/math.sqrt(D), q)\n",
    "                dloss_dk_blck = tl.dot(tl.trans(dloss_dx_blck)/sqrt_D, q_blck, dloss_dk_blck)\n",
    "            \n",
    "            dloss_dv_blck_ptr = bs_h_dloss_dv_ptr + k_t_n_offsets[:,None] * dloss_dv_stride1 + d_offsets[None, :] * dloss_dv_stride2\n",
    "            dloss_dv_mask = (k_t_n_offsets[:,None] <N) & (d_offsets[None, :]<D)\n",
    "            tl.store(dloss_dv_blck_ptr, dloss_dv_blck, mask=dloss_dv_mask)\n",
    "            \n",
    "            dloss_dk_blck_ptr = bs_h_dloss_dk_ptr + k_t_n_offsets[:,None] * dloss_dk_stride1 + d_offsets[None, :] * dloss_dk_stride2                \n",
    "            dloss_dk_mask = (k_t_n_offsets[:,None] <N) & (d_offsets[None, :]<D)\n",
    "            tl.store(dloss_dk_blck_ptr, dloss_dk_blck, mask=dloss_dk_mask)\n",
    "                \n",
    "\n",
    "def n_t_scaled_dot_prod_attn_bkwd3_t(dloss_dx, acts, qkv:torch.Tensor, mask:torch.Tensor, train=True, p_gen_aux=None):\n",
    "    q, k, v = torch.unbind(qkv, dim=2) # BS x H x N x D\n",
    "    BS, H, N, D = q.shape\n",
    "    \n",
    "    dloss_dx = dloss_dx.reshape(BS*H, N, D)\n",
    "    q = q.reshape(BS*H, N, D)\n",
    "    k = k.reshape(BS*H, N, D)\n",
    "    v = v.reshape(BS*H, N, D)\n",
    "    mask = mask[0] # Asumme mask being the same across rows. TODO XXX: make that assumption throughput the code\n",
    "    acts0, acts1, output = acts\n",
    "    acts0 = acts0.reshape(BS*H, N)\n",
    "    acts1 = acts1.reshape(BS*H, N)    \n",
    "    output = output.reshape(BS*H, N, D)\n",
    "    \n",
    "    dloss_dq = torch.zeros_like(q)\n",
    "    dloss_dk = torch.zeros_like(k)\n",
    "    dloss_dv = torch.zeros_like(v)    \n",
    "    \n",
    "    # TODO T: check if some matrices are contiguous?\n",
    "    grid = (min(BS*H, 80),) # TODO T: We can bump it up to 160?\n",
    "\n",
    "    # Tuned params given num_warps=8, and BS, H, N, D = 8, 12, 512, 64\n",
    "    num_warps = 8\n",
    "    num_stages = 2 # TODO T: I don't think this helps\n",
    "    # TODO T: We have more memory to use, but smaller block_size use sparsity of causal mask better \n",
    "    BLOCK_SIZE_Q_N = 32  \n",
    "    BLOCK_SIZE_K_T_N = 32 \n",
    "    BLOCK_SIZE_D = triton.next_power_of_2(D)\n",
    "    \n",
    "    # We enforce causal masking for now, but the assert below cost too much perf\n",
    "    #assert torch.allclose(mask, torch.tril(torch.ones((N, N), device=mask.device, dtype=torch.bool))), \"Assumes causal mask\"\n",
    "    assert BLOCK_SIZE_K_T_N>= BLOCK_SIZE_Q_N, \"Due to the limited support for levarging causal mask\"\n",
    "\n",
    "    if not train:\n",
    "        p_gen_aux = 0 # Need to mock some value for triton to compile the kernel without errors\n",
    "    k_t = torch.transpose(k, -2, -1)\n",
    "    t_scaled_dot_prod_attn_bkwd3_k[grid](\n",
    "        dloss_dx, q, k_t, v, output, mask, acts0, acts1,\n",
    "        dloss_dq, dloss_dk, dloss_dv,\n",
    "        dloss_dx.stride(0), dloss_dx.stride(1), dloss_dx.stride(2),\n",
    "        q.stride(0), q.stride(1), q.stride(2), k_t.stride(0), k_t.stride(1), k_t.stride(2), \n",
    "        v.stride(0), v.stride(1), v.stride(2),\n",
    "        output.stride(0), output.stride(1), output.stride(2),        \n",
    "        mask.stride(0), mask.stride(1), acts0.stride(0), acts1.stride(0),\n",
    "        dloss_dq.stride(0), dloss_dq.stride(1), dloss_dq.stride(2),        \n",
    "        dloss_dk.stride(0), dloss_dk.stride(1), dloss_dk.stride(2),\n",
    "        dloss_dv.stride(0), dloss_dv.stride(1), dloss_dv.stride(2),\n",
    "        train, p_gen_aux,\n",
    "        BS*H, N, D,\n",
    "        BLOCK_SIZE_Q_N=BLOCK_SIZE_Q_N, BLOCK_SIZE_K_T_N = BLOCK_SIZE_K_T_N, BLOCK_SIZE_D=BLOCK_SIZE_D,\n",
    "        num_warps=num_warps, num_stages=num_stages)\n",
    "    \n",
    "    return dloss_dq.reshape(BS, H, N, D), dloss_dk.reshape(BS, H, N, D), dloss_dv.reshape(BS, H, N, D)\n",
    "\n",
    "from model_triton import n_t_scaled_dot_prod_attn_fwd3_t\n",
    "output, new_acts = n_t_scaled_dot_prod_attn_fwd3_t(qkv, mask, train, p_gen_aux) # note new_acts contains output \n",
    "def fn_t(qkv):\n",
    "    return n_t_scaled_dot_prod_attn_bkwd3_t(dloss_dx, new_acts, qkv, mask, train, p_gen_aux)\n",
    "\n",
    "# Burn it \n",
    "fn_t(qkv)\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "for _ in range(N_RUNS):\n",
    "    result2 = fn_t(qkv)\n",
    "    #result = fn_t(dloss_dx, qkv)\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(f'total', total)\n",
    "print('result2', result2[0].shape, result2[0][:2, :2, -4:, -4:])\n",
    "print('result2', result2[1].shape, result2[1][:2, :2, -4:, -4:])\n",
    "#print('result2', result2[2].shape, result2[2][:2, :2, :4, :4])\n",
    "print('result2', result2[2].shape, result2[2][:2, :2, -4:, -4:])\n",
    "\n",
    "assert torch.allclose(result[0], result2[0], rtol=5e-2, atol=3e-3), (result[0].shape, result2[0].shape, result[0][:2, :2, -4:, -4:], result2[0][:2, :2, -4:, -4:])\n",
    "assert torch.allclose(result[1], result2[1], rtol=5e-2, atol=2e-3), (result[1].shape, result2[1].shape, result[1][:2, :2, -4:, -4:], result2[1][:2, :2, -4:, -4:])\n",
    "assert torch.allclose(result[2], result2[2], rtol=5e-2, atol=2e-3), (result[2].shape, result2[2].shape, result[2][:2, :2, -4:, -4:], result2[2][:2, :2, -4:, -4:])\n",
    "\n",
    "#assert torch.allclose(result[0], result2[0], rtol=5e-2, atol=4e-4), (result[0].shape, result2[0].shape, result[0][:2, :2, -4:, -4:], result2[0][:2, :2, -4:, -4:])\n",
    "#assert torch.allclose(result[2], result2[2], rtol=5e-2, atol=4e-4), (result[2].shape, result2[2].shape, result[2][:2, :2, -4:, -4:], result2[2][:2, :2, -4:, -4:])\n",
    "\n",
    "#assert torch.allclose(result, result2, rtol=1e-1, atol=1e-3), (result.shape, result2.shape, result[:2, :2, :4, :4], result2[:2, :2, :4, :4])\n",
    "#assert torch.allclose(result, result2, rtol=3e-2, atol=4e-4), (result.shape, result2.shape, result[:2, :2, -4:, -4:], result2[:2, :2, -4:, -4:])\n",
    "#assert torch.allclose(result, result2, rtol=1e-2, atol=5e-4), (result.shape, result2.shape, result[:2, :2, -4:, -4:], result2[:2, :2, -4:, -4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "13191235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_scaled_dot_prod_attn_fwd_t:\n",
      "    BS_H    Triton     torch      naive\n",
      "0   32.0  1.456000  0.799280   3.574848\n",
      "1   48.0  1.568416  1.177520   5.275648\n",
      "2   96.0  3.061440  2.303232  10.398016\n",
      "3  128.0  3.239136  3.029792  13.815472\n",
      "4  256.0  6.385632  6.022528  27.465696\n"
     ]
    }
   ],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['BS_H'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[32, 48, 96, 128, 256],\n",
    "        #x_vals=[96],\n",
    "        #x_vals=[128 * 2**i for i in range(0, 6)],\n",
    "        #x_vals=[128 * i for i in range(2, 33)],  # Different possible values for `x_name`.\n",
    "        #x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['triton', 'torch', 'naive'],  # Possible values for `line_arg`.\n",
    "        line_names=['Triton', 'torch', 'naive'],  # Label name for the lines.\n",
    "        styles=[('blue', '-'), ('green', '-'), ('orange', '-')],  # Line styles.\n",
    "        ylabel='ms',  # Label name for the y-axis.\n",
    "        plot_name='t_scaled_dot_prod_attn_fwd_t',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "        # TODO T: Use real M i.e. \n",
    "    ))\n",
    "def benchmark(BS_H, provider):\n",
    "    #BS, H, N, D = 1, BS_H, 32, 16\n",
    "    #BS, H, N, D = 8, 12, 512, 64\n",
    "    BS, H, N, D = 1, BS_H, 512, 64 \n",
    "    dloss_dx = torch.randn((BS, H, N, D), device=\"cuda\")\n",
    "    qkv = torch.randn((BS, H, 3, N, D), device=\"cuda\")  \n",
    "    \n",
    "    old_acts = t_scaled_dot_prod_attn_fwd3(qkv, mask, train, p_gen_aux)[1]\n",
    "    def fn_naive(qkv):\n",
    "        return t_scaled_dot_prod_attn_bkwd3(dloss_dx, old_acts, qkv,  mask, train, p_gen_aux)\n",
    "    fn_jit = torch.compile(fn_naive)\n",
    "    output, new_acts = n_t_scaled_dot_prod_attn_fwd3_t(qkv, mask, train, p_gen_aux) # note new_acts contains output \n",
    "    def fn_t(qkv):\n",
    "        return n_t_scaled_dot_prod_attn_bkwd3_t(dloss_dx, new_acts, qkv, mask, train, p_gen_aux)\n",
    "\n",
    "    \n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_jit(qkv), quantiles=quantiles)\n",
    "        #ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_jit(dloss_dx, x), quantiles=quantiles)\n",
    "        #ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_jit(aa, bb), quantiles=quantiles)\n",
    "        #ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_t(aa, bb), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_t(qkv), quantiles=quantiles)        \n",
    "        #ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_t(dloss_dx, x), quantiles=quantiles)\n",
    "        #ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_t(aa, bb), quantiles=quantiles)\n",
    "    if provider == 'naive':\n",
    "       ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_naive(qkv), quantiles=quantiles)\n",
    "       #ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_naive(dloss_dx, x), quantiles=quantiles)\n",
    "    #perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3) # TODO XXX: investigate whether this is right. In the tutorial they operate on different dtype\n",
    "    perf = lambda ms: ms\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3019c26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22723MB, multi_processor_count=80, uuid=61ea3d2d-53a8-44f6-4844-0bcc29aa720b, L2_cache_size=6MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_shared_mem': 101376,\n",
       " 'max_num_regs': 65536,\n",
       " 'multiprocessor_count': 80,\n",
       " 'warpSize': 32,\n",
       " 'sm_clock_rate': 1710000,\n",
       " 'mem_clock_rate': 6251000,\n",
       " 'mem_bus_width': 384}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_properties(\"cuda\"))\n",
    "from triton.runtime import driver\n",
    "device = torch.cuda.current_device()\n",
    "properties = driver.active.utils.get_device_properties(device)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "WARP_SIZE = properties[\"warpSize\"] # Not 64 as A100\n",
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "898bd4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_stages 2 num_warps 8\n",
      "n_regs 255 size_smem 41088\n",
      "occupancy 1 2\n",
      "num_programs 80\n"
     ]
    }
   ],
   "source": [
    "num_stages = 4 if SIZE_SMEM > 200000 else 2\n",
    "num_warps = 8\n",
    "print(f'num_stages', num_stages, 'num_warps', num_warps)\n",
    "\n",
    "q, k, v = torch.unbind(qkv, dim=2) # BS x H x N x D\n",
    "BS, H, N, D = q.shape\n",
    "\n",
    "dloss_dx = dloss_dx.reshape(BS*H, N, D)\n",
    "q = q.reshape(BS*H, N, D)\n",
    "k = k.reshape(BS*H, N, D)\n",
    "v = v.reshape(BS*H, N, D)\n",
    "temp_mask = mask[0] # Asumme mask being the same across rows. TODO XXX: make that assumption throughput the code\n",
    "acts0, acts1, _ = new_acts\n",
    "acts0 = acts0.reshape(BS*H, N)\n",
    "acts1 = acts1.reshape(BS*H, N)    \n",
    "temp_output = output.reshape(BS*H, N, D)\n",
    "dloss_dq = torch.zeros_like(q)\n",
    "dloss_dk = torch.zeros_like(k)\n",
    "dloss_dv = torch.zeros_like(v)    \n",
    "\n",
    "BLOCK_SIZE_Q_N = 32 #64 #128\n",
    "BLOCK_SIZE_K_T_N = 32 #64\n",
    "BLOCK_SIZE_D = triton.next_power_of_2(D)\n",
    "\n",
    "if not train:\n",
    "    p_gen_aux = 0 # Need to mock some value for triton to compile the kernel without errors\n",
    "k_t = torch.transpose(k, -2, -1)\n",
    "kernel = t_scaled_dot_prod_attn_bkwd3_k.warmup(\n",
    "    dloss_dx, q, k_t, v, temp_output, temp_mask, acts0, acts1,\n",
    "    dloss_dq, dloss_dk, dloss_dv,\n",
    "    dloss_dx.stride(0), dloss_dx.stride(1), dloss_dx.stride(2),\n",
    "    q.stride(0), q.stride(1), q.stride(2), k_t.stride(0), k_t.stride(1), k_t.stride(2), \n",
    "    v.stride(0), v.stride(1), v.stride(2),\n",
    "    temp_output.stride(0), temp_output.stride(1), temp_output.stride(2),        \n",
    "    mask.stride(0), mask.stride(1), acts0.stride(0), acts1.stride(0),\n",
    "    dloss_dq.stride(0), dloss_dq.stride(1), dloss_dq.stride(2),        \n",
    "    dloss_dk.stride(0), dloss_dk.stride(1), dloss_dk.stride(2),\n",
    "    dloss_dv.stride(0), dloss_dv.stride(1), dloss_dv.stride(2),\n",
    "    train, p_gen_aux,\n",
    "    BS*H, N, D,\n",
    "    BLOCK_SIZE_Q_N=BLOCK_SIZE_Q_N, BLOCK_SIZE_K_T_N = BLOCK_SIZE_K_T_N, BLOCK_SIZE_D=BLOCK_SIZE_D,\n",
    "    grid=(1, ), num_warps=num_warps, num_stages=num_stages)\n",
    "\n",
    "kernel._init_handles()\n",
    "n_regs = kernel.n_regs\n",
    "size_smem = kernel.metadata.shared\n",
    "print(f'n_regs', n_regs, 'size_smem', size_smem)\n",
    "\n",
    "occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "print(f'occupancy', occupancy, SIZE_SMEM // size_smem)\n",
    "occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
    "num_programs = NUM_SM * occupancy\n",
    "print(f'num_programs', num_programs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "32\n",
    "2080\n",
    "4128\n",
    "6176"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
