{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a1ec7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_triton import *\n",
    "\n",
    "seq_len = 32\n",
    "vocab_size = 1000 #12000\n",
    "layers = 1 #12\n",
    "emb_dim= 512\n",
    "nheads = 2 #8\n",
    "ffn_dim = 256\n",
    "params = init_transformer_gpt2(vocab_size, emb_dim, layers, nheads, ffn_dim, seq_len)\n",
    "\n",
    "# Print params if needed\n",
    "# for p_grp_ind, p_grp in enumerate(params):\n",
    "#     for p_ind, p in enumerate(p_grp):\n",
    "#         print(p_grp_ind, p_ind, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "51655edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST 1 test_tlayer_attn_head_fwd -> works fine\n",
    "attn_head_params = params[2][2][1]\n",
    "torch.manual_seed(0)\n",
    "q = torch.randn((seq_len, emb_dim), dtype=torch.float32, device=\"cuda\")\n",
    "k = torch.randn((seq_len, emb_dim), dtype=torch.float32, device=\"cuda\")\n",
    "v = torch.randn((seq_len, emb_dim), dtype=torch.float32, device=\"cuda\")\n",
    "y_mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool, device=\"cuda\"))\n",
    "#y_mask = torch.unsqueeze(y_mask,0)\n",
    "\n",
    "res1=tlayer_attn_head_fwd(attn_head_params, (q,k,v), y_mask, False)\n",
    "\n",
    "batched_qkv = tuple([torch.unsqueeze(it,0) for it in (q,k,v)])\n",
    "batched_y_mask = torch.unsqueeze(y_mask,0)\n",
    "res2=t_tlayer_attn_head_fwd(attn_head_params, batched_qkv, batched_y_mask, False)[0]\n",
    "torch.allclose(res1, res2, 1e-04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "60eb574c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST 2 test_tlayer_attn_heads_fwd -> numerical differences due to vmap being used in one of them\n",
    "attn_heads_params = params[2][2]\n",
    "torch.manual_seed(0)\n",
    "q = torch.randn((seq_len, emb_dim), dtype=torch.float32, device=\"cuda\")\n",
    "k = torch.randn((seq_len, emb_dim), dtype=torch.float32, device=\"cuda\")\n",
    "v = torch.randn((seq_len, emb_dim), dtype=torch.float32, device=\"cuda\")\n",
    "y_mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool, device=\"cuda\"))\n",
    "\n",
    "res1=tlayer_attn_heads_fwd(attn_heads_params[:1], (q,k,v), y_mask, False)\n",
    "\n",
    "batched_qkv = tuple([torch.unsqueeze(it,0) for it in (q,k,v)])\n",
    "batched_y_mask = torch.unsqueeze(y_mask,0)\n",
    "res2=t_tlayer_attn_heads_fwd(attn_heads_params, batched_qkv, batched_y_mask, False)[0]\n",
    "torch.allclose(res1, res2, 1e-01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "34820882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST 3 test_tlayer_attn_fwd -> numerical differences, due to vmap being used\n",
    "attn_fwd_params = params[2][2:-6]\n",
    "torch.manual_seed(0)\n",
    "q = torch.randn((seq_len, emb_dim), dtype=torch.float32, device=\"cuda\")\n",
    "k = torch.randn((seq_len, emb_dim), dtype=torch.float32, device=\"cuda\")\n",
    "v = torch.randn((seq_len, emb_dim), dtype=torch.float32, device=\"cuda\")\n",
    "y_mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool, device=\"cuda\"))\n",
    "#y_mask = torch.unsqueeze(y_mask,0)\n",
    "\n",
    "res1=tlayer_attn_fwd(attn_fwd_params, (q,k,v), y_mask, False)\n",
    "\n",
    "batched_qkv = tuple([torch.unsqueeze(it,0) for it in (q,k,v)])\n",
    "batched_y_mask = torch.unsqueeze(y_mask,0)\n",
    "res2=t_tlayer_attn_fwd(attn_fwd_params, batched_qkv, batched_y_mask, False)[0]\n",
    "torch.allclose(res1, res2, 1e-01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbd6fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
