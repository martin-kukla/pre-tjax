{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1981fa3c-b02c-44f4-bc1c-7bedb76ebf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FineWeb-Edu dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c913c462084c23a9bfaeed23be8e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer bpe_tokenizer_fineweb-edu_sample-10BT_100k_ds_merges_30k.pickle\n",
      "Tokenizing dataset\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['x', 'y'],\n",
      "        num_rows: 87048\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['x', 'y'],\n",
      "        num_rows: 9673\n",
      "    })\n",
      "})\n",
      "Loading HellaSwag dataset\n",
      "Tokenizing dataset\n",
      "Dataset({\n",
      "    features: ['x', 'y'],\n",
      "    num_rows: 10042\n",
      "})\n",
      "{'x': [5419, 6016, 22663, 5409, 22663, 5410, 9759, 5404, 5983, 7069, 0, 5419, 5458, 12663, 6818, 16545, 14281, 0, 5419, 11340, 5410, 6740, 5781, 22926, 6247, 10869, 0, 11375, 20711, 5820, 15975, 5411, 5416, 5410, 34963, 0, 3], 'y': [5410, 6259, 5419, 12716, 5416, 5410, 34963, 5676]}\n",
      "['a man is sitting on a roof. he ']\n",
      "['is using wrap to wrap a pair of skis. ', 'is ripping level tiles off. ', \"is holding a rubik's cube. \", 'starts pulling up roofing on a roof. ']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "### DATASETs\n",
    "import datasets\n",
    "from tokenized_dataset import load_tokenized_dataset_gpt2, load_tokenized_dataset_hellaswag, get_batched_examples, get_batched_examples_packed\n",
    "ds, (tokenize, detokenize, tokenizer_vocab_size) = load_tokenized_dataset_gpt2(\"train[:1%]\") #:1% or :1000\n",
    "ds = ds.train_test_split(test_size=0.1, seed=42) # TODO: put seed in better place? does it mess up with resume_from_checkpoint logic?\n",
    "ds = datasets.DatasetDict({\n",
    "    'train': ds['train'],\n",
    "    'validation': ds['test'] #rename\n",
    "})\n",
    "print(ds)\n",
    "\n",
    "hellaswag_ds = load_tokenized_dataset_hellaswag(tokenize)\n",
    "print(hellaswag_ds)\n",
    "\n",
    "# Tests:\n",
    "item = next(x for x in hellaswag_ds)\n",
    "print(item)\n",
    "print(detokenize((item['y'],)))\n",
    "item_x = item['x']\n",
    "def unpack_hellaswag_x(item_x):\n",
    "    choices = []\n",
    "    while 0 in item_x:\n",
    "        ind=item_x.index(0)\n",
    "        choices.append(item_x[:ind])\n",
    "        #print(detokenize((item_x[:ind],))) # choices\n",
    "        item_x = item_x[ind+1:]\n",
    "    assert len(item_x)==1\n",
    "    return choices, item_x[0]\n",
    "choices, label = unpack_hellaswag_x(item['x'])\n",
    "print(detokenize(choices)) # TODO XXX: one of chocies has \", while others have '. Is it anything serious?\n",
    "print(label)\n",
    "\n",
    "# item_x = item['x']\n",
    "# while 0 in item_x:\n",
    "#     ind=item_x.index(0)\n",
    "#     print(detokenize((item_x[:ind],))) # choices\n",
    "#     item_x = item_x[ind+1:]\n",
    "# print(item_x[:ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa54b18-25f2-409c-961b-50f1169af4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5419  6016 22663  5409 22663  5410  9759  5404  5983  7069     0  5419\n",
      "  5458 12663  6818 16545 14281     0  5419 11340  5410  6740  5781 22926\n",
      "  6247 10869     0 11375 20711  5820 15975  5411  5416  5410 34963     0\n",
      "     3     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "[ 6777  5883  5406  9429  5412  6078 25532     0 29852  5391  6392 14325\n",
      " 20598     0 29852  5410 23282 11692  5409  5391  6392 11814  5395     0\n",
      " 11726  5406  5969  5527  5391  7652  5939  6078 14457     0     3     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "[ 5474  6001  7825 21119  6985  6527  5410  8015  5412  5410 13735  6005\n",
      "  5410  9517  6657  8096     0  5474 10906  5391  5945    11  5498  5572\n",
      "  6506  6527  5391  8015 16132  6266  5409 12609     0 12475  5412  5410\n",
      "  5945 20998  6005  5391  6259 21119  9956     0 11788  6506  6527  5391\n",
      " 10133  5477  6005  5391  6259  5412  5688 30811  7255 11891  5406  8756\n",
      "  5551  5404  5945    11     3 30759     0     2     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "[12201  5767  5410 21180     0  9137  5412  5410  5861  5395     0  5969\n",
      "  5527  5688  6440  7339  5391 11668  5404  5410  5807  5518     0 11726\n",
      "  5416  5688  9722  5406  7244  7829     0     2     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "[11589  5688  6440  6294  5416  5391  5861  5395     0  9137  5820  5525\n",
      "  5391  5861  5395     0 10381  5409 16393  5688  6440  5939  5391  5807\n",
      "  5518     0  5463 10962  5420  5551  5404  5391  5861  5395     0     1\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "[11375  7945 13218  5480     0 28973  5509  5531 10910  5406 23787  5411\n",
      "  6118 25532     0  5419 23009  5416  5391  5861  5395     0  6292 16410\n",
      "     8  8444  5416  5688  9519  6571     0     1     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "[10458  5409  7074  5391 11508 11256  5487  5688  6440  6005  7725  5426\n",
      "  5391 34256     0  8244  5409  5496 18443  6005  9799  5391 11508 32088\n",
      "     0 13393  6344  5406 13273  5409  5391  8288  5433  5676  5640  5593\n",
      "     0 14216  5410 24401  5412  8449  5404  5391 34256     0     2     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "[ 9879 19444  5404 13637  5406  7038  5487  5410  5409  5496 34550     0\n",
      "  5474  6001  7825  5412  6926     3 16735 18922  5410 11683  7737  5395\n",
      "     0 12890  5410  6733  5412  5410 28137 13735  5412  5391  8952     0\n",
      "  6693  5412  5391  9463  5442 14794  8566  5406 18576  5518     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m x, y, x_mask, y_mask, yx_mask, x_indices, y_indices \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#print(x)\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m choices, label \u001b[38;5;241m=\u001b[39m \u001b[43munpack_hellaswag_batched_x\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#TODO XXX: finish\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m choices:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#_, (loss_val, acc, toks_prop) = loss_eval(params, jnp.array(x), jnp.array(y), jnp.array(x_mask), jnp.array(y_mask), jnp.array(yx_mask), jnp.array(x_indices), jnp.array(y_indices))\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     loss_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m, in \u001b[0;36munpack_hellaswag_batched_x\u001b[0;34m(batched_x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item_x \u001b[38;5;129;01min\u001b[39;00m batched_x:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(item_x)\n\u001b[0;32m---> 13\u001b[0m     item_choices, item_label \u001b[38;5;241m=\u001b[39m \u001b[43munpack_hellaswag_x\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrim_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem_x\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     choices\u001b[38;5;241m.\u001b[39mappend(item_choices)\n\u001b[1;32m     15\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(item_label)\n",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m, in \u001b[0;36munpack_hellaswag_x\u001b[0;34m(item_x)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m#print(detokenize((item_x[:ind],))) # choices\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     item_x \u001b[38;5;241m=\u001b[39m item_x[ind\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(item_x)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m choices, item_x[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_vocab_size = tokenizer_vocab_size + 3 # add padding token (0) + start of sequence token + end of sequence token \n",
    "START_TOK = tokenizer_vocab_size + 1\n",
    "END_TOK = tokenizer_vocab_size + 2 # TODO: in standard LLM convention, it should be 1. Also, it could be part of tokenizer_vocab_size\n",
    "\n",
    "\n",
    "# Compute HellaSwag score\n",
    "import numpy as np\n",
    "def unpack_hellaswag_batched_x(batched_x: np.ndarray):\n",
    "    choices = []\n",
    "    labels = []\n",
    "    for item_x in batched_x:\n",
    "        print(item_x)\n",
    "        item_choices, item_label = unpack_hellaswag_x(list(np.trim_zeros(item_x)))\n",
    "        choices.append(item_choices)\n",
    "        labels.append(item_label)\n",
    "    return list(map(list, zip(*choices))), labels\n",
    "\n",
    "\n",
    "hellaswag_accs = []\n",
    "for _, batch in enumerate(get_batched_examples(hellaswag_ds, 2, 100, START_TOK, END_TOK, split=None)): \n",
    "    batch_losses = []\n",
    "    x, y, x_mask, y_mask, yx_mask, x_indices, y_indices = batch\n",
    "    #print(x)\n",
    "    choices, label = unpack_hellaswag_batched_x(x) #TODO XXX: finish\n",
    "    for choice in choices:\n",
    "        #_, (loss_val, acc, toks_prop) = loss_eval(params, jnp.array(x), jnp.array(y), jnp.array(x_mask), jnp.array(y_mask), jnp.array(yx_mask), jnp.array(x_indices), jnp.array(y_indices))\n",
    "        loss_val = 0\n",
    "        batch_losses.append(loss_val) # TODO XXX: we need loss per item, or just code up logprob func\n",
    "    # TODO XXX: select min for each item in batch, then add accs to hellaswag_accs\n",
    "       \n",
    "hellaswag_acc = sum(hellaswag_accs)/len(hellaswag_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8556a5a-db80-4087-810a-cdda9df0f2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
