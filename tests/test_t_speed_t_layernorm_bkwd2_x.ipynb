{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8091f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "--------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Torch-Compiled Region        95.16%       1.770ms        98.12%       1.825ms       1.825ms       0.000us         0.00%      78.112us      78.112us             1  \n",
      "    triton_per_fused_mean_mul_reciprocal_std_sub_0         1.48%      27.570us         2.95%      54.950us      54.950us      78.112us       100.00%      78.112us      78.112us             1  \n",
      "                                           triton_         0.00%       0.000us         0.00%       0.000us       0.000us      78.112us       100.00%      78.112us      78.112us             1  \n",
      "                          TorchDynamo Cache Lookup         1.32%      24.641us         1.32%      24.641us      24.641us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                    cuLaunchKernel         1.47%      27.380us         1.47%      27.380us      27.380us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                             cudaDeviceSynchronize         0.56%      10.400us         0.56%      10.400us      10.400us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "--------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.860ms\n",
      "Self CUDA time total: 78.112us\n",
      "\n",
      "JIT total 0.0007872581481933594\n",
      "Naive total 0.04044532775878906\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model_triton import *\n",
    "\n",
    "dloss_dx = torch.randn((8, 512, 768), device=\"cuda\")\n",
    "#Two shapes are being used: [8, 12, 512, 512], and 4096, 35374\n",
    "layer_params = (torch.randn((768), device=\"cuda\"), torch.randn((768), device=\"cuda\"))\n",
    "#aa = torch.randn((2, 768), device=\"cuda\")\n",
    "aa = torch.randn((8, 512, 768), device=\"cuda\")\n",
    "#aa = torch.randn((4096, 35374), device=\"cuda\")\n",
    "#aa = aa.view(-1)\n",
    "N = 1 #100\n",
    "\n",
    "from functools import partial\n",
    "def fn_naive(dloss_dx, x):\n",
    "    return t_layernorm_bkwd2_x(dloss_dx, layer_params, x)\n",
    "fn_jit = torch.compile(fn_naive)\n",
    "# burn it\n",
    "#fn_jit(aa) \n",
    "fn_jit(dloss_dx, aa) \n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity, schedule\n",
    "activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA]\n",
    "with profile(activities=activities, record_shapes=True) as prof:\n",
    "    for _ in range(N):\n",
    "        #result = fn_jit(aa)\n",
    "        result = fn_jit(dloss_dx, aa)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "for _ in range(N):\n",
    "    #result = fn_jit(aa)\n",
    "    result = fn_jit(dloss_dx, aa)\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(f'JIT total', total)\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "for _ in range(N):\n",
    "    #result = fn_naive(aa)\n",
    "    result = fn_naive(dloss_dx, aa)\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(f'Naive total', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a3d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import triton\n",
    "# print(triton.runtime.driver.active.get_current_target())\n",
    "# device = \"cuda\" #triton.runtime.driver.active.get_active_torch_device()\n",
    "# properties = triton.runtime.driver.active.utils.get_device_properties(device)\n",
    "# See https://github.com/triton-lang/triton/issues/5628, and https://github.com/triton-lang/triton/issues/5388\n",
    "# properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43bfe3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "    t_layernorm_bkwd2_x_k         0.00%       0.000us         0.00%       0.000us       0.000us      81.249us       100.00%      81.249us      81.249us             1  \n",
      "            aten::reshape         0.51%      10.500us         1.74%      35.960us      11.987us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "               aten::view         1.23%      25.460us         1.23%      25.460us       8.487us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "         aten::empty_like         0.39%       8.120us        82.31%       1.702ms       1.702ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "      aten::empty_strided        81.92%       1.694ms        81.92%       1.694ms       1.694ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "           cuLaunchKernel        15.53%     321.163us        15.53%     321.163us     321.163us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "    cudaDeviceSynchronize         0.42%       8.650us         0.42%       8.650us       8.650us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.068ms\n",
      "Self CUDA time total: 81.249us\n",
      "\n",
      "total 0.00038886070251464844\n"
     ]
    }
   ],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# Note that the kernel assumes that n_cols < BLOCK_SIZE\n",
    "# TODO T: investigate numerical differences from torch.func implementation\n",
    "@triton.jit\n",
    "def t_layernorm_bkwd2_x_k(dloss_dx_ptr,\n",
    "                    param1_ptr,\n",
    "                    x_ptr,\n",
    "                    output_ptr,\n",
    "                    dloss_dx_stride,\n",
    "                    x_row_stride,\n",
    "                    output_row_stride,\n",
    "                    n_rows,\n",
    "                    n_cols,\n",
    "                    BLOCK_SIZE: tl.constexpr,\n",
    "                    num_stages: tl.constexpr,\n",
    "                    ):\n",
    "    row_start = tl.program_id(0)\n",
    "    row_step = tl.num_programs(0)\n",
    "    \n",
    "    # Load shared params\n",
    "    # TODO T: I think triton will load them once into shared memory -> confirm\n",
    "    offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_cols\n",
    "    param1 = tl.load(param1_ptr + offsets, mask=mask, other=0.0)  \n",
    "        \n",
    "    for row_idx in tl.range(row_start, n_rows, row_step, num_stages):\n",
    "        dloss_dx_row_start_ptr = dloss_dx_ptr + row_idx * dloss_dx_stride\n",
    "        dloss_dx = tl.load(dloss_dx_row_start_ptr + offsets, mask=mask, other=0.0)\n",
    "        dloss_dx = dloss_dx * param1\n",
    "        x_row_start_ptr = x_ptr + row_idx * x_row_stride    \n",
    "        x = tl.load(x_row_start_ptr + offsets, mask=mask, other=0.0)\n",
    "        \n",
    "        # compute mean and std for x\n",
    "        x_sum = tl.sum(x, axis=0)\n",
    "        x_mu = x_sum/ n_cols\n",
    "        x_minus_mu = x - x_mu\n",
    "        x_minus_mu2 = x_minus_mu * x_minus_mu\n",
    "        x_minus_mu2_sum = tl.sum(x_minus_mu2, axis=0)\n",
    "        x_sigma2 = x_minus_mu2_sum / (n_cols-1)\n",
    "        x_sigma = tl.sqrt_rn(x_sigma2)\n",
    "        \n",
    "        # normalize x\n",
    "        x_norm = x_minus_mu/x_sigma    \n",
    "        \n",
    "        # bkwd quantities\n",
    "        dloss_dx_sum = tl.sum(dloss_dx, axis=0)\n",
    "        dloss_dx_mu = dloss_dx_sum/n_cols\n",
    "        dloss_dx_x_norm = dloss_dx * x_norm\n",
    "        dloss_dx_x_norm_sum = tl.sum(dloss_dx_x_norm, axis=0)\n",
    "        dloss_dx_x_norm_mu = dloss_dx_x_norm_sum/n_cols\n",
    "        \n",
    "        n_adj = n_cols/(n_cols-1) # adjust for estimated vs calculated sigma\n",
    "        output = dloss_dx - dloss_dx_mu - x_norm * dloss_dx_x_norm_mu * n_adj\n",
    "        output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "        tl.store(output_row_start_ptr + offsets, output, mask=mask)\n",
    "    \n",
    "def t_layernorm_bkwd2_x_t(dloss_dx:torch.Tensor, layer_params: torch.Tensor, x: torch.Tensor):\n",
    "    # TODO T: without this reshape, this func is 2times faster?\n",
    "    dloss_dx_2d = dloss_dx.reshape((-1, dloss_dx.shape[-1]))\n",
    "    x_2d = x.reshape((-1, x.shape[-1])) \n",
    "    n_rows, n_cols = x_2d.shape\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols) \n",
    "    output = torch.empty_like(x_2d)\n",
    "    # TODO T: The below numbers were tuned for A10 by choosing num_warps=8\n",
    "    num_warps = 8\n",
    "    num_stages = 2\n",
    "    num_programs = min(n_rows, 480) \n",
    "    t_layernorm_bkwd2_x_k[(num_programs,)](dloss_dx_2d, layer_params[0], x_2d, output, \n",
    "                                       dloss_dx_2d.stride(0), x_2d.stride(0), output.stride(0), n_rows, n_cols, \n",
    "                                       BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps, num_stages=num_stages)\n",
    "    return output.reshape(dloss_dx.shape)\n",
    "\n",
    "def fn_t(dloss_dx, x):\n",
    "    return t_layernorm_bkwd2_x_t(dloss_dx, layer_params, x)\n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity, schedule\n",
    "activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA]\n",
    "with profile(activities=activities, record_shapes=True) as prof:\n",
    "    for _ in range(N):\n",
    "        #result = fn_t(aa)\n",
    "        result = fn_t(dloss_dx, aa)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20))\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "for _ in range(N):\n",
    "    #result = fn_t(aa)\n",
    "    result = fn_t(dloss_dx, aa)\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(f'total', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7f8bd08",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "(torch.Size([8, 512, 768]), torch.Size([8, 512, 768]), tensor([[ 0.4920, -0.2179,  2.5760,  ...,  0.0135, -0.4564,  0.2398],\n        [-0.4153, -1.2734,  1.0217,  ...,  0.1981,  0.0496,  0.3957],\n        [ 0.3358,  1.3016, -0.4917,  ..., -0.0966, -0.3950,  0.0786],\n        [-0.0707, -0.4156,  1.0075,  ..., -0.0899,  0.4186, -0.0349]],\n       device='cuda:0'), tensor([[ 0.4896, -0.2168,  2.5637,  ...,  0.0135, -0.4543,  0.2387],\n        [-0.4047, -1.2411,  0.9958,  ...,  0.1931,  0.0484,  0.3856],\n        [ 0.3568,  1.3829, -0.5225,  ..., -0.1027, -0.4197,  0.0835],\n        [-0.0709, -0.4169,  1.0107,  ..., -0.0901,  0.4199, -0.0350]],\n       device='cuda:0'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#res2 = fn_t(aa)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m res2 \u001b[38;5;241m=\u001b[39m fn_t(dloss_dx, aa)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(res1, res2, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), (res1\u001b[38;5;241m.\u001b[39mshape, res2\u001b[38;5;241m.\u001b[39mshape, res1[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m:], res2[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m:])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#assert torch.allclose(res1, res2), (res1[0], res2[0])\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mres1\u001b[39m\u001b[38;5;124m'\u001b[39m, res1\u001b[38;5;241m.\u001b[39mshape, res1[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mAssertionError\u001b[0m: (torch.Size([8, 512, 768]), torch.Size([8, 512, 768]), tensor([[ 0.4920, -0.2179,  2.5760,  ...,  0.0135, -0.4564,  0.2398],\n        [-0.4153, -1.2734,  1.0217,  ...,  0.1981,  0.0496,  0.3957],\n        [ 0.3358,  1.3016, -0.4917,  ..., -0.0966, -0.3950,  0.0786],\n        [-0.0707, -0.4156,  1.0075,  ..., -0.0899,  0.4186, -0.0349]],\n       device='cuda:0'), tensor([[ 0.4896, -0.2168,  2.5637,  ...,  0.0135, -0.4543,  0.2387],\n        [-0.4047, -1.2411,  0.9958,  ...,  0.1931,  0.0484,  0.3856],\n        [ 0.3568,  1.3829, -0.5225,  ..., -0.1027, -0.4197,  0.0835],\n        [-0.0709, -0.4169,  1.0107,  ..., -0.0901,  0.4199, -0.0350]],\n       device='cuda:0'))"
     ]
    }
   ],
   "source": [
    "#res1 = fn_jit(aa)\n",
    "res1 = fn_jit(dloss_dx, aa)\n",
    "#res2 = fn_t(aa)\n",
    "res2 = fn_t(dloss_dx, aa)\n",
    "\n",
    "assert torch.allclose(res1, res2, atol=1e-2, rtol=0), (res1.shape, res2.shape, res1[0, -4:], res2[0, -4:])\n",
    "#assert torch.allclose(res1, res2), (res1[0], res2[0])\n",
    "print(f'res1', res1.shape, res1[0])\n",
    "print(f'res2', res2.shape, res2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d117425c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-5.1964e-01, -9.2317e-01, -4.8219e-01,  ..., -3.9431e-01,\n",
      "          -1.2810e-01,  3.1543e-01],\n",
      "         [-2.0826e+00, -1.2466e+00,  1.8060e+00,  ...,  2.0965e-01,\n",
      "           1.2677e+00,  1.0453e+00],\n",
      "         [-3.2024e+00,  1.2447e+00,  1.2724e+00,  ..., -3.2989e-01,\n",
      "           8.7698e-01, -3.5632e-01],\n",
      "         ...,\n",
      "         [-1.5875e+00, -1.9465e-01,  2.7189e-01,  ..., -3.5884e-01,\n",
      "           1.2794e+00,  2.4107e-01],\n",
      "         [-1.1515e+00, -2.0797e+00,  2.0817e+00,  ..., -2.2421e-02,\n",
      "          -4.2768e-01,  3.8299e-01],\n",
      "         [-2.6354e+00,  1.1654e+00,  2.6262e-01,  ..., -1.1016e-01,\n",
      "          -8.1310e-01,  4.1021e-01]],\n",
      "\n",
      "        [[ 3.9872e-01, -8.9222e-01, -1.9155e+00,  ..., -3.7835e-01,\n",
      "           2.3540e+00, -2.3153e-01],\n",
      "         [-1.4556e+00,  7.9443e-01,  3.9355e-01,  ..., -4.1607e-01,\n",
      "           1.6717e+00,  2.0582e-01],\n",
      "         [-1.3239e+00, -2.3227e-01, -6.1886e-01,  ..., -1.2954e-01,\n",
      "           1.6016e+00,  5.2502e-02],\n",
      "         ...,\n",
      "         [ 1.0826e+00, -5.6339e-01, -1.5369e+00,  ..., -1.7963e-01,\n",
      "           1.7286e+00, -6.6292e-02],\n",
      "         [-1.0139e+00,  1.4124e+00,  1.5957e+00,  ..., -2.6340e-04,\n",
      "           4.6073e-01, -1.7296e-01],\n",
      "         [-9.1509e-01,  2.2050e+00,  2.9081e-01,  ..., -5.8678e-02,\n",
      "           1.9525e+00, -4.3889e-02]],\n",
      "\n",
      "        [[-5.9280e-01, -1.6489e+00,  1.8050e-01,  ...,  6.9466e-02,\n",
      "           2.0537e+00, -1.2142e-02],\n",
      "         [ 1.9024e-01, -1.2353e+00, -5.8371e-01,  ...,  2.9345e-02,\n",
      "           1.6825e+00,  2.4940e-02],\n",
      "         [-2.1626e+00,  1.3855e+00,  2.2411e+00,  ...,  1.7439e-02,\n",
      "           1.6982e+00,  1.9079e-02],\n",
      "         ...,\n",
      "         [-3.7184e-01,  1.1605e+00,  5.6691e-01,  ..., -2.2938e-01,\n",
      "           1.1831e+00,  2.2879e-01],\n",
      "         [-1.3006e+00,  1.3748e+00, -8.0142e-04,  ..., -4.7624e-02,\n",
      "           1.0629e+00,  5.5250e-02],\n",
      "         [-1.6862e+00, -2.0790e+00, -5.9575e-01,  ..., -3.9911e-01,\n",
      "           1.0767e+00,  5.7400e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.1978e+00,  5.9633e-01, -9.6316e-01,  ...,  1.0499e-01,\n",
      "           9.1149e-01,  7.0663e-01],\n",
      "         [-1.0605e+00,  4.5132e-01,  7.3367e-01,  ..., -1.0334e-01,\n",
      "           1.4554e+00,  1.3242e-01],\n",
      "         [-5.6618e-01, -4.6655e-01, -3.5781e-01,  ..., -1.5522e-01,\n",
      "           1.2123e+00, -5.8182e-01],\n",
      "         ...,\n",
      "         [ 3.1847e-01,  9.5571e-01,  1.6446e+00,  ..., -2.2597e-01,\n",
      "           1.5608e+00,  2.4701e-02],\n",
      "         [-7.2110e-01,  2.1302e-01, -4.1080e-01,  ...,  2.3821e-02,\n",
      "           2.6506e+00, -5.6042e-01],\n",
      "         [-1.3198e-01,  2.0806e+00,  1.4550e+00,  ..., -5.0115e-01,\n",
      "          -4.2155e-01, -2.1376e-01]],\n",
      "\n",
      "        [[ 1.1491e+00,  3.8068e-01, -3.0786e-01,  ...,  2.5389e-01,\n",
      "           8.0189e-02,  7.3440e-02],\n",
      "         [-7.3514e-01, -2.7069e-01,  3.2933e-01,  ..., -2.5824e-01,\n",
      "           1.7422e+00, -3.5769e-01],\n",
      "         [-2.8671e+00, -7.4016e-01,  3.3617e-01,  ...,  8.6450e-02,\n",
      "           1.3176e+00, -1.2882e-01],\n",
      "         ...,\n",
      "         [-2.0258e-01, -1.8578e+00, -8.4649e-01,  ..., -3.0241e-01,\n",
      "           2.4815e+00, -3.1732e-02],\n",
      "         [-2.7230e+00, -7.3228e-01, -4.5638e-02,  ..., -1.5108e-02,\n",
      "          -3.3541e-01,  9.6476e-01],\n",
      "         [-2.7584e-01,  8.2007e-01, -3.1928e-02,  ..., -6.9229e-02,\n",
      "           2.5317e+00,  6.1585e-01]],\n",
      "\n",
      "        [[-5.8106e-01,  2.7700e+00, -1.6551e+00,  ..., -1.2204e-01,\n",
      "           2.0283e-01,  2.8292e-01],\n",
      "         [-4.6935e-01, -4.4371e+00, -8.1958e-01,  ..., -5.9045e-01,\n",
      "           1.6127e+00,  2.8425e-01],\n",
      "         [-2.0362e+00,  1.9389e-01,  6.7479e-02,  ..., -1.9795e-01,\n",
      "           6.0383e-02,  1.3522e+00],\n",
      "         ...,\n",
      "         [-1.0532e+00, -2.0739e-01,  6.7400e-01,  ...,  4.8171e-02,\n",
      "           1.9444e+00,  5.2581e-01],\n",
      "         [ 7.5337e-01,  2.1910e+00,  1.8372e-01,  ...,  1.5708e-02,\n",
      "           4.2433e-01, -5.2349e-01],\n",
      "         [-1.3894e+00, -1.0525e+00,  2.6708e+00,  ..., -2.7623e-01,\n",
      "           1.0121e+00,  2.1384e-01]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e45d105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_dropout_fwd:\n",
      "       N     Triton       Torch      naive\n",
      "0  768.0  445.65472  457.050739  55.379937\n"
     ]
    }
   ],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[768], #[128 * i for i in range(2, 100)],  # Different possible values for `x_name`.\n",
    "        #x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['triton', 'torch', 'naive'],  # Possible values for `line_arg`.\n",
    "        line_names=['Triton', 'Torch', 'naive'],  # Label name for the lines.\n",
    "        styles=[('blue', '-'), ('green', '-'), ('orange', '-')],  # Line styles.\n",
    "        ylabel='GB/s',  # Label name for the y-axis.\n",
    "        plot_name='t_dropout_fwd',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={'M':4096},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "        # TODO T: Use real M i.e. \n",
    "    ))\n",
    "def benchmark(M, N, provider):\n",
    "    dloss_dx = torch.rand(M, N, device=\"cuda\", dtype=torch.float32)    \n",
    "    x = torch.rand(M, N, device=\"cuda\", dtype=torch.float32)\n",
    "    stream = getattr(torch, \"cuda\").Stream() # TODO XXX XXX: what is this stream about?\n",
    "    getattr(torch, \"cuda\").set_stream(stream)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        #ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_jit(x), quantiles=quantiles)\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_jit(dloss_dx, x), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        #ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_t(x), quantiles=quantiles)        \n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_t(dloss_dx, x), quantiles=quantiles)\n",
    "    if provider == 'naive':\n",
    "        #ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_naive(x), quantiles=quantiles)\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_naive(dloss_dx, x), quantiles=quantiles)\n",
    "    gbps = lambda ms: 3 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dbdfddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22723MB, multi_processor_count=80, uuid=df82317b-d317-7368-eed6-8c8a4866f43b, L2_cache_size=6MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_shared_mem': 101376,\n",
       " 'max_num_regs': 65536,\n",
       " 'multiprocessor_count': 80,\n",
       " 'warpSize': 32,\n",
       " 'sm_clock_rate': 1710000,\n",
       " 'mem_clock_rate': 6251000,\n",
       " 'mem_bus_width': 384}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_properties(\"cuda\"))\n",
    "from triton.runtime import driver\n",
    "device = torch.cuda.current_device()\n",
    "properties = driver.active.utils.get_device_properties(device)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "WARP_SIZE = properties[\"warpSize\"] # Not 64 as A100\n",
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb92e5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_stages 2 num_warps 8\n",
      "n_regs 40 size_smem 8224\n",
      "occupancy 6 12\n",
      "num_programs 480\n"
     ]
    }
   ],
   "source": [
    "num_stages = 4 if SIZE_SMEM > 200000 else 2\n",
    "num_warps = 8\n",
    "dloss_dx_2d = dloss_dx.reshape((-1, dloss_dx.shape[-1]))\n",
    "x_2d = aa.reshape((-1, aa.shape[-1])) # TODO T: without this reshape, this func is 2times faster\n",
    "n_rows, n_cols = x_2d.shape\n",
    "BLOCK_SIZE = triton.next_power_of_2(n_cols) \n",
    "output = torch.empty_like(x_2d)\n",
    "print(f'num_stages', num_stages, 'num_warps', num_warps)\n",
    "\n",
    "kernel = t_layernorm_bkwd2_x_k.warmup(dloss_dx_2d, layer_params[0], x_2d, output, \n",
    "                                        dloss_dx_2d.stride(0), x_2d.stride(0), output.stride(0), \n",
    "                                        n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,\n",
    "                                        num_stages=num_stages, num_warps=num_warps, grid=(1, ))\n",
    "kernel._init_handles()\n",
    "n_regs = kernel.n_regs\n",
    "size_smem = kernel.metadata.shared\n",
    "print(f'n_regs', n_regs, 'size_smem', size_smem)\n",
    "\n",
    "occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "size_smem = max(1, size_smem) # accounts for divisiion by 0 below. size_smem=0 is possible\n",
    "print(f'occupancy', occupancy, SIZE_SMEM // size_smem)\n",
    "occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
    "num_programs = NUM_SM * occupancy\n",
    "print(f'num_programs', num_programs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e3e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "32\n",
    "2080\n",
    "4128\n",
    "6176"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
