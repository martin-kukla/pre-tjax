{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3641b790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa tensor([[-0.0208,  1.6458, -0.0943, -1.7180],\n",
      "        [ 0.3743, -0.5518,  0.6232,  0.2076],\n",
      "        [ 0.5863,  0.2572, -0.4931, -0.6361],\n",
      "        [ 1.1984,  0.5690,  0.0333,  0.4207],\n",
      "        [ 1.4335,  1.2339,  0.6477, -1.0560],\n",
      "        [ 2.6699, -3.0328, -0.9142,  0.4571],\n",
      "        [ 0.5359,  1.7751, -1.0170, -1.0332],\n",
      "        [-1.4559,  1.2538, -1.3986, -0.2163]], device='cuda:0')\n",
      "output [constexpr[8]]\n",
      "output tensor([ 1.6458, -0.5518,  0.2572,  0.5690,  1.2339, -3.0328,  1.7751,  1.2538],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRITON_INTERPRET\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "N, D = 8, 4\n",
    "aa = torch.randn((N, D), device=\"cuda\")\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "from model_triton import gelu_k, T_DROPOUT_RATE, dropout_k\n",
    "\n",
    "import triton.language as core\n",
    "@triton.jit\n",
    "def _indicator(n_dims: core.constexpr, idx: core.constexpr, pos: core.constexpr):\n",
    "    core.static_assert(idx < n_dims)\n",
    "    core.static_assert((pos == 0) or (pos == 1))\n",
    "    y = core.arange(0, 2)\n",
    "    if pos == 0:\n",
    "        y = 1 - y\n",
    "    for n in core.static_range(0, n_dims):\n",
    "        if n != n_dims - 1 - idx:\n",
    "            y = core.expand_dims(y, n)\n",
    "    return y\n",
    "\n",
    "@triton.jit\n",
    "def _take_slice(x, n_dims: core.constexpr, idx: core.constexpr, pos: core.constexpr, keep_dim: core.constexpr = True):\n",
    "    y = tl.sum(x * _indicator(n_dims, idx, pos), n_dims - 1 - idx)\n",
    "    if keep_dim:\n",
    "        y = core.expand_dims(y, n_dims - 1 - idx)\n",
    "\n",
    "    return y\n",
    "\n",
    "from triton_extensions import _take_slice_, _put_slice_\n",
    "# @triton.jit\n",
    "# def _indicator_(n_dims: core.constexpr, idx: core.constexpr, pos: core.constexpr, pos_dim: core.constexpr):\n",
    "#     core.static_assert(idx < n_dims)\n",
    "#     core.static_assert((pos>0) and (pos <= pos_dim))\n",
    "#     y = core.arange(1, pos_dim+1)\n",
    "#     y = tl.where(y==pos, 1, 0)\n",
    "    \n",
    "#     for n in core.static_range(0, n_dims):\n",
    "#         if n != n_dims - 1 - idx:\n",
    "#             y = core.expand_dims(y, n)\n",
    "#     return y\n",
    "\n",
    "# @triton.jit\n",
    "# def _take_slice_(x, n_dims: core.constexpr, idx: core.constexpr, pos: core.constexpr, pos_dim:core.constexpr, keep_dim: core.constexpr = True):\n",
    "#     ind = _indicator_(n_dims, idx, pos+1, pos_dim)\n",
    "#     y = tl.sum(x * ind, n_dims - 1 - idx)\n",
    "#     if keep_dim:\n",
    "#         y = core.expand_dims(y, n_dims - 1 - idx)\n",
    "\n",
    "#     return y\n",
    "\n",
    "# @triton.jit\n",
    "# def _put_slice_(x, n_dims: core.constexpr, idx: core.constexpr, pos: core.constexpr, pos_dim:core.constexpr, input_slice):\n",
    "#     ind = _indicator_(n_dims, idx, pos+1, pos_dim)\n",
    "#     print(f'x', x.shape)\n",
    "#     print(f'input_slice', input_slice.shape)\n",
    "#     print(f'ind', ind.shape, ind)\n",
    "#     y = tl.where(ind, input_slice, x)\n",
    "#     return y\n",
    "\n",
    "@triton.jit\n",
    "def my_kernel(input_ptr, input_stride0, input_stride1, output_ptr, \n",
    "              N:tl.constexpr, D:tl.constexpr):\n",
    "    n_offsets = tl.arange(0, N)            \n",
    "    d_offsets = tl.arange(0, D)\n",
    "    input_blck_ptr = input_ptr + n_offsets[:,None] * input_stride0 + d_offsets[None,:] * input_stride1\n",
    "    input_blck_mask = (n_offsets[:,None] < N) & (d_offsets[None, :] < D)\n",
    "    input_blck = tl.load(input_blck_ptr, mask=input_blck_mask, other=0.0)\n",
    "    #print(f'input_blck', input_blck)\n",
    "    \n",
    "    #output = _take_slice(input_blck, 2, 0, 0, keep_dim=False)\n",
    "    output = _take_slice_(input_blck, 2, 0, 1, 4, keep_dim=False) # take column\n",
    "    print(f'output', output.shape)\n",
    "    #output = _take_slice_(input_blck, 2, 1, 1, 8, keep_dim=False) # take row \n",
    "    \n",
    "    # copying column slice\n",
    "    #slice1 = _take_slice_(input_blck, 2, 0, 1, 4, keep_dim=True) # take column\n",
    "    #input_blck = _put_slice_(input_blck, 2, 0, 2, 4, slice1)    \n",
    "    #print(f'input_blck', input_blck)\n",
    "    \n",
    "    # copying row slice\n",
    "    slice1 = _take_slice_(input_blck, 2, 1, 1, 8, keep_dim=True)\n",
    "    input_blck = _put_slice_(input_blck, 2, 1, 2, 8, slice1)    \n",
    "    #print(f'input_blck', input_blck)\n",
    "    \n",
    "    #print('output', output)\n",
    "    tl.store(output_ptr + n_offsets, output, mask=n_offsets<N)    \n",
    "    \n",
    "    \n",
    "print('aa', aa)\n",
    "output = torch.zeros((N), device=\"cuda\")\n",
    "my_kernel[(1,)](aa, aa.stride(0), aa.stride(1), output, N, D)\n",
    "print('output', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f7abc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
