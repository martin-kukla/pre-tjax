{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb4d296b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11198,  8854, 13365,  ..., 35315, 21375, 26806],\n",
      "        [18511,  4283, 13204,  ...,  2442, 10715, 13912],\n",
      "        [31653, 13222, 31654,  ...,   447, 25360, 19412],\n",
      "        ...,\n",
      "        [15107, 30298, 20934,  ..., 33034, 22555, 34683],\n",
      "        [24485, 15681, 34809,  ...,  3716,  3049, 15418],\n",
      "        [12881,   886, 28761,  ..., 17332,  4690,  2574]], device='cuda:0')\n",
      "JIT total 0.11308550834655762\n",
      "Naive total 0.2157893180847168\n",
      "res1[0] 10.974886894226074 4094\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"TRITON_INTERPRET\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "from loss_and_optimizer_triton import t_avg_cross_entropy_loss_bkwd3\n",
    "\n",
    "\n",
    "BS, N, V = 8, 512, 35374\n",
    "#BS, N, V = 8, 512, 35328\n",
    "#BS, N, V = 8, 512, 4377\n",
    "#BS, N, V = 1, 4, 35328\n",
    "#BS, N, V = 1, 4, 8377\n",
    "#BS, N, V = 1, 4, 4377 #32\n",
    "#BS, N, V = 1, 4, 1000 #32\n",
    "#BS, N, V = 1, 4, 128 #32\n",
    "y_labels = torch.randint(V, (BS, N), device=\"cuda\")\n",
    "x_logits = torch.randn((BS, N, V), device=\"cuda\")\n",
    "print(y_labels)\n",
    "\n",
    "fn_naive = t_avg_cross_entropy_loss_bkwd3\n",
    "fn_jit = torch.compile(fn_naive)\n",
    "# burn it\n",
    "fn_jit(y_labels, x_logits) \n",
    "N_RUNS=10 #5 #100\n",
    "\n",
    "res1 = fn_naive(y_labels, x_logits)\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "for _ in range(N_RUNS):\n",
    "    #result = fn_jit(aa)\n",
    "    result = fn_jit(y_labels, x_logits)\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(f'JIT total', total)\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "for _ in range(N_RUNS):\n",
    "    #result = fn_naive(aa)\n",
    "    result = fn_naive(y_labels, x_logits)\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(f'Naive total', total)\n",
    "\n",
    "print(f'res1[0]', res1[0].item(), res1[1].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "907e5784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0.06529831886291504\n",
      "res2[0] 10.974886894226074 4094\n"
     ]
    }
   ],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def t_avg_cross_entropy_loss_bkwd3_k(y_labels_ptr,\n",
    "                    x_logits_ptr,\n",
    "                    loss_ptr,\n",
    "                    dloss_dx_ptr,\n",
    "                    aux_idx_ptr, # Auxilary indexing operation, as alternativ for tl.gather. It uses global memory: overhead + slow\n",
    "                    x_logits_row_stride,\n",
    "                    dloss_dx_row_stride,\n",
    "                    aux_idx_row_stride,\n",
    "                    nonzero_count,\n",
    "                    n_rows,\n",
    "                    n_cols,\n",
    "                    BLOCK_SIZE: tl.constexpr,\n",
    "                    num_stages: tl.constexpr,\n",
    "                    ):\n",
    "    row_start = tl.program_id(0)\n",
    "    row_step = tl.num_programs(0)\n",
    "    blcks = tl.cdiv(n_cols, BLOCK_SIZE)\n",
    "    \n",
    "    for row_idx in tl.range(row_start, n_rows, row_step, num_stages):\n",
    "        y_label = tl.load(y_labels_ptr + row_idx) # TODO T: load once, and keep it in shared memory?\n",
    "        x_logits_row_start_ptr = x_logits_ptr + row_idx * x_logits_row_stride\n",
    "        \n",
    "        # Online softmax (https://arxiv.org/pdf/1805.02867)\n",
    "        # computes x_logits_max & x_logits_sumexp with one memory load \n",
    "        x_logits_max = -1e9\n",
    "        x_logits_sumexp = 0.0\n",
    "        d_s = tl.full((BLOCK_SIZE, ), 1, dtype=tl.float32)\n",
    "        for blck_idx in tl.range(0, blcks):\n",
    "            offsets = blck_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "            mask = offsets < n_cols\n",
    "            x_logits = tl.load(x_logits_row_start_ptr + offsets, mask=mask, other=-1e9)\n",
    "            blck_x_logits_max = tl.max(x_logits, axis=0)\n",
    "            n_x_logits_max = tl.maximum(x_logits_max, blck_x_logits_max)\n",
    "            x_logits_sumexp = x_logits_sumexp * tl.exp(x_logits_max - n_x_logits_max) + tl.sum(tl.exp(x_logits - n_x_logits_max))\n",
    "            x_logits_max = n_x_logits_max\n",
    "        \n",
    "        # If not padding token, contribute to loss/dloss_dx computation\n",
    "        if y_label!=0:\n",
    "            for blck_idx in tl.range(0, blcks):\n",
    "                offsets = blck_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "                mask = offsets < n_cols\n",
    "                x_logits = tl.load(x_logits_row_start_ptr + offsets, mask=mask, other=-1e9)\n",
    "                x_logits = x_logits - x_logits_max\n",
    "                x_logits_exp = tl.exp(x_logits)\n",
    "                \n",
    "                if y_label>= blck_idx*BLOCK_SIZE and y_label<(blck_idx+1)*BLOCK_SIZE:    \n",
    "                    # Workaround for the lack of tl.gather for now (slow, as it uses non-local memory)\n",
    "                    #loss = tl.gather(x_logits, y_label, 0) - x_logits_logsumexp\n",
    "                    aux_idx_row_start_ptr = aux_idx_ptr + row_idx * aux_idx_row_stride\n",
    "                    aux_idx_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "                    aux_idx = tl.load(aux_idx_row_start_ptr + aux_idx_offsets) # no need for mask here\n",
    "                    logit_for_y = tl.sum(x_logits * aux_idx)\n",
    "                    loss = logit_for_y - tl.log(x_logits_sumexp)\n",
    "                    loss = - loss/nonzero_count\n",
    "                    tl.atomic_add(loss_ptr, loss)\n",
    "\n",
    "                # propagate back:\n",
    "                # I used torch.scatter_add_, which I am not sure I can perform locally? thus\n",
    "                # using atomic_add twice\n",
    "                dloss_dx_row_start_ptr = dloss_dx_ptr + row_idx * dloss_dx_row_stride\n",
    "                if y_label>= blck_idx*BLOCK_SIZE and y_label<(blck_idx+1)*BLOCK_SIZE: # TODO T: Can I do it using mask?\n",
    "                    tl.atomic_add(dloss_dx_row_start_ptr + y_label, -1/nonzero_count)\n",
    "                log_softmax_jac = - x_logits_exp/x_logits_sumexp\n",
    "                dloss_dx = -log_softmax_jac/nonzero_count\n",
    "                tl.atomic_add(dloss_dx_row_start_ptr + offsets, dloss_dx, mask=mask)\n",
    "        \n",
    "def t_avg_cross_entropy_loss_bkwd3_t(y_labels, x_logits):\n",
    "    dloss_dx_shape = x_logits.shape \n",
    "    y_labels = y_labels.reshape((-1,))\n",
    "    y_labels = y_labels.to(torch.int64) # TODO XXX: shouldn't we pass y_labels in int64 already?\n",
    "    x_logits = x_logits.reshape((y_labels.numel(), -1))\n",
    "    nonzero_count = torch.count_nonzero(y_labels)\n",
    "    \n",
    "    n_rows, n_cols = x_logits.shape\n",
    "    loss = torch.zeros((1), device=x_logits.device) # can we just return value from triton kernel instead? I doubt that\n",
    "    dloss_dx = torch.zeros_like(x_logits)\n",
    "    # TODO T: The below numbers were tuned for A10 by choosing num_warps=8\n",
    "    num_warps=8\n",
    "    num_stages=2\n",
    "    BLOCK_SIZE = 1024\n",
    "    aux_idx = torch.zeros((n_rows, BLOCK_SIZE), device=x_logits.device, dtype=torch.bool)\n",
    "    aux_idx.scatter_(1, (y_labels % BLOCK_SIZE).unsqueeze(1), True)\n",
    "    num_programs = min(n_rows, 480)\n",
    "    \n",
    "    t_avg_cross_entropy_loss_bkwd3_k[(num_programs,)](y_labels, x_logits, loss, dloss_dx, aux_idx, x_logits.stride(0), dloss_dx.stride(0), aux_idx.stride(0), nonzero_count.item(), \n",
    "                                                      n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps, num_stages=num_stages)\n",
    "    \n",
    "    return loss, nonzero_count, dloss_dx.reshape(dloss_dx_shape)\n",
    "    \n",
    "fn_t = t_avg_cross_entropy_loss_bkwd3_t\n",
    "\n",
    "res2 = fn_t(y_labels, x_logits)\n",
    "import time\n",
    "t0 = time.time()\n",
    "for _ in range(N_RUNS):\n",
    "    #result = fn_t(aa)\n",
    "    result = fn_t(y_labels, x_logits)\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(f'total', total)\n",
    "\n",
    "print(f'res2[0]', res2[0].item(), res2[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03c56d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(res1[0], res2[0]), (res1[0].item(), res2[0].item())\n",
    "assert res1[1].item() == res2[1].item(), (res1[1], res2[1])\n",
    "assert torch.allclose(res1[2], res2[2]), (res1[2].shape, res2[2].shape, res1[2], res2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c612834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1097d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22723MB, multi_processor_count=80, uuid=61ea3d2d-53a8-44f6-4844-0bcc29aa720b, L2_cache_size=6MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_shared_mem': 101376,\n",
       " 'max_num_regs': 65536,\n",
       " 'multiprocessor_count': 80,\n",
       " 'warpSize': 32,\n",
       " 'sm_clock_rate': 1710000,\n",
       " 'mem_clock_rate': 6251000,\n",
       " 'mem_bus_width': 384}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_properties(\"cuda\"))\n",
    "from triton.runtime import driver\n",
    "device = torch.cuda.current_device()\n",
    "properties = driver.active.utils.get_device_properties(device)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "WARP_SIZE = properties[\"warpSize\"] # Not 64 as A100\n",
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "feee7085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLOCK_SIZE 1024 num_stages 2 num_warps 8\n",
      "n_regs 40 size_smem 4096\n",
      "occupancy 6 24\n",
      "num_programs 480\n"
     ]
    }
   ],
   "source": [
    "num_stages = 4 if SIZE_SMEM > 200000 else 2\n",
    "num_warps = 8\n",
    "y_labels_1d = y_labels.reshape((-1,))\n",
    "y_labels_1d = y_labels_1d.to(torch.int64)\n",
    "x_logits_2d = x_logits.reshape((y_labels.numel(), -1))\n",
    "nonzero_count = torch.count_nonzero(y_labels_1d)\n",
    "\n",
    "n_rows, n_cols = x_logits_2d.shape\n",
    "BLOCK_SIZE = 1024 #512\n",
    "loss = torch.zeros((1), device=x_logits_2d.device) # can we just return value from triton kernel instead? I doubt that\n",
    "dloss_dx = torch.zeros_like(x_logits_2d)\n",
    "aux_idx = torch.zeros((n_rows, BLOCK_SIZE), device=x_logits_2d.device, dtype=torch.bool)\n",
    "aux_idx.scatter_(1, (y_labels_1d % BLOCK_SIZE).unsqueeze(1), True)\n",
    "print(f'BLOCK_SIZE', BLOCK_SIZE, 'num_stages', num_stages, 'num_warps', num_warps)\n",
    "\n",
    "kernel = t_avg_cross_entropy_loss_bkwd3_k.warmup(y_labels_1d, x_logits_2d, loss, dloss_dx, aux_idx, x_logits_2d.stride(0), dloss_dx.stride(0), aux_idx.stride(0), nonzero_count.item(), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,\n",
    "                                   num_stages=num_stages, num_warps=num_warps, grid=(1, ))\n",
    "kernel._init_handles()\n",
    "n_regs = kernel.n_regs\n",
    "size_smem = kernel.metadata.shared\n",
    "print(f'n_regs', n_regs, 'size_smem', size_smem)\n",
    "\n",
    "occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "size_smem = max(1, size_smem)\n",
    "print(f'occupancy', occupancy, SIZE_SMEM // size_smem)\n",
    "occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
    "num_programs = NUM_SM * occupancy\n",
    "print(f'num_programs', num_programs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e60340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
