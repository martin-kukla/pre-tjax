{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b29a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "       Torch-Compiled Region: 0/0        92.11%       1.796ms        96.68%       1.885ms       1.885ms       0.000us         0.00%     119.326us     119.326us             1  \n",
      "          t_layernorm_bkwd2_p_k_0         0.72%      13.981us         1.05%      20.491us      20.491us     116.318us        97.48%     116.318us     116.318us             1  \n",
      "            t_layernorm_bkwd2_p_k         0.00%       0.000us         0.00%       0.000us       0.000us     116.318us        97.48%     116.318us     116.318us             1  \n",
      "    triton_poi_fused_zeros_like_0         1.76%      34.281us         3.51%      68.441us      34.221us       3.008us         2.52%       3.008us       1.504us             2  \n",
      "    triton_poi_fused_zeros_like_0         0.00%       0.000us         0.00%       0.000us       0.000us       3.008us         2.52%       3.008us       1.504us             2  \n",
      "         TorchDynamo Cache Lookup         1.56%      30.461us         1.56%      30.461us      30.461us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                   cuLaunchKernel         2.09%      40.670us         2.09%      40.670us      13.557us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "            cudaDeviceSynchronize         1.76%      34.360us         1.76%      34.360us      34.360us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.950ms\n",
      "Self CUDA time total: 119.326us\n",
      "\n",
      "JIT total 0.0006122589111328125\n",
      "Naive total 0.1339859962463379\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model_triton import *\n",
    "\n",
    "dloss_dx = torch.randn((8, 512, 768), device=\"cuda\")\n",
    "#dloss_dx = torch.randn((1, 768), device=\"cuda\")\n",
    "#Two shapes are being used: [8, 12, 512, 512], and 4096, 35374\n",
    "layer_params = (torch.randn((768), device=\"cuda\"), torch.randn((768), device=\"cuda\"))\n",
    "#aa = torch.randn((2, 768), device=\"cuda\")\n",
    "aa = torch.randn((8, 512, 768), device=\"cuda\")\n",
    "#aa = torch.randn((1, 768), device=\"cuda\")\n",
    "#aa = torch.randn((4096, 35374), device=\"cuda\")\n",
    "#aa = aa.view(-1)\n",
    "N = 1 #100\n",
    "\n",
    "from functools import partial\n",
    "def fn_naive(dloss_dx, x):\n",
    "    return t_layernorm_bkwd2_p(dloss_dx, layer_params, x)\n",
    "fn_jit = torch.compile(fn_naive)\n",
    "# burn it\n",
    "#fn_jit(aa) \n",
    "fn_jit(dloss_dx, aa) \n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity, schedule\n",
    "activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA]\n",
    "with profile(activities=activities, record_shapes=True) as prof:\n",
    "    for _ in range(N):\n",
    "        #result = fn_jit(aa)\n",
    "        result = fn_jit(dloss_dx, aa)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "for _ in range(N):\n",
    "    #result = fn_jit(aa)\n",
    "    result = fn_jit(dloss_dx, aa)\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(f'JIT total', total)\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "for _ in range(N):\n",
    "    #result = fn_naive(aa)\n",
    "    result = fn_naive(dloss_dx, aa)\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(f'Naive total', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ede499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import triton\n",
    "# print(triton.runtime.driver.active.get_current_target())\n",
    "# device = \"cuda\" #triton.runtime.driver.active.get_active_torch_device()\n",
    "# properties = triton.runtime.driver.active.utils.get_device_properties(device)\n",
    "# See https://github.com/triton-lang/triton/issues/5628, and https://github.com/triton-lang/triton/issues/5388\n",
    "# properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc37de6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  t_layernorm_bkwd2_p_k         0.00%       0.000us         0.00%       0.000us       0.000us      93.472us        96.91%      93.472us      93.472us             1  \n",
      "                                       aten::zeros_like        18.61%     811.503us        98.31%       4.287ms       2.144ms       0.000us         0.00%       2.976us       1.488us             2  \n",
      "                                            aten::zero_        16.28%     709.981us        17.92%     781.483us     390.741us       0.000us         0.00%       2.976us       1.488us             2  \n",
      "                                            aten::fill_         0.61%      26.770us         1.64%      71.502us      35.751us       2.976us         3.09%       2.976us       1.488us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.976us         3.09%       2.976us       1.488us             2  \n",
      "                                          aten::reshape         0.16%       6.830us         0.54%      23.650us      11.825us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                             aten::view         0.39%      16.820us         0.39%      16.820us       8.410us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                       aten::empty_like        23.97%       1.045ms        61.78%       2.694ms       1.347ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                    aten::empty_strided        37.81%       1.649ms        37.81%       1.649ms     824.358us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                       cudaLaunchKernel         1.03%      44.732us         1.03%      44.732us      22.366us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                         cuLaunchKernel         0.50%      21.600us         0.50%      21.600us      21.600us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaDeviceSynchronize         0.65%      28.321us         0.65%      28.321us      28.321us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.361ms\n",
      "Self CUDA time total: 96.448us\n",
      "\n",
      "total 0.0005567073822021484\n"
     ]
    }
   ],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# Note that the kernel assumes that n_cols < BLOCK_SIZE\n",
    "# TODO T: investigate numerical differences from torch.func implementation\n",
    "@triton.jit\n",
    "def t_layernorm_bkwd2_p_k(dloss_dx_ptr,\n",
    "                    x_ptr,\n",
    "                    output1_ptr,\n",
    "                    output2_ptr,                          \n",
    "                    dloss_dx_stride,\n",
    "                    x_row_stride,                        \n",
    "                    n_rows,\n",
    "                    n_cols,\n",
    "                    BLOCK_SIZE: tl.constexpr,\n",
    "                    num_stages: tl.constexpr,\n",
    "                    ):\n",
    "    row_start = tl.program_id(0)\n",
    "    row_step = tl.num_programs(0)\n",
    "    \n",
    "    offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_cols\n",
    "    \n",
    "    # TODO T: how much this is being reused?\n",
    "    _output1 = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n",
    "    _output2 = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n",
    "        \n",
    "    for row_idx in tl.range(row_start, n_rows, row_step, num_stages):\n",
    "        dloss_dx_row_start_ptr = dloss_dx_ptr + row_idx * dloss_dx_stride\n",
    "        dloss_dx = tl.load(dloss_dx_row_start_ptr + offsets, mask=mask, other=0.0)\n",
    "        x_row_start_ptr = x_ptr + row_idx * x_row_stride    \n",
    "        x = tl.load(x_row_start_ptr + offsets, mask=mask, other=0.0)\n",
    "        \n",
    "        # compute mean and std for x\n",
    "        x_sum = tl.sum(x, axis=0)\n",
    "        x_mu = x_sum/ n_cols\n",
    "        x_minus_mu = x - x_mu\n",
    "        x_minus_mu2 = x_minus_mu * x_minus_mu\n",
    "        x_minus_mu2_sum = tl.sum(x_minus_mu2, axis=0)\n",
    "        x_sigma2 = x_minus_mu2_sum / (n_cols-1)\n",
    "        x_sigma = tl.sqrt_rn(x_sigma2)\n",
    "        \n",
    "        # normalize x\n",
    "        x_norm = x_minus_mu/x_sigma    \n",
    "        \n",
    "        _output1 += dloss_dx * x_norm\n",
    "        _output2 += dloss_dx\n",
    "    \n",
    "    # TODO T: Should we add parallel reduction strategy here: save to partial GROUP_SIZE_M sums first, before summing it up?\n",
    "    tl.atomic_add(output1_ptr + offsets, _output1, mask=mask)\n",
    "    tl.atomic_add(output2_ptr + offsets, _output2, mask=mask)    \n",
    "    \n",
    "def t_layernorm_bkwd2_p_t(dloss_dx:torch.Tensor, layer_params: torch.Tensor, x: torch.Tensor):\n",
    "    # TODO T: without this reshape, this func is 2times faster?\n",
    "    dloss_dx_2d = dloss_dx.reshape((-1, dloss_dx.shape[-1]))\n",
    "    x_2d = x.reshape((-1, x.shape[-1])) \n",
    "    n_rows, n_cols = x_2d.shape\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols) \n",
    "    output1 = torch.zeros_like(layer_params[0])\n",
    "    output2 = torch.zeros_like(layer_params[1])    \n",
    "    \n",
    "    # TODO T: The below numbers were tuned for A10 by choosing num_warps=8\n",
    "    num_warps = 8\n",
    "    num_stages = 2\n",
    "    num_programs = min(n_rows, 480) \n",
    "    t_layernorm_bkwd2_p_k[(num_programs,)](dloss_dx_2d, x_2d, output1, output2, \n",
    "                                       dloss_dx_2d.stride(0), x_2d.stride(0), n_rows, n_cols, \n",
    "                                       BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps, num_stages=num_stages)\n",
    "    return output1, output2 \n",
    "\n",
    "def fn_t(dloss_dx, x):\n",
    "    return t_layernorm_bkwd2_p_t(dloss_dx, layer_params, x)\n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity, schedule\n",
    "activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA]\n",
    "with profile(activities=activities, record_shapes=True) as prof:\n",
    "    for _ in range(N):\n",
    "        #result = fn_t(aa)\n",
    "        result = fn_t(dloss_dx, aa)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20))\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "for _ in range(N):\n",
    "    #result = fn_t(aa)\n",
    "    result = fn_t(dloss_dx, aa)\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(f'total', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8e154d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1 torch.Size([768]) tensor([  95.1687,  -25.0189,  -59.5536,   43.9867,   -1.0060,  -11.9509,\n",
      "          29.2568,   37.7534,  -32.6870,    1.8032,  -11.9498,   -9.6395,\n",
      "         -22.2823,   36.5493,  -50.3863,   -2.3215,   16.2722,  161.2084,\n",
      "           4.1640,  -92.0618,  143.4859,  -52.1562,  -28.1901,  -90.8348,\n",
      "          33.8247,   11.2040,   52.2683,   -7.9592, -109.8448,   -5.5466,\n",
      "          18.1443,    7.8433,  148.9875,   22.3122,   56.9081,    8.9039,\n",
      "          11.6286,   -2.4500,  -25.6496,   32.2235,   -0.3576, -130.9826,\n",
      "          53.0317,   65.4559,   84.4505,   41.9895, -100.6363,   11.3724,\n",
      "           4.3540,  -67.5684,  -54.6326,  -83.4864,   93.3548,   30.4224,\n",
      "         119.0786,  -46.5971,  -59.4636, -111.6934,  -91.6971,  -79.6589,\n",
      "         -68.4512,  112.8705,   87.5746,  -25.3938,  -30.1264,  -16.2564,\n",
      "         -90.3486,   69.9113,  -82.8568,   -4.0096,  -20.9173,  -65.0024,\n",
      "         -46.2275,   11.9035,   43.8028,   18.2876,  -44.1837,   39.7555,\n",
      "         -48.2667,   73.3372, -107.5729,   61.2763,   -2.3786, -105.2141,\n",
      "         -37.3913,    1.2168,  -71.4639,  -55.1916,   93.3209,   22.3747,\n",
      "          23.1898,   24.8817,  -71.3236,  -80.4180,   23.4042,   42.2785,\n",
      "           3.2289,  130.6844,  -41.9659,  -61.6106], device='cuda:0')\n",
      "res2 torch.Size([768]) tensor([  95.1687,  -25.0189,  -59.5535,   43.9867,   -1.0060,  -11.9508,\n",
      "          29.2568,   37.7534,  -32.6870,    1.8032,  -11.9498,   -9.6395,\n",
      "         -22.2823,   36.5493,  -50.3862,   -2.3215,   16.2721,  161.2085,\n",
      "           4.1640,  -92.0617,  143.4860,  -52.1562,  -28.1901,  -90.8348,\n",
      "          33.8248,   11.2041,   52.2683,   -7.9593, -109.8447,   -5.5465,\n",
      "          18.1443,    7.8433,  148.9875,   22.3122,   56.9081,    8.9039,\n",
      "          11.6286,   -2.4500,  -25.6496,   32.2235,   -0.3576, -130.9826,\n",
      "          53.0318,   65.4559,   84.4505,   41.9895, -100.6363,   11.3724,\n",
      "           4.3540,  -67.5684,  -54.6326,  -83.4864,   93.3547,   30.4224,\n",
      "         119.0785,  -46.5971,  -59.4636, -111.6934,  -91.6971,  -79.6589,\n",
      "         -68.4512,  112.8704,   87.5745,  -25.3938,  -30.1264,  -16.2564,\n",
      "         -90.3487,   69.9113,  -82.8568,   -4.0096,  -20.9172,  -65.0025,\n",
      "         -46.2276,   11.9034,   43.8028,   18.2876,  -44.1837,   39.7555,\n",
      "         -48.2667,   73.3372, -107.5728,   61.2763,   -2.3787, -105.2141,\n",
      "         -37.3914,    1.2168,  -71.4639,  -55.1917,   93.3209,   22.3747,\n",
      "          23.1898,   24.8817,  -71.3237,  -80.4180,   23.4042,   42.2786,\n",
      "           3.2289,  130.6844,  -41.9659,  -61.6106], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#res1 = fn_jit(aa)\n",
    "res1 = fn_jit(dloss_dx, aa)\n",
    "#res2 = fn_t(aa)\n",
    "res2 = fn_t(dloss_dx, aa)\n",
    "\n",
    "\n",
    "assert torch.allclose(res1[0], res2[0], atol=1e-2, rtol=0), (res1[0].shape, res2[0].shape, res1[0][:100], res2[0][:100])\n",
    "assert torch.allclose(res1[1], res2[1], atol=1e-2, rtol=0), (res1[1].shape, res2[1].shape, res1[1][:100], res2[1][:100])\n",
    "#assert torch.allclose(res1, res2), (res1[0], res2[0])\n",
    "print(f'res1', res1[0].shape, res1[0][:100])\n",
    "print(f'res2', res2[0].shape, res2[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a9918e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  24.4056,  -40.0551,  -10.6713,   40.7296,  -93.1700,  -37.2333,\n",
      "         -16.0784,  -37.0965,   68.9446,  -34.1143,   -8.0013,   87.3877,\n",
      "           7.0199,   -1.1236,    1.1803,   -8.4736,   45.7447,  -49.6264,\n",
      "          47.9383,   56.9961,   30.9900,  -41.3200,  -68.3624,   14.3464,\n",
      "         -62.5568,   26.7465,  -68.7216,  109.0114,  -74.8925,   75.0364,\n",
      "          68.8853,  111.5215,   22.3440,  -43.0244,   76.2037,  -21.0002,\n",
      "         -52.6470,  -14.3917,  -32.5757,  -78.8080,  -60.3404,  -29.6343,\n",
      "          29.0757,  -14.0219,  -24.4524,   59.0663,   71.7406,  111.2338,\n",
      "         -30.9470,  -27.4130,   60.2276,  -16.4526,  -93.9371,   71.3842,\n",
      "          -5.0634,  -11.1472,   17.0944,   90.8599,  -57.7976,  -26.7024,\n",
      "         -33.0511,  -72.8697,  116.0010,   62.7593,  151.1949,  -53.2054,\n",
      "          86.0129,   18.3840,  -93.3876,   20.7278,  -20.6406, -144.1550,\n",
      "          30.9153,   36.3842,  -69.4481,  -25.1651,   89.7278,   17.8879,\n",
      "          10.0968, -110.9552,  -12.0548,   17.3168,   63.0861,   89.4908,\n",
      "          22.0224,  -32.2407,   87.2561, -101.6970,  -76.0362,   11.7128,\n",
      "         -10.7386,   -0.4022,  -99.0372,   15.9147,  102.4907,   -5.0265,\n",
      "          22.3404,   25.7066,    1.2557,    0.9275], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(res1[1][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41abe4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  24.4056,  -40.0551,  -10.6713,   40.7296,  -93.1700,  -37.2333,\n",
      "         -16.0784,  -37.0965,   68.9446,  -34.1143,   -8.0013,   87.3877,\n",
      "           7.0199,   -1.1236,    1.1803,   -8.4736,   45.7447,  -49.6264,\n",
      "          47.9383,   56.9961,   30.9900,  -41.3200,  -68.3623,   14.3464,\n",
      "         -62.5568,   26.7465,  -68.7216,  109.0113,  -74.8926,   75.0364,\n",
      "          68.8853,  111.5216,   22.3440,  -43.0244,   76.2037,  -21.0002,\n",
      "         -52.6470,  -14.3917,  -32.5757,  -78.8079,  -60.3405,  -29.6343,\n",
      "          29.0757,  -14.0219,  -24.4524,   59.0663,   71.7407,  111.2338,\n",
      "         -30.9470,  -27.4131,   60.2276,  -16.4526,  -93.9371,   71.3841,\n",
      "          -5.0634,  -11.1472,   17.0943,   90.8599,  -57.7976,  -26.7024,\n",
      "         -33.0511,  -72.8697,  116.0010,   62.7593,  151.1948,  -53.2054,\n",
      "          86.0128,   18.3840,  -93.3876,   20.7278,  -20.6406, -144.1550,\n",
      "          30.9153,   36.3843,  -69.4481,  -25.1651,   89.7277,   17.8879,\n",
      "          10.0968, -110.9551,  -12.0549,   17.3168,   63.0861,   89.4909,\n",
      "          22.0223,  -32.2407,   87.2562, -101.6970,  -76.0363,   11.7128,\n",
      "         -10.7386,   -0.4022,  -99.0372,   15.9147,  102.4907,   -5.0265,\n",
      "          22.3404,   25.7067,    1.2558,    0.9274], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(res2[1][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d3b1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_dropout_fwd:\n",
      "       N      Triton       Torch       naive\n",
      "0  768.0  356.173905  356.496821  357.036318\n"
     ]
    }
   ],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[768], #[128 * i for i in range(2, 100)],  # Different possible values for `x_name`.\n",
    "        #x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['triton', 'torch', 'naive'],  # Possible values for `line_arg`.\n",
    "        line_names=['Triton', 'Torch', 'naive'],  # Label name for the lines.\n",
    "        styles=[('blue', '-'), ('green', '-'), ('orange', '-')],  # Line styles.\n",
    "        ylabel='GB/s',  # Label name for the y-axis.\n",
    "        plot_name='t_dropout_fwd',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={'M':4096},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "        # TODO T: Use real M i.e. \n",
    "    ))\n",
    "def benchmark(M, N, provider):\n",
    "    dloss_dx = torch.rand(M, N, device=\"cuda\", dtype=torch.float32)    \n",
    "    x = torch.rand(M, N, device=\"cuda\", dtype=torch.float32)\n",
    "    stream = getattr(torch, \"cuda\").Stream() # TODO XXX XXX: what is this stream about?\n",
    "    getattr(torch, \"cuda\").set_stream(stream)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'torch':\n",
    "        #ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_jit(x), quantiles=quantiles)\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_jit(dloss_dx, x), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        #ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_t(x), quantiles=quantiles)        \n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_t(dloss_dx, x), quantiles=quantiles)\n",
    "    if provider == 'naive':\n",
    "        #ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_naive(x), quantiles=quantiles)\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: fn_naive(dloss_dx, x), quantiles=quantiles)\n",
    "    gbps = lambda ms: 3 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b209b93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='NVIDIA A10G', major=8, minor=6, total_memory=22723MB, multi_processor_count=80, uuid=61ea3d2d-53a8-44f6-4844-0bcc29aa720b, L2_cache_size=6MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_shared_mem': 101376,\n",
       " 'max_num_regs': 65536,\n",
       " 'multiprocessor_count': 80,\n",
       " 'warpSize': 32,\n",
       " 'sm_clock_rate': 1710000,\n",
       " 'mem_clock_rate': 6251000,\n",
       " 'mem_bus_width': 384}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_properties(\"cuda\"))\n",
    "from triton.runtime import driver\n",
    "device = torch.cuda.current_device()\n",
    "properties = driver.active.utils.get_device_properties(device)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "WARP_SIZE = properties[\"warpSize\"] # Not 64 as A100\n",
    "properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61c61a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_stages 2 num_warps 8\n",
      "n_regs 39 size_smem 8224\n",
      "occupancy 6 12\n",
      "num_programs 480\n"
     ]
    }
   ],
   "source": [
    "num_stages = 4 if SIZE_SMEM > 200000 else 2\n",
    "num_warps = 8\n",
    "dloss_dx_2d = dloss_dx.reshape((-1, dloss_dx.shape[-1]))\n",
    "x_2d = aa.reshape((-1, aa.shape[-1])) # TODO T: without this reshape, this func is 2times faster\n",
    "n_rows, n_cols = x_2d.shape\n",
    "BLOCK_SIZE = triton.next_power_of_2(n_cols) \n",
    "output1 = torch.zeros_like(layer_params[0])\n",
    "output2 = torch.zeros_like(layer_params[1]) \n",
    "print(f'num_stages', num_stages, 'num_warps', num_warps)\n",
    "\n",
    "kernel = t_layernorm_bkwd2_p_k.warmup(dloss_dx_2d, x_2d, output1, output2, \n",
    "                                        dloss_dx_2d.stride(0), x_2d.stride(0), \n",
    "                                        n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,\n",
    "                                        num_stages=num_stages, num_warps=num_warps, grid=(1, ))\n",
    "kernel._init_handles()\n",
    "n_regs = kernel.n_regs\n",
    "size_smem = kernel.metadata.shared\n",
    "print(f'n_regs', n_regs, 'size_smem', size_smem)\n",
    "\n",
    "occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "size_smem = max(1, size_smem) # accounts for divisiion by 0 below. size_smem=0 is possible\n",
    "print(f'occupancy', occupancy, SIZE_SMEM // size_smem)\n",
    "occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
    "num_programs = NUM_SM * occupancy\n",
    "print(f'num_programs', num_programs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09068a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "32\n",
    "2080\n",
    "4128\n",
    "6176"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
