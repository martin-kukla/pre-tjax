{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3275a27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_MEM_FRACTION=0.95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[cuda(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### JAX\n",
    "\n",
    "# UPDATE/TODO XXX: We can now move to jax24.04-py3 (https://docs.nvidia.com/deeplearning/frameworks/jax-release-notes/rel-24-04.html)\n",
    "# TODO: this is slightly faster even with the warning -> invewstigate (current jax version is 0.4.26, where the image has 0.4.17)\n",
    "#! pip install -U \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "#2024-05-02 08:16:04.763248: W external/xla/xla/service/gpu/nvptx_compiler.cc:718] \n",
    "#The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.4.131). \n",
    "#Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. \n",
    "#You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
    "\n",
    "# TODO: It looks like I am suffering from fragmentation on GPU, thus enabling prelocation\n",
    "# Disable JAX memory preallocation\n",
    "#import os\n",
    "#os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]=\"false\"\n",
    "#os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\".90\"\n",
    "#%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "%env XLA_PYTHON_CLIENT_MEM_FRACTION=0.95\n",
    "\n",
    "#!LD_LIBRARY_PATH=/usr/local/cuda/compat:$LD_LIBRARY_PATH\n",
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeacfa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Loading tokenizer bpe_tokenizer_ds_train_all_merges_35k.pickle\n",
      "Tokenizing dataset\n"
     ]
    }
   ],
   "source": [
    "### DATASET\n",
    "from tokenized_dataset import load_tokenized_dataset, get_batched_examples, get_batched_examples_packed\n",
    "ds, (tokenizer, detokenize, tokenizer_vocab_size) = load_tokenized_dataset()\n",
    "ds = ds.shuffle(seed=42) # TODO: put it in better place? does it mess up with resume_from_checkpoint logic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea127b58",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 38561\n",
      "Number of params: 63883425\n"
     ]
    }
   ],
   "source": [
    "### MODEL\n",
    "\n",
    "from model import *\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, lax \n",
    "from jax import random\n",
    "\n",
    "LAYERS = 6\n",
    "model_vocab_size = tokenizer_vocab_size + 3 # add padding token (0) + start of sequence token + end of sequence token \n",
    "START_TOK = tokenizer_vocab_size + 1\n",
    "END_TOK = tokenizer_vocab_size + 2 # TODO: in standard LLM convention, it should be 1. Also, it could be part of tokenizer_vocab_size\n",
    "EMB_DIM=512\n",
    "FFN_DIM=2048\n",
    "NUM_HEADS = 8\n",
    "params = init_transformer(model_vocab_size, EMB_DIM, LAYERS, NUM_HEADS, FFN_DIM, random.PRNGKey(0))\n",
    "\n",
    "print(f'Vocabulary size: {model_vocab_size}')\n",
    "num_params = sum([jnp.size(p_leaf) for p_leaf in jax.tree_util.tree_leaves(params)])\n",
    "print(f'Number of params: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8867381",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss + grads\n",
    "#def one_hot(x, k, dtype=jnp.float32): \n",
    "#    \"\"\"Create a one-hot encoding of x of size k.\"\"\" \n",
    "#    return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "#\n",
    "#batched_one_hot = vmap(one_hot, in_axes=(0, None))\n",
    "\n",
    "def avg_cross_entropy_loss(y_labels, x_logits): # y_labels: batch_len x seq_len, x_logits: batch_len x seq_len x vocab_size\n",
    "    # Note that in jax, un-jitted reshape calls are producing copies of array instead of views.\n",
    "    # However, for jitted, this SHOULD be optmized away (I checked this function that indeed it is).\n",
    "    y_labels_1d = jnp.reshape(y_labels, -1) # there is probably a way of doing it while staying in 2d..\n",
    "    x_logits_2d = jnp.reshape(x_logits, (y_labels.size, -1))\n",
    "    elements_loss = log_softmax(x_logits_2d)[(jnp.arange(y_labels.size), y_labels_1d)]\n",
    "    elements_loss = jnp.where(y_labels_1d != 0, elements_loss, jnp.nan) # account for padding tokens\n",
    "    result = -jnp.nanmean(elements_loss) \n",
    "    return result, jnp.count_nonzero(y_labels)\n",
    "    \n",
    "def accuracy(y_labels, x_logits):\n",
    "    return jnp.nanmean(jnp.where(y_labels!=0, y_labels == jnp.argmax(x_logits, axis=-1), jnp.nan))\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "@partial(jax.jit, static_argnames=['sample_len'])\n",
    "def predict(params, x, x_mask, y_mask, yx_mask, x_indices, y_indices, sample_len): # TODO: code up not-scanned version, which could be faster on GPU\n",
    "    def predict_step(step_i, y):\n",
    "        # TODO: Cache key-value pairs\n",
    "        new_y = batched_forward_aiayn(params, x, y, x_mask, y_mask, yx_mask, x_indices, y_indices, random.PRNGKey(0), False) \n",
    "        new_toks = jnp.argmax(new_y[:, step_i], axis=-1)\n",
    "        y = y.at[:,step_i+1].set(new_toks)\n",
    "        return y\n",
    "    \n",
    "    start_toks = jnp.full((x.shape[0], sample_len), START_TOK)\n",
    "    y_sample = jax.lax.fori_loop(0, sample_len, predict_step, start_toks) \n",
    "    y_sample = jnp.where(jax.lax.cummax(y_sample, axis=1) != END_TOK, y_sample, 0) # replace END token, and what follows with padding\n",
    "\n",
    "    y_sample = y_sample[:, 1:]\n",
    "    return jnp.where(y_sample!=START_TOK, y_sample, 0) # It should not be happening, but for random model it might.2\n",
    "\n",
    "def loss(params, x, y, x_mask, y_mask, yx_mask, x_indices, y_indices, key, train):  # inputs: batch_size x seq_len\n",
    "    y_in = y[:, :-1]\n",
    "    y_out = y[:, 1:]\n",
    "    \n",
    "    # TODO: write it without copying memory? is it possible? \n",
    "    logits = batched_forward_aiayn(params, x, y_in, x_mask, y_mask, yx_mask, x_indices, y_indices, key, train) \n",
    "    loss_val, tokens_count = avg_cross_entropy_loss(y_out, logits)\n",
    "    acc = accuracy(y_out, logits) # TODO: Do I need to stop_gradient on this? I think not, but double-check\n",
    "    return loss_val, (loss_val, acc, tokens_count/jnp.size(y_out)) # TODO: this is wrapping, but we could make use of jax.value_and_grad instead\n",
    "\n",
    "loss_train = partial(loss, train=True)\n",
    "loss_eval = jit(partial(loss, key=random.PRNGKey(0), train=False))\n",
    "\n",
    "grad_loss = jit(grad(loss_train, has_aux=True))\n",
    "#grad_loss = grad(loss_train, has_aux=True)\n",
    "\n",
    "#print(f'iter #{i} loss {loss_train(params, jnp.array(x[:1]), jnp.array(y[:1]), random.PRNGKey(0))[0] }')\n",
    "\n",
    "#with jax.disable_jit():\n",
    "#print(f'iter #{i} loss {predict(params, jnp.array(x[:2], 50)) }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30567a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Optimizers\n",
    "\n",
    "# TODO: any call to this function can be replaced by jax's tree_map\n",
    "def elwise(params_and_grads, func): # generically applying func element-wise\n",
    "    return [ [ func(*p_and_g) for p_and_g in zip(*p_and_g_items)] for p_and_g_items in zip(*params_and_grads)]\n",
    "\n",
    "def sgd(params, grads, lr):\n",
    "    return elwise((params, grads), lambda p,g: p - lr * g)\n",
    "\n",
    "@jit\n",
    "def adam(params, grads, lr, betas, epsilon, moments, i):\n",
    "    t = i + 1 # TODO: should we decuple iteration from t, and threading t instead?\n",
    "    moments = [elwise((moment, grads), lambda m, g: b*m + (1-b) * pow(g, pow_g)) for b, moment, pow_g in zip(betas, moments, [1,2])]\n",
    "    bias_corrected_moments = [elwise((moment,), lambda m: m / (1 - pow(b,t))) for b, moment in zip(betas, moments)]\n",
    "    params = elwise((params, *bias_corrected_moments), lambda p,m,v: p - lr * m / (jnp.sqrt(v) + epsilon))\n",
    "    return params, moments\n",
    "\n",
    "from functools import partial\n",
    "@partial(jax.jit, donate_argnames=(\"params\",\"moments\"))\n",
    "def adam_in_place(params, grads, lr, betas, epsilon, moments, i):\n",
    "    t = i + 1 # TODO: should we decuple iteration from t, and threading t instead?\n",
    "\n",
    "    # TODO: once write it more effiently, combine both loops + vmap (if possible)?\n",
    "    \n",
    "    # update moments\n",
    "    for b, moment, pow_g in zip(betas, moments, [1,2]): \n",
    "        for grp_i in range(len(grads)):\n",
    "            for p_i in range(len(grads[grp_i])):\n",
    "                moment[grp_i][p_i] = moment[grp_i][p_i].at[:].multiply(b)\n",
    "                moment[grp_i][p_i] = moment[grp_i][p_i].at[:].add((1-b) * pow(grads[grp_i][p_i], pow_g))\n",
    "\n",
    "    # update grads\n",
    "    for grp_i in range(len(grads)):\n",
    "        for p_i in range(len(grads[grp_i])):\n",
    "            bias_correct_func = lambda b, m: m / (1 - pow(b,t))\n",
    "            m = bias_correct_func(betas[0], moments[0][grp_i][p_i])\n",
    "            v = bias_correct_func(betas[1], moments[1][grp_i][p_i])\n",
    "            params[grp_i][p_i] =  params[grp_i][p_i].at[:].add(-lr * m / (jnp.sqrt(v) + epsilon))\n",
    "            \n",
    "    return params, moments\n",
    "\n",
    "# Testing Adam in place:\n",
    "#original_pointer = params[0][0].unsafe_buffer_pointer()\n",
    "#params, moments = adam_in_place(params, grads, lr, betas, epsilon, moments, i)\n",
    "#assert params[0][0].unsafe_buffer_pointer() == original_pointer # will not fail\n",
    "#params, moments = adam(params, grads, lr, betas, epsilon, moments, i)\n",
    "#assert params[0][0].unsafe_buffer_pointer() == original_pointer # will fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1539026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "2024-10-30 17:30:11.984962: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730309412.003799     961 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730309412.009616     961 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmkukla\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/efs/notebooks/mkukla/pre-tjax/wandb/run-20241030_173014-pnqpcozk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mkukla/t/runs/pnqpcozk' target=\"_blank\">mischievous-poltergeist-1</a></strong> to <a href='https://wandb.ai/mkukla/t' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mkukla/t' target=\"_blank\">https://wandb.ai/mkukla/t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mkukla/t/runs/pnqpcozk' target=\"_blank\">https://wandb.ai/mkukla/t/runs/pnqpcozk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mkukla/t/runs/pnqpcozk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f95ebb3add0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Infra utils\n",
    "def print_mem_stats():\n",
    "    mem_stats = jax.devices()[0].memory_stats()\n",
    "    conv = lambda k: mem_stats[k] / pow(1000,3)\n",
    "    print(f'GB in use: {conv(\"bytes_in_use\")}. GB limit: {conv(\"bytes_limit\")}')\n",
    "\n",
    "import wandb\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"t\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    #config={\n",
    "    #\"learning_rate\": 0.02,\n",
    "    #\"architecture\": \"CNN\",\n",
    "    #\"dataset\": \"CIFAR-100\",\n",
    "    #\"epochs\": 10,\n",
    "    #}\n",
    "    sync_tensorboard=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef0c359e-7099-400a-98fb-466f98b52f29",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /lego/storage/output/runs/Oct30_17-30-15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params: 63883425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4000/800000 [45:53<152:12:23,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows. \tY: manchmal nicht, denn im extremfall führt sie dazu, dass menschen extreme körperliche risiken auf sich nehmen und dem glücksspiel oder drogen verfallen, um sie zu lindern - das haben studien gezeigt.  \tPREDS: es ist die von der europäischen union und die sich die die die sich in der europäischen union und die sich in der europäischen union und die sich in der europäischen union zu zu zu zu zu und und und und und und und und und und und und \n",
      "\n",
      "X:the city is mentioned in ancient greek and ancient egyptian legends. \tY: die stadt wird in altgriechischen und altägyptischen legenden erwähnt.  \tPREDS: die erste und die die die die und und die und und und und und und to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to \n",
      "\n",
      "X:the resolution caused lively public debate. \tY: diese entscheidung rief in der öffentlichkeit eine lebhafte diskussion hervor.  \tPREDS: die aussprache ist die kommission der europäischen union zu zu zu zu zu zu to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to \n",
      "\n",
      "X:we want to look 5-10 years into the future and predict cardinal changes in this interaction. \tY: wir jedoch wollen 5-10 jahre vorausblicken und die radikalen veränderungen dieser interaktion vorhersehen.  \tPREDS: wir haben wir wir wir die die die die die die die die die die die die die die to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4eb8d80f4ab4dd8b7d264253106a5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9e77bf88c34f8d8ff87154f7a92102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e61a67e5ed4408860298303a05816a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4730/800000 [55:15<154:51:09,  1.43it/s]\n",
      "  1%|          | 9460/800000 [54:51<152:49:09,  1.44it/s]\n",
      "  1%|▏         | 10997/800000 [17:31<149:52:50,  1.46it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 10%|█         | 80000/800000 [50:07<139:12:57,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows. \tY: manchmal nicht, denn im extremfall führt sie dazu, dass menschen extreme körperliche risiken auf sich nehmen und dem glücksspiel oder drogen verfallen, um sie zu lindern - das haben studien gezeigt.  \tPREDS: manchmal nein, denn im äußersten sinne kann es dazu führen, dass die menschen sich selbst unsinnigen physischen risiken aussetzen, sich in form von missbrauch versetzen oder sich mit inhalten begnüerererererererererererer\n",
      "\n",
      "X:the city is mentioned in ancient greek and ancient egyptian legends. \tY: die stadt wird in altgriechischen und altägyptischen legenden erwähnt.  \tPREDS: die stadt wird in alten griechischen und alten ägyptischen legenden erwähnt. \n",
      "\n",
      "X:the resolution caused lively public debate. \tY: diese entscheidung rief in der öffentlichkeit eine lebhafte diskussion hervor.  \tPREDS: die entschließung hat eine lebhafte öffentliche debatte verursacht. \n",
      "\n",
      "X:we want to look 5-10 years into the future and predict cardinal changes in this interaction. \tY: wir jedoch wollen 5-10 jahre vorausblicken und die radikalen veränderungen dieser interaktion vorhersehen.  \tPREDS: wir wollen 5-10 jahre in die zukunft blicken und grundlegende veränderungen in dieser interaktion vorhersagen. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 80410/800000 [55:41<141:11:45,  1.42it/s]\n",
      " 10%|█         | 81788/800000 [15:40<136:12:01,  1.46it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 12%|█▏        | 92000/800000 [24:18<134:40:44,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows. \tY: manchmal nicht, denn im extremfall führt sie dazu, dass menschen extreme körperliche risiken auf sich nehmen und dem glücksspiel oder drogen verfallen, um sie zu lindern - das haben studien gezeigt.  \tPREDS: manchmal nein, weil es im extremfall dazu führen kann, dass die menschen absurde physische risiken eingehen, sich in missbrauch versetzen oder sich in substanz verdampfen lassen, wie es die forschung zeigt. eineineineineineineineineineineinein\n",
      "\n",
      "X:the city is mentioned in ancient greek and ancient egyptian legends. \tY: die stadt wird in altgriechischen und altägyptischen legenden erwähnt.  \tPREDS: die stadt wird in den alten griechischen und alten ägyptischen legenden erwähnt. \n",
      "\n",
      "X:the resolution caused lively public debate. \tY: diese entscheidung rief in der öffentlichkeit eine lebhafte diskussion hervor.  \tPREDS: die entschließung hat eine lebhafte öffentliche debatte verursacht. \n",
      "\n",
      "X:we want to look 5-10 years into the future and predict cardinal changes in this interaction. \tY: wir jedoch wollen 5-10 jahre vorausblicken und die radikalen veränderungen dieser interaktion vorhersehen.  \tPREDS: wir wollen in die zukunft 5-10 jahre blicken und cardinal changes in dieser interaktion voraussagen. marmarmarmarmarmarmarmarmarmarmarmarmarmarmarmarmarmarmarmarmarmarmarmarmarmarmarmarmarmar\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 94600/800000 [54:48<136:13:55,  1.44it/s]\n",
      " 12%|█▏        | 96000/800000 [15:58<133:55:12,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows. \tY: manchmal nicht, denn im extremfall führt sie dazu, dass menschen extreme körperliche risiken auf sich nehmen und dem glücksspiel oder drogen verfallen, um sie zu lindern - das haben studien gezeigt.  \tPREDS: manchmal nein, denn auf der extremen ebene kann es dazu führen, dass die menschen absurde physische risiken nehmen, sich in den besitz von missbrauch versetzen oder sich in den inhalten einmischen, wie bedarbedarbedarbedarbedarbedarbedarbedarbedarbedarbedarbedar\n",
      "\n",
      "X:the city is mentioned in ancient greek and ancient egyptian legends. \tY: die stadt wird in altgriechischen und altägyptischen legenden erwähnt.  \tPREDS: die stadt wird in alten griechischen und alten ägyptischen legenden erwähnt. \n",
      "\n",
      "X:the resolution caused lively public debate. \tY: diese entscheidung rief in der öffentlichkeit eine lebhafte diskussion hervor.  \tPREDS: die entschließung hat eine lebhafte öffentliche debatte ausgelöst. \n",
      "\n",
      "X:we want to look 5-10 years into the future and predict cardinal changes in this interaction. \tY: wir jedoch wollen 5-10 jahre vorausblicken und die radikalen veränderungen dieser interaktion vorhersehen.  \tPREDS: wir wollen in die zukunft blicken und die kardinalveränderungen in dieser interaktion voraussagen. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 99330/800000 [54:45<135:12:43,  1.44it/s]\n",
      " 12%|█▎        | 100000/800000 [07:40<133:30:16,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:sometimes no, because in its extreme it can lead people to take absurd physical risks, gamble or indulge in substance abuse as a way to ease it, research shows. \tY: manchmal nicht, denn im extremfall führt sie dazu, dass menschen extreme körperliche risiken auf sich nehmen und dem glücksspiel oder drogen verfallen, um sie zu lindern - das haben studien gezeigt.  \tPREDS: manchmal nein, denn auf seiner oberfläche kann es dazu führen, dass die menschen absurde physische risiken nehmen, sich in den griff begeben oder sich in den drogenkonsum einmischen, um es zu obobobobobobobobobobobob\n",
      "\n",
      "X:the city is mentioned in ancient greek and ancient egyptian legends. \tY: die stadt wird in altgriechischen und altägyptischen legenden erwähnt.  \tPREDS: die stadt wird in den antiken griechischen und alten ägyptischen legenden erwähnt. \n",
      "\n",
      "X:the resolution caused lively public debate. \tY: diese entscheidung rief in der öffentlichkeit eine lebhafte diskussion hervor.  \tPREDS: die entschließung hat eine lebhafte öffentliche debatte ausgelöst. \n",
      "\n",
      "X:we want to look 5-10 years into the future and predict cardinal changes in this interaction. \tY: wir jedoch wollen 5-10 jahre vorausblicken und die radikalen veränderungen dieser interaktion vorhersehen.  \tPREDS: wir wollen 5-10 jahre in die zukunft blicken und prognostizieren kardinalveränderungen in dieser obobobobobobobobobobobobobobobobobobobobobobobobobobobobobob\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 101750/800000 [28:28<136:56:56,  1.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m acc_grads, grad_loss_rest\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m#for _, batch in tqdm(enumerate(itertools.islice(get_batched_examples(ds, batch_size, seq_len, START_TOK, END_TOK, skip_n_rows = ds_train_rows_read), num_steps)), initial=i, total=num_steps, smoothing=0):\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mislice(get_batched_examples_packed(ds, batch_size, seq_len, START_TOK, END_TOK, pack_frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m, skip_n_rows \u001b[38;5;241m=\u001b[39m ds_train_rows_read), num_steps)), initial\u001b[38;5;241m=\u001b[39mi, total\u001b[38;5;241m=\u001b[39mnum_steps, smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     64\u001b[0m         x, y, x_mask, y_mask, yx_mask, x_indices, y_indices \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;66;03m# TODO: introduce update func, which does grad_loss and adam, and then call/jit that function instead of calling/jitting two separate ones\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/efs/notebooks/mkukla/pre-tjax/tokenized_dataset.py:158\u001b[0m, in \u001b[0;36mget_batched_examples_packed\u001b[0;34m(ds, batch_size, seq_len, start_tok, end_tok, pack_frac, split, skip_n_rows)\u001b[0m\n\u001b[1;32m    155\u001b[0m     batch_y_lens\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;28mlen\u001b[39m(item_y)])\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m batch_size:\n\u001b[0;32m--> 158\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [convert_batch_item(\u001b[38;5;241m*\u001b[39mit, seq_len, x_lens, y_lens) \u001b[38;5;28;01mfor\u001b[39;00m it, x_lens, y_lens \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch, batch_x_lens, batch_y_lens)]\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m pack_batch(batch)\n\u001b[1;32m    161\u001b[0m     batch \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/efs/notebooks/mkukla/pre-tjax/tokenized_dataset.py:158\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    155\u001b[0m     batch_y_lens\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;28mlen\u001b[39m(item_y)])\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m batch_size:\n\u001b[0;32m--> 158\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [\u001b[43mconvert_batch_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_lens\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m it, x_lens, y_lens \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch, batch_x_lens, batch_y_lens)]\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m pack_batch(batch)\n\u001b[1;32m    161\u001b[0m     batch \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/efs/notebooks/mkukla/pre-tjax/tokenized_dataset.py:121\u001b[0m, in \u001b[0;36mconvert_batch_item\u001b[0;34m(x, y, seq_len, x_lens, y_lens)\u001b[0m\n\u001b[1;32m    118\u001b[0m     x_packs, y_packs, yx_packs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     x_indices, y_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(seq_len), np\u001b[38;5;241m.\u001b[39marange(seq_len)\n\u001b[0;32m--> 121\u001b[0m x_mask, y_mask, yx_mask \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_packs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_packs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myx_packs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (x, y_plus_one, x_mask, y_mask, yx_mask, x_indices, y_indices)\n",
      "File \u001b[0;32m/efs/notebooks/mkukla/pre-tjax/tokenized_dataset.py:52\u001b[0m, in \u001b[0;36mbuild_masks\u001b[0;34m(x, y, x_packs, y_packs, yx_packs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Mask padded y tokens + add \"autoregressive\" mask\u001b[39;00m\n\u001b[1;32m     51\u001b[0m y_pad_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m, np\u001b[38;5;241m.\u001b[39mones((y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m y_mask \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtri\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m y_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply(np\u001b[38;5;241m.\u001b[39mmultiply(y_mask, y_pad_mask), y_pad_mask[:, \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_packs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Training loop\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import itertools\n",
    "import pickle\n",
    "import evaluate\n",
    "\n",
    "# Infra training params\n",
    "run_name = datetime.datetime.now().strftime(\"%h%d_%H-%M-%S\")\n",
    "# Since implementation of gradient accumulation is very primitve, we need logging & checkpoint steps params\n",
    "# to be multiplication of gradient_accumulations_steps. \n",
    "# TODO: Introduce effective step (conditioned on accumulation steps), and do logging/checkpoint in respect to effective  step\n",
    "log_every_steps = 16\n",
    "eval_every_steps = 4000 #500 * 8 machines\n",
    "eval_n_examples = 4\n",
    "writer = SummaryWriter(f'/lego/storage/output/runs/{run_name}')\n",
    "#checkpoint_every_steps = None #500 * 8 machines\n",
    "checkpoint_every_steps = 4000 #20000\n",
    "resume_from_checkpoint = None\n",
    "#resume_from_checkpoint = 'runs/Jun07_10-12-10/checkpoint_4000.pkl' # TODO: Confirm runs from checkpoints are still fully reproducible\n",
    "\n",
    "\n",
    "# ML training params\n",
    "key_training = random.PRNGKey(0) \n",
    "batch_size= 512 #416 # TODO: Investigate OOMs when 496? #512\n",
    "gradient_accumulations_steps = 8 # to imitate paper's 8 devices\n",
    "num_steps = 800000 # paper's 100k steps *  8 devices\n",
    "lr = 0.001 # Effectively ignored if lr scheduler is used (i.e. warmup_steps is set to something else than None)\n",
    "warmup_steps= 4000\n",
    "betas = (0.9, 0.98) \n",
    "epsilon = 10e-9\n",
    "moments = [elwise((params,), lambda p: jnp.zeros_like(p)) for _ in range(2)] # moment esimtates\n",
    "seq_len = 50 # TODO: 124 is maximum length in validation dataset. \n",
    "x_tokens_per_batch = 15000 #For variable batch len, we don't use it as we can fit less data (paper does 25k)\n",
    "\n",
    "x_eval, y_eval, x_eval_mask, y_eval_mask, yx_eval_mask, x_eval_indices, y_eval_indices  = next(get_batched_examples(ds, eval_n_examples, seq_len, START_TOK, END_TOK, \"validation\")) \n",
    "\n",
    "i = 0 \n",
    "ds_train_rows_read = 0\n",
    "if resume_from_checkpoint is not None:\n",
    "    with open(resume_from_checkpoint,'rb') as f:\n",
    "        i, ds_train_rows_read, params, moments, key_training = pickle.load(f)   \n",
    "        print(f'Resuming training from the checkpoint: i {i} ds_train_rows_read {ds_train_rows_read}')\n",
    "\n",
    "num_params = sum([jnp.size(p_leaf) for p_leaf in jax.tree_util.tree_leaves(params)])\n",
    "print(f'Number of params: {num_params}')\n",
    "grads = jax.tree_util.tree_map(lambda x: jnp.zeros_like(x), params)\n",
    "\n",
    "from functools import partial\n",
    "@partial(jax.jit, donate_argnames=(\"acc_grads\"))\n",
    "def acc_grad_loss(acc_grads, params, x, y, x_mask, y_mask, yx_mask, x_indices, y_indices, key_iter):\n",
    "    i_step_grads, grad_loss_rest = grad_loss(params, x, y, x_mask, y_mask, yx_mask, x_indices, y_indices, key_iter)\n",
    "    \n",
    "    for grp_i in range(len(acc_grads)):\n",
    "        for p_i in range(len(acc_grads[grp_i])):\n",
    "            acc_grads[grp_i][p_i] =  acc_grads[grp_i][p_i].at[:].add(i_step_grads[grp_i][p_i])\n",
    "            \n",
    "    return acc_grads, grad_loss_rest\n",
    "\n",
    "while True:\n",
    "    #for _, batch in tqdm(enumerate(itertools.islice(get_batched_examples(ds, batch_size, seq_len, START_TOK, END_TOK, skip_n_rows = ds_train_rows_read), num_steps)), initial=i, total=num_steps, smoothing=0):\n",
    "    for _, batch in tqdm(enumerate(itertools.islice(get_batched_examples_packed(ds, batch_size, seq_len, START_TOK, END_TOK, pack_frac=0.75, skip_n_rows = ds_train_rows_read), num_steps)), initial=i, total=num_steps, smoothing=0):\n",
    "        x, y, x_mask, y_mask, yx_mask, x_indices, y_indices = batch\n",
    "        # Training step\n",
    "        # TODO: introduce update func, which does grad_loss and adam, and then call/jit that function instead of calling/jitting two separate ones\n",
    "        key_training, key_iter = random.split(key_training, 2)\n",
    "        grads, (loss_val, acc, _) = acc_grad_loss(grads, params, jnp.array(x), jnp.array(y), jnp.array(x_mask), jnp.array(y_mask), jnp.array(yx_mask), jnp.array(x_indices), jnp.array(y_indices), key_iter)\n",
    "        #grads, (loss_val, acc) = grad_loss(params, jnp.array(x), jnp.array(y), key_iter)\n",
    "        \n",
    "        #params = sgd(params, grads, lr)\n",
    "        if warmup_steps is not None:\n",
    "            i_multidevice = i // gradient_accumulations_steps\n",
    "            lr = pow(EMB_DIM, -0.5) * min(pow((i_multidevice+1), -0.5), (i_multidevice+1) * pow(warmup_steps, -1.5))\n",
    "\n",
    "        if i > 0 and i % gradient_accumulations_steps == 0:\n",
    "            for grp_i in range(len(grads)):\n",
    "                for p_i in range(len(grads[grp_i])):\n",
    "                    grads[grp_i][p_i] =  grads[grp_i][p_i].at[:].divide(gradient_accumulations_steps)\n",
    "            \n",
    "            #params, moments = adam(params, grads, lr, betas, epsilon, moments, i)\n",
    "            params, moments = adam_in_place(params, grads, lr, betas, epsilon, moments, i)\n",
    "    \n",
    "        # Logging:\n",
    "        if i%log_every_steps==0:\n",
    "            loss_val = loss_val.item()\n",
    "            acc = acc.item()\n",
    "            \n",
    "            import math\n",
    "            #@jit # TODO: I can't jit that one\n",
    "            def g_l2norm_squared(g_list):\n",
    "                return pow(jnp.linalg.norm(g_list),2)\n",
    "            def l2norm(grads): # computing l2norm without any memory copies\n",
    "                return math.sqrt(sum([ sum([g_l2norm_squared(g) for g in g_items]) for g_items in grads]))\n",
    "            def grps_l2norms(grads):\n",
    "                return [ math.sqrt(sum([g_l2norm_squared(g) for g in g_items])) for g_items in grads]\n",
    "            grad_norm = l2norm(grads)\n",
    "            grps_grad_norms = grps_l2norms(grads)\n",
    "\n",
    "            \n",
    "            #print(f'iter #{i} loss {loss_val} acc {acc} lr {lr} grad_norm {grad_norm}')\n",
    "            #print_mem_stats() # TODO: monitor it in tensorboard?\n",
    "            writer.add_scalar('train/loss', loss_val, i)\n",
    "            writer.add_scalar('train/acc', acc, i)\n",
    "            writer.add_scalar('train/lr', lr, i)\n",
    "            writer.add_scalar('train/grad_norm', grad_norm, i)\n",
    "            for grp_i, grp_grad_norm in enumerate(grps_grad_norms):\n",
    "                writer.add_scalar(f'train_details/grad_norm_grp_{grp_i}', grp_grad_norm, i)\n",
    "\n",
    "            # TODO: some metrics computed on x, other on y. Make it consistent\n",
    "            #pad_tokens_prop = sum([y_row.count(0) for y_row in y]) / sum([len(y_row) for y_row in y])\n",
    "            import numpy as np\n",
    "            pad_tokens_prop = np.count_nonzero(y==0) / y.size\n",
    "            writer.add_scalar('train_data/pad_tokens_prop', pad_tokens_prop, i)\n",
    "            writer.add_scalar('train_data/batch_size', len(x), i)\n",
    "            writer.add_scalar('train_data/batch_seq_len', len(x[0]), i)\n",
    "            writer.add_scalar('train_data/batch_total_tokens', len(x) * len(x[0]), i)\n",
    "\n",
    "        # Zeroed accumulated grads: we have to do it after computing grad norms\n",
    "        if i > 0 and i % gradient_accumulations_steps == 0: \n",
    "            for grp_i in range(len(grads)):\n",
    "                for p_i in range(len(grads[grp_i])):\n",
    "                    grads[grp_i][p_i] =  grads[grp_i][p_i].at[:].set(0)\n",
    "            \n",
    "        # Evaluation\n",
    "        if i>0 and i%eval_every_steps==0:\n",
    "            val_losses = []\n",
    "            val_accs = []\n",
    "            val_toks_props = []\n",
    "            for eval_step, batch in enumerate(get_batched_examples(ds, batch_size, seq_len, START_TOK, END_TOK, split=\"validation\")): \n",
    "                x, y, x_mask, y_mask, yx_mask, x_indices, y_indices = batch\n",
    "                _, (loss_val, acc, toks_prop) = loss_eval(params, jnp.array(x), jnp.array(y), jnp.array(x_mask), jnp.array(y_mask), jnp.array(yx_mask), jnp.array(x_indices), jnp.array(y_indices))\n",
    "                val_losses.append(loss_val)\n",
    "                val_accs.append(acc)\n",
    "                val_toks_props.append(toks_prop)\n",
    "            writer.add_scalar('eval/loss', jnp.average(jnp.hstack(val_losses), weights = jnp.hstack(val_toks_props)).item(), i)\n",
    "            writer.add_scalar('eval/acc', jnp.average(jnp.hstack(val_accs), weights = jnp.hstack(val_toks_props)).item(), i)\n",
    "            \n",
    "            # Few predictions\n",
    "            y_sample = predict(params, jnp.array(x_eval), jnp.array(x_eval_mask), jnp.array(y_eval_mask), jnp.array(yx_eval_mask), jnp.array(x_eval_indices), jnp.array(y_eval_indices), seq_len)\n",
    "            y_sample = tuple([item.tolist() for item in y_sample])\n",
    "            def detokenize_y_in(y):\n",
    "                y_out = y[:, 1:]\n",
    "                y_out[y_out == END_TOK] = 0\n",
    "                return detokenize(y_out)\n",
    "            for detokenized_x_eval, detokenized_y_eval, detokenized_y_sample in zip(detokenize(x_eval), detokenize_y_in(y_eval), detokenize(y_sample)):\n",
    "                print(f'X:{detokenized_x_eval}\\tY: {detokenized_y_eval} \\tPREDS: {detokenized_y_sample}\\n')\n",
    "\n",
    "            # Compute BLEU score (it takes around around half a minute)\n",
    "            references = [] \n",
    "            predictions = []\n",
    "            for _, batch in enumerate(get_batched_examples(ds, batch_size, seq_len, START_TOK, END_TOK, split=\"validation\")): \n",
    "                x, y, x_mask, y_mask, yx_mask, x_indices, y_indices = batch\n",
    "                y_sample = predict(params, jnp.array(x), jnp.array(x_mask), jnp.array(y_mask), jnp.array(yx_mask), jnp.array(x_indices), jnp.array(y_indices), seq_len)\n",
    "                y_sample = tuple([item.tolist() for item in y_sample])\n",
    "\n",
    "                for detokenized_x, detokenized_y, detokenized_y_sample in zip(detokenize(x), detokenize_y_in(y), detokenize(y_sample)):\n",
    "                    references.append(detokenized_y)\n",
    "                    predictions.append(detokenized_y_sample)\n",
    "            \n",
    "            bleu = evaluate.load(\"bleu\")\n",
    "            try: # HotFix: For small eval_steps, it's possible to get ZeroDivisionError\n",
    "                results = bleu.compute(predictions=predictions, references=references)\n",
    "            except ZeroDivisionError:\n",
    "                results = {\"bleu\": -1, \"precisions\": [-1, -1, -1, 1]}\n",
    "            writer.add_scalar('eval/bleu', results['bleu'], i)\n",
    "            writer.add_scalar('eval/bleu.precision0', results['precisions'][0], i)\n",
    "            writer.add_scalar('eval/bleu.precision1', results['precisions'][1], i)\n",
    "            writer.add_scalar('eval/bleu.precision2', results['precisions'][2], i)\n",
    "            writer.add_scalar('eval/bleu.precision3', results['precisions'][3], i)\n",
    "                \n",
    "        i = i + 1\n",
    "        ds_train_rows_read = ds_train_rows_read + len(x)\n",
    "\n",
    "        # Checkpointing (i, ds_train_rows_read, params, moments).\n",
    "        if checkpoint_every_steps is not None and (i>0 and i%checkpoint_every_steps==0):\n",
    "            import os\n",
    "            training_state = (i, ds_train_rows_read, params, moments, key_training)\n",
    "            filename = f'runs/{run_name}/checkpoint_{i}.pkl'\n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump(training_state, f)\n",
    "\n",
    "        if i> num_steps:\n",
    "            break\n",
    "                \n",
    "    ds_train_rows_read=0 # After each epoch, reset dataset pointer\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42440b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Final test predictions + BLEU computation\n",
    "print(f'Few predictions for validation dataset')\n",
    "y_sample = predict(params, jnp.array(x_eval))\n",
    "y_sample = tuple([item.tolist() for item in y_sample])\n",
    "for detekonized_x_eval, detokenized_y_eval, detokenized_y_sample in zip(detokenize(x_eval), detokenize(y_eval), detokenize(y_sample)):\n",
    "    print(f'X:{detekonized_x_eval}\\tY: {detokenized_y_eval} \\tPREDS: {detokenized_y_sample}\\n')\n",
    "    references.append(detokenized_y_eval)\n",
    "    predictions.append(detokenized_y_sample)\n",
    "\n",
    "print(f'Computing BLEU for validation dataset')\n",
    "import evaluate\n",
    "references = [] \n",
    "predictions = []\n",
    "for _, (x, y) in tqdm(enumerate(get_batched_examples_per_length(ds, x_tokens_per_batch, split=\"validation\"))):\n",
    "    y_sample = predict(params, jnp.array(x), seq_len)\n",
    "    y_sample = tuple([item.tolist() for item in y_sample])\n",
    "    for detekonized_x_eval, detokenized_y_eval, detokenized_y_sample in zip(detokenize(x), detokenize(y), detokenize(y_sample)):\n",
    "        references.append(detokenized_y_eval)\n",
    "        predictions.append(detokenized_y_sample)\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
