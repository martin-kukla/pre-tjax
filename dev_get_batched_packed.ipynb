{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aa347269-333d-45f1-8165-cc15f76c2adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Loading tokenizer bpe_tokenizer_ds_train_all_merges_35k.pickle\n",
      "Tokenizing dataset\n",
      "[[23954 12114  3613  3586 12466     0     0     0     0     0]\n",
      " [ 3667 21876 28867  3586 12466  3613  3586  3960  4795  3751]\n",
      " [ 3585 15064  3626  3829  3850  3827 31442  3586  4489 11945]\n",
      " [ 3829  3827 15275  3612  6044  3599  3745  7010  3594  3586]\n",
      " [ 3594  3586 32712  3667  4156  4239  3617 20984  3612  6958]\n",
      " [ 5895  4535  3788 11576  3687  3745  6958  3655  4435  3571]\n",
      " [ 9339  5544 19234  3614 23218  3612  6958  3655  4435  3571]\n",
      " [ 6698  4468  3599  3612  5318  3613 16215     0     0     0]\n",
      " [ 3829  3850  3743  8528  3857  3586  6314  3614 13569  3686]\n",
      " [ 3943  3613  3586  4641 31869  4017  4043  9155  3594 22023]]\n",
      "[[31352  3606 29833     0     0     0     0     0     0     0]\n",
      " [ 3638 37931  3610  3979 36357  3787 15348  9617 19088 27915]\n",
      " [ 3869  3725 26275 24456  3732  3606  3619 23742  7960    35]\n",
      " [ 3755  4794  6502  3606 10816  4052  3884  7320  3755 16943]\n",
      " [ 4910  4547  3638  3725 17257  3762  3700  3732  3837  3606]\n",
      " [ 3638  5897  7644  3835  3683  3884  6005 34029  9844  3683]\n",
      " [16600  4794 35212  3835  3683  3884  6005 34029  6958  3655]\n",
      " [ 4888  8262  3992  6465 21881     0     0     0     0     0]\n",
      " [ 3869  3725  8473  3949  3606 19176  3618  3787 19965  7419]\n",
      " [ 3683  3644  6932  3629  3609 12649 37717  3610  3605  3594]]\n"
     ]
    }
   ],
   "source": [
    "from tokenized_dataset import load_tokenized_dataset\n",
    "import numpy as np\n",
    "\n",
    "def build_masks(x, y):\n",
    "    # Mask padded x tokens\n",
    "    x_mask = np.ones((x.shape[0], x.shape[0]))\n",
    "    x_pad_mask = np.where(x != 0, np.ones((x.shape[0])), 0)\n",
    "    x_mask = np.multiply(np.multiply(x_mask, x_pad_mask), x_pad_mask[:, None])\n",
    "\n",
    "    # Mask padded y tokens + add \"autoregressive\" mask\n",
    "    y_pad_mask = np.where(y != 0, np.ones((y.shape[0])), 0)\n",
    "    y_mask = np.tri(y.shape[0], y.shape[0])\n",
    "    y_mask = np.multiply(np.multiply(y_mask, y_pad_mask), y_pad_mask[:, None])\n",
    "\n",
    "    # Mask padded yx tokens\n",
    "    yx_mask = np.ones((y.shape[0], x.shape[0]))\n",
    "    yx_mask = np.multiply(np.multiply(yx_mask, x_pad_mask), y_pad_mask[:, None])\n",
    "\n",
    "    return x_mask, y_mask, yx_mask\n",
    "\n",
    "pad_and_trunc = lambda toks, seq_len: (toks + [0] * (seq_len - len(toks)))[:seq_len]\n",
    "pack_batch = lambda batch: [np.stack(el) for el in map(list, zip(*batch))]\n",
    "\n",
    "# Complete batch to batch_size with pad tokens only sequences \n",
    "def complete_last_batch(batch, batch_size):\n",
    "    for _ in range(batch_size - len(batch)):\n",
    "        x = np.zeros(seq_len)\n",
    "        y = np.zeros(seq_len)\n",
    "        x_mask, y_mask, yx_mask = build_masks(x,y)\n",
    "        batch.append((x, y, x_mask, y_mask, yx_mask))\n",
    "    return batch\n",
    "    \n",
    "\n",
    "def get_batched_examples(ds, batch_size, seq_len, split=\"train\", skip_n_rows=None):\n",
    "    ds_split = ds[split].skip(skip_n_rows) if skip_n_rows is not None else ds[split]\n",
    "\n",
    "    batch = []\n",
    "    for item in ds_split:\n",
    "        x = np.array(pad_and_trunc(item['x'], seq_len))\n",
    "        y = np.array(pad_and_trunc(item['y'], seq_len))\n",
    "        x_mask, y_mask, yx_mask = build_masks(x,y)\n",
    "        batch.append((x, y, x_mask, y_mask, yx_mask))\n",
    "        if len(batch) == batch_size:\n",
    "            yield pack_batch(batch)\n",
    "            batch = []\n",
    "    if split!=\"train\" and len(batch) > 0: # Note I don't use last few rows left in train split..\n",
    "        yield pack_batch(complete_last_batch(batch, batch_size))\n",
    "\n",
    "ds, _ = load_tokenized_dataset()\n",
    "\n",
    "x, y, x_mask, y_mask, yx_mask = next(get_batched_examples(ds, 10, 10))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4ac4afff-bf6b-449d-8ab4-bfdd446142df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Loading tokenizer bpe_tokenizer_ds_train_all_merges_35k.pickle\n",
      "Tokenizing dataset\n",
      "50\n",
      "50\n",
      "[[23954 12114  3613  3586 12466 38560 38559  3667 21876 28867  3586 12466\n",
      "   3613  3586  3960  4795  3751  7291  5434  3599 20578  9201 10178 20119\n",
      "   3614  3667  4081  4239  6204  6664  3617  6454  3829  3612  9825  4161\n",
      "   5567  3594  3586  6096  3686  3829 16708  3612 11638  6807  4110 17835\n",
      "      0     0]\n",
      " [ 3585 15064  3626  3829  3850  3827 31442  3586  4489 11945    34 21147\n",
      "  16112  4435 13814  3617  5324  4351  3788  5221  3586  4641  3594  3612\n",
      "   5239  3613  4924 18161  3612  8545  3613  7082 23814  3686 12332  4424\n",
      "   4489  3751 16858     0     0     0     0     0     0     0     0     0\n",
      "      0     0]]\n",
      "[[31352  3606 29833 38560 38559  3638 37931  3610  3979 36357  3787 15348\n",
      "   9617 19088 27915 29833  3730  4157  6131  3704 22472 12444 10706  4521\n",
      "  14141  6282  6715  4083 10585 16869  3618  8898  4213  3725 10838 21975\n",
      "  18117     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]\n",
      " [ 3869  3725 26275 24456  3732  3606  3619 23742  7960    35 14131     8\n",
      "   9865 19186  4106  3815  6759 13498  5189  3988  6035 12318  4905  4878\n",
      "  10775  3692 31364 31731 18394     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]]\n"
     ]
    }
   ],
   "source": [
    "from tokenized_dataset import load_tokenized_dataset\n",
    "import numpy as np\n",
    "\n",
    "def get_batched_examples_packed(ds, batch_size, seq_len, start_tok, end_tok, pack_frac = 0.5, split=\"train\", skip_n_rows=None):\n",
    "    assert split==\"train\"\n",
    "    ds_split = ds[split].skip(skip_n_rows) if skip_n_rows is not None else ds[split]\n",
    "    \n",
    "    batch = []\n",
    "    for item in ds_split:\n",
    "        # Either append to previous batch item or create new one\n",
    "        if len(batch)>0 and (len(batch[-1][0]) < seq_len * pack_frac and len(batch[-1][1]) < seq_len * pack_frac):\n",
    "            pack_func = lambda x, y: x + [end_tok, start_tok] + y\n",
    "            batch[-1] = (pack_func(batch[-1][0], item['x']), pack_func(batch[-1][1], item['y']))\n",
    "        else:\n",
    "            batch.append((item['x'],item['y']))\n",
    "            \n",
    "        if len(batch) == batch_size:\n",
    "            def convert_batch_item(x, y):\n",
    "                x = np.array(pad_and_trunc(x, seq_len))\n",
    "                y = np.array(pad_and_trunc(y, seq_len))\n",
    "                x_mask, y_mask, yx_mask = build_masks(x,y)\n",
    "                return (x, y, x_mask, y_mask, yx_mask)\n",
    "            batch = [convert_batch_item(*it) for it in batch]\n",
    "            print(len(batch[0][0]))\n",
    "            print(len(batch[1][0]))\n",
    "            \n",
    "            yield pack_batch(batch)\n",
    "            batch = []\n",
    "            \n",
    "    if split!=\"train\" and len(batch) > 0: # Note I don't use last few rows left in train split..\n",
    "        yield pack_batch(complete_last_batch(batch, batch_size))\n",
    "\n",
    "ds, (_, _, tokenizer_vocab_size) = load_tokenized_dataset()\n",
    "START_TOK = tokenizer_vocab_size + 1\n",
    "END_TOK = tokenizer_vocab_size + 2\n",
    "\n",
    "x, y, x_mask, y_mask, yx_mask = next(get_batched_examples_packed(ds, 2, 50, START_TOK, END_TOK, pack_frac=0.75))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b09ee-c1ac-4592-afda-1689b6641c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
