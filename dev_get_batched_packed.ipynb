{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa347269-333d-45f1-8165-cc15f76c2adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Loading tokenizer bpe_tokenizer_ds_train_all_merges_35k.pickle\n",
      "Tokenizing dataset\n",
      "[[23954 12114  3613  3586 12466     0     0     0     0     0]\n",
      " [ 3667 21876 28867  3586 12466  3613  3586  3960  4795  3751]]\n",
      "[[31352  3606 29833     0     0     0     0     0     0     0]\n",
      " [ 3638 37931  3610  3979 36357  3787 15348  9617 19088 27915]]\n"
     ]
    }
   ],
   "source": [
    "from tokenized_dataset import load_tokenized_dataset\n",
    "import numpy as np\n",
    "\n",
    "def build_masks(x, y, x_packs=None, y_packs=None, yx_packs=None): # x: seq_len, y: seq_len\n",
    "    # Mask padded x tokens\n",
    "    x_mask = np.ones((x.shape[0], x.shape[0]))\n",
    "    x_pad_mask = np.where(x != 0, np.ones((x.shape[0])), 0)\n",
    "    x_mask = np.multiply(np.multiply(x_mask, x_pad_mask), x_pad_mask[:, None])\n",
    "    if x_packs is not None:\n",
    "        x_mask = np.multiply(x_mask, x_packs)\n",
    "\n",
    "    # Mask padded y tokens + add \"autoregressive\" mask\n",
    "    y_pad_mask = np.where(y != 0, np.ones((y.shape[0])), 0)\n",
    "    y_mask = np.tri(y.shape[0], y.shape[0])\n",
    "    y_mask = np.multiply(np.multiply(y_mask, y_pad_mask), y_pad_mask[:, None])\n",
    "    if y_packs is not None:\n",
    "        y_mask = np.multiply(y_mask, y_packs)\n",
    "\n",
    "    # Mask padded yx tokens\n",
    "    yx_mask = np.ones((y.shape[0], x.shape[0]))\n",
    "    yx_mask = np.multiply(np.multiply(yx_mask, x_pad_mask), y_pad_mask[:, None])\n",
    "    if yx_packs is not None:\n",
    "        yx_mask = np.multiply(yx_mask, yx_packs)\n",
    "\n",
    "    return x_mask, y_mask, yx_mask\n",
    "\n",
    "pad_and_trunc = lambda toks, seq_len: (toks + [0] * (seq_len - len(toks)))[:seq_len]\n",
    "pack_batch = lambda batch: [np.stack(el) for el in map(list, zip(*batch))]\n",
    "\n",
    "# Complete batch to batch_size with pad tokens only sequences \n",
    "def complete_last_batch(batch, batch_size):\n",
    "    for _ in range(batch_size - len(batch)):\n",
    "        x = np.zeros(seq_len)\n",
    "        y = np.zeros(seq_len)\n",
    "        x_mask, y_mask, yx_mask = build_masks(x,y)\n",
    "        batch.append((x, y, x_mask, y_mask, yx_mask))\n",
    "    return batch\n",
    "\n",
    "def ones_block_diag(lens_dim0, lens_dim1):\n",
    "    result = np.zeros((sum(lens_dim0), sum(lens_dim1)))\n",
    "    start_ind_dim0 = 0\n",
    "    start_ind_dim1 = 0\n",
    "    for len_dim0, len_dim1 in zip(lens_dim0, lens_dim1):\n",
    "        result[start_ind_dim0:start_ind_dim0+len_dim0, start_ind_dim1:start_ind_dim1+len_dim1] = np.ones((len_dim0, len_dim1))\n",
    "        start_ind_dim0 +=len_dim0\n",
    "        start_ind_dim1 +=len_dim1\n",
    "    return result\n",
    "\n",
    "def create_packs(x_lens, y_lens):\n",
    "    x_packs = ones_block_diag(x_lens, x_lens)\n",
    "    y_packs = ones_block_diag(y_lens, y_lens)\n",
    "    yx_packs = ones_block_diag(y_lens, x_lens)\n",
    "    return x_packs, y_packs, yx_packs\n",
    "    \n",
    "def convert_batch_item(x, y, seq_len, x_lens=None, y_lens=None):\n",
    "    x = np.array(pad_and_trunc(x, seq_len))\n",
    "    y = np.array(pad_and_trunc(y, seq_len))\n",
    "\n",
    "    # Account for packing\n",
    "    if x_lens is not None:\n",
    "        assert y_lens is not None\n",
    "        x_packs, y_packs, yx_packs = create_packs(x_lens, y_lens)\n",
    "    else:\n",
    "        x_packs, y_packs, yx_packs = None, None, None\n",
    "        \n",
    "    x_mask, y_mask, yx_mask = build_masks(x,y, x_packs, y_packs, yx_packs)\n",
    "    return (x, y, x_mask, y_mask, yx_mask)\n",
    "                \n",
    "def get_batched_examples(ds, batch_size, seq_len, split=\"train\", skip_n_rows=None):\n",
    "    ds_split = ds[split].skip(skip_n_rows) if skip_n_rows is not None else ds[split]\n",
    "\n",
    "    batch = []\n",
    "    for item in ds_split:\n",
    "        batch.append(convert_batch_item(item['x'], item['y'], seq_len))\n",
    "        if len(batch) == batch_size:\n",
    "            yield pack_batch(batch)\n",
    "            batch = []\n",
    "    if split!=\"train\" and len(batch) > 0: # Note I don't use last few rows left in train split..\n",
    "        yield pack_batch(complete_last_batch(batch, batch_size))\n",
    "\n",
    "ds, _ = load_tokenized_dataset()\n",
    "\n",
    "x, y, x_mask, y_mask, yx_mask = next(get_batched_examples(ds, 2, 10))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ac4afff-bf6b-449d-8ab4-bfdd446142df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Loading tokenizer bpe_tokenizer_ds_train_all_merges_35k.pickle\n",
      "Tokenizing dataset\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,10) (5,5) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m START_TOK \u001b[38;5;241m=\u001b[39m tokenizer_vocab_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     30\u001b[0m END_TOK \u001b[38;5;241m=\u001b[39m tokenizer_vocab_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 32\u001b[0m x, y, x_mask, y_mask, yx_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mget_batched_examples_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTART_TOK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEND_TOK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpack_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36mget_batched_examples_packed\u001b[0;34m(ds, batch_size, seq_len, start_tok, end_tok, pack_frac, split, skip_n_rows)\u001b[0m\n\u001b[1;32m     17\u001b[0m     batch_y_lens\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;28mlen\u001b[39m(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m])])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m batch_size:\n\u001b[0;32m---> 20\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [convert_batch_item(\u001b[38;5;241m*\u001b[39mit, seq_len, x_lens, y_lens) \u001b[38;5;28;01mfor\u001b[39;00m it, x_lens, y_lens \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch, batch_x_lens, batch_y_lens)]\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m pack_batch(batch)\n\u001b[1;32m     23\u001b[0m     batch \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m     batch_y_lens\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;28mlen\u001b[39m(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m])])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m batch_size:\n\u001b[0;32m---> 20\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [\u001b[43mconvert_batch_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_lens\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m it, x_lens, y_lens \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch, batch_x_lens, batch_y_lens)]\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m pack_batch(batch)\n\u001b[1;32m     23\u001b[0m     batch \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[17], line 66\u001b[0m, in \u001b[0;36mconvert_batch_item\u001b[0;34m(x, y, seq_len, x_lens, y_lens)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     x_packs, y_packs, yx_packs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m x_mask, y_mask, yx_mask \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_packs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_packs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myx_packs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (x, y, x_mask, y_mask, yx_mask)\n",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m, in \u001b[0;36mbuild_masks\u001b[0;34m(x, y, x_packs, y_packs, yx_packs)\u001b[0m\n\u001b[1;32m      8\u001b[0m x_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply(np\u001b[38;5;241m.\u001b[39mmultiply(x_mask, x_pad_mask), x_pad_mask[:, \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_packs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     x_mask \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_packs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Mask padded y tokens + add \"autoregressive\" mask\u001b[39;00m\n\u001b[1;32m     13\u001b[0m y_pad_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m, np\u001b[38;5;241m.\u001b[39mones((y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])), \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,10) (5,5) "
     ]
    }
   ],
   "source": [
    "def get_batched_examples_packed(ds, batch_size, seq_len, start_tok, end_tok, pack_frac = 0.5, split=\"train\", skip_n_rows=None):\n",
    "    assert split==\"train\"\n",
    "    ds_split = ds[split].skip(skip_n_rows) if skip_n_rows is not None else ds[split]\n",
    "    \n",
    "    batch = []\n",
    "    batch_x_lens = []\n",
    "    batch_y_lens = []\n",
    "    for item in ds_split:\n",
    "        # Either append to previous batch item or create new one\n",
    "        if len(batch)>0 and (len(batch[-1][0]) < seq_len * pack_frac and len(batch[-1][1]) < seq_len * pack_frac):\n",
    "            pack_func = lambda x, y: x + [end_tok, start_tok] + y\n",
    "            batch[-1] = (pack_func(batch[-1][0], item['x']), pack_func(batch[-1][1], item['y']))\n",
    "            #batch_x_lens[-1].append() # TODO\n",
    "        else:\n",
    "            batch.append((item['x'],item['y']))\n",
    "            batch_x_lens.append([len(item['x'])])\n",
    "            batch_y_lens.append([len(item['y'])])\n",
    "            \n",
    "        if len(batch) == batch_size:\n",
    "            batch = [convert_batch_item(*it, seq_len, x_lens, y_lens) for it, x_lens, y_lens in zip(batch, batch_x_lens, batch_y_lens)]\n",
    "            \n",
    "            yield pack_batch(batch)\n",
    "            batch = []\n",
    "            \n",
    "    if split!=\"train\" and len(batch) > 0: # Note I don't use last few rows left in train split..\n",
    "        yield pack_batch(complete_last_batch(batch, batch_size))\n",
    "\n",
    "ds, (_, _, tokenizer_vocab_size) = load_tokenized_dataset()\n",
    "START_TOK = tokenizer_vocab_size + 1\n",
    "END_TOK = tokenizer_vocab_size + 2\n",
    "\n",
    "x, y, x_mask, y_mask, yx_mask = next(get_batched_examples_packed(ds, 2, 10, START_TOK, END_TOK, pack_frac=0.75))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc6b09ee-c1ac-4592-afda-1689b6641c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT\n",
      "[1 2 3 5 6 7 0]\n",
      "[11 12 13 15 16  0  0]\n",
      "\n",
      "PACKS\n",
      "[[1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1.]]\n",
      "[[1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "MASKS\n",
      "[[1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 5, 6, 7, 0])\n",
    "y = np.array([11, 12, 13, 15, 16, 0, 0])\n",
    "print(\"INPUT\")\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "print(\"\\nPACKS\")\n",
    "x_packs, y_packs, yx_packs = create_packs([3, 3, 1], [3, 2, 2])\n",
    "print(x_packs)\n",
    "print(y_packs)\n",
    "print(yx_packs)\n",
    "\n",
    "print(\"\\nMASKS\")\n",
    "x_mask, y_mask, yx_mask = build_masks(x, y, x_packs=x_packs, y_packs=y_packs, yx_packs = yx_packs)\n",
    "print(x_mask)\n",
    "print(y_mask)\n",
    "print(yx_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d39f06-6d86-48a2-adf3-2cd79b5fb387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [3]]\n",
      "(7, 1)\n",
      "(1, 2)\n",
      "\n",
      "POS_ARRAY\n",
      "(7, 2)\n",
      "[[0.   0.  ]\n",
      " [1.   0.01]\n",
      " [2.   0.02]\n",
      " [0.   0.  ]\n",
      " [1.   0.01]\n",
      " [2.   0.02]\n",
      " [3.   0.03]]\n",
      "\n",
      "STACKED\n",
      "(7, 2, 2)\n",
      "[[[ 0.          1.        ]\n",
      "  [ 0.          1.        ]]\n",
      "\n",
      " [[ 0.841471    0.5403023 ]\n",
      "  [ 0.00999983  0.99995   ]]\n",
      "\n",
      " [[ 0.90929747 -0.4161468 ]\n",
      "  [ 0.01999867  0.9998    ]]\n",
      "\n",
      " [[ 0.          1.        ]\n",
      "  [ 0.          1.        ]]\n",
      "\n",
      " [[ 0.841471    0.5403023 ]\n",
      "  [ 0.00999983  0.99995   ]]\n",
      "\n",
      " [[ 0.90929747 -0.4161468 ]\n",
      "  [ 0.01999867  0.9998    ]]\n",
      "\n",
      " [[ 0.14112    -0.9899925 ]\n",
      "  [ 0.0299955   0.99955004]]]\n",
      "[[ 0.          1.          0.          1.        ]\n",
      " [ 0.841471    0.5403023   0.00999983  0.99995   ]\n",
      " [ 0.90929747 -0.4161468   0.01999867  0.9998    ]\n",
      " [ 0.          1.          0.          1.        ]\n",
      " [ 0.841471    0.5403023   0.00999983  0.99995   ]\n",
      " [ 0.90929747 -0.4161468   0.01999867  0.9998    ]\n",
      " [ 0.14112    -0.9899925   0.0299955   0.99955004]]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "def pos_encodings(x, indices): # input: seq_len x emb_dim\n",
    "    seq_len, emb_dim = x.shape\n",
    "\n",
    "    #indices = jnp.arange(seq_len)[:, None]\n",
    "    print(indices.shape)\n",
    "    div_term = jnp.fromfunction(lambda i: 1 / pow(10000, 2 * i/emb_dim), (int(emb_dim/2),), dtype=float)[None, :]\n",
    "    print(div_term.shape)\n",
    "\n",
    "    print(f'\\nPOS_ARRAY')\n",
    "    pos_array = jnp.dot(indices, div_term)\n",
    "    print(pos_array.shape)\n",
    "    print(pos_array)\n",
    "\n",
    "    print(f'\\nSTACKED')\n",
    "    stacked = jnp.stack((jnp.sin(pos_array), jnp.cos(pos_array)), axis=2)\n",
    "    print(stacked.shape)\n",
    "    print(stacked)\n",
    "    return stacked.reshape(seq_len, emb_dim)\n",
    "\n",
    "x =random.uniform(random.PRNGKey(0), (7, 4)) # 7 seq_len, 4 emb_dim\n",
    "#x_indices = jnp.arange(x.shape[0])[:, None]\n",
    "# TODO: indices need to be inferred from mask OR \n",
    "# should we get lens from data collator instead,\n",
    "# (creating indices from lens is relatively straightforward)\n",
    "# However, lens have variable size, which doesn't make it perfect\n",
    "# for JAX\n",
    "x_indices = jnp.array([0, 1, 2, 0, 1, 2, 3])[:, None] \n",
    "print(x_indices)\n",
    "print(pos_encodings(x, x_indices))\n",
    "\n",
    "#y =random.uniform(random.PRNGKey(0), (7, 4)) # 7 seq_len, 4 emb_dim\n",
    "#print(pos_encodings(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d92bb7c-946f-46a4-b087-6f348fa6dcd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
