{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa347269-333d-45f1-8165-cc15f76c2adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Loading tokenizer bpe_tokenizer_ds_train_all_merges_35k.pickle\n",
      "Tokenizing dataset\n",
      "[[23954 12114  3613  3586 12466     0     0     0     0     0]\n",
      " [ 3667 21876 28867  3586 12466  3613  3586  3960  4795  3751]]\n",
      "[[31352  3606 29833     0     0     0     0     0     0     0]\n",
      " [ 3638 37931  3610  3979 36357  3787 15348  9617 19088 27915]]\n"
     ]
    }
   ],
   "source": [
    "from tokenized_dataset import load_tokenized_dataset\n",
    "import numpy as np\n",
    "\n",
    "def build_masks(x, y): # x: seq_len, y: seq_len\n",
    "    # Mask padded x tokens\n",
    "    x_mask = np.ones((x.shape[0], x.shape[0]))\n",
    "    x_pad_mask = np.where(x != 0, np.ones((x.shape[0])), 0)\n",
    "    x_mask = np.multiply(np.multiply(x_mask, x_pad_mask), x_pad_mask[:, None])\n",
    "\n",
    "    # Mask padded y tokens + add \"autoregressive\" mask\n",
    "    y_pad_mask = np.where(y != 0, np.ones((y.shape[0])), 0)\n",
    "    y_mask = np.tri(y.shape[0], y.shape[0])\n",
    "    y_mask = np.multiply(np.multiply(y_mask, y_pad_mask), y_pad_mask[:, None])\n",
    "\n",
    "    # Mask padded yx tokens\n",
    "    yx_mask = np.ones((y.shape[0], x.shape[0]))\n",
    "    yx_mask = np.multiply(np.multiply(yx_mask, x_pad_mask), y_pad_mask[:, None])\n",
    "\n",
    "    return x_mask, y_mask, yx_mask\n",
    "\n",
    "pad_and_trunc = lambda toks, seq_len: (toks + [0] * (seq_len - len(toks)))[:seq_len]\n",
    "pack_batch = lambda batch: [np.stack(el) for el in map(list, zip(*batch))]\n",
    "\n",
    "# Complete batch to batch_size with pad tokens only sequences \n",
    "def complete_last_batch(batch, batch_size):\n",
    "    for _ in range(batch_size - len(batch)):\n",
    "        x = np.zeros(seq_len)\n",
    "        y = np.zeros(seq_len)\n",
    "        x_mask, y_mask, yx_mask = build_masks(x,y)\n",
    "        batch.append((x, y, x_mask, y_mask, yx_mask))\n",
    "    return batch\n",
    "    \n",
    "\n",
    "def get_batched_examples(ds, batch_size, seq_len, split=\"train\", skip_n_rows=None):\n",
    "    ds_split = ds[split].skip(skip_n_rows) if skip_n_rows is not None else ds[split]\n",
    "\n",
    "    batch = []\n",
    "    for item in ds_split:\n",
    "        x = np.array(pad_and_trunc(item['x'], seq_len))\n",
    "        y = np.array(pad_and_trunc(item['y'], seq_len))\n",
    "        x_mask, y_mask, yx_mask = build_masks(x,y)\n",
    "        batch.append((x, y, x_mask, y_mask, yx_mask))\n",
    "        if len(batch) == batch_size:\n",
    "            yield pack_batch(batch)\n",
    "            batch = []\n",
    "    if split!=\"train\" and len(batch) > 0: # Note I don't use last few rows left in train split..\n",
    "        yield pack_batch(complete_last_batch(batch, batch_size))\n",
    "\n",
    "ds, _ = load_tokenized_dataset()\n",
    "\n",
    "x, y, x_mask, y_mask, yx_mask = next(get_batched_examples(ds, 2, 10))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac4afff-bf6b-449d-8ab4-bfdd446142df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Loading tokenizer bpe_tokenizer_ds_train_all_merges_35k.pickle\n",
      "Tokenizing dataset\n",
      "10\n",
      "10\n",
      "[[23954 12114  3613  3586 12466 38560 38559  3667 21876 28867]\n",
      " [ 3585 15064  3626  3829  3850  3827 31442  3586  4489 11945]\n",
      " [ 3829  3827 15275  3612  6044  3599  3745  7010  3594  3586]\n",
      " [ 3594  3586 32712  3667  4156  4239  3617 20984  3612  6958]\n",
      " [ 5895  4535  3788 11576  3687  3745  6958  3655  4435  3571]\n",
      " [ 9339  5544 19234  3614 23218  3612  6958  3655  4435  3571]\n",
      " [ 6698  4468  3599  3612  5318  3613 16215 38560 38559  3829]\n",
      " [ 3943  3613  3586  4641 31869  4017  4043  9155  3594 22023]\n",
      " [ 4081  3632  3743  7996  3687  8240  6698  4468  3617 12907]\n",
      " [14445  4148  4031 11341  3667  7401  3642  6866  3613  3586]]\n",
      "[[31352  3606 29833 38560 38559  3638 37931  3610  3979 36357]\n",
      " [ 3869  3725 26275 24456  3732  3606  3619 23742  7960    35]\n",
      " [ 3755  4794  6502  3606 10816  4052  3884  7320  3755 16943]\n",
      " [ 4910  4547  3638  3725 17257  3762  3700  3732  3837  3606]\n",
      " [ 3638  5897  7644  3835  3683  3884  6005 34029  9844  3683]\n",
      " [16600  4794 35212  3835  3683  3884  6005 34029  6958  3655]\n",
      " [ 4888  8262  3992  6465 21881 38560 38559  3869  3725  8473]\n",
      " [ 3683  3644  6932  3629  3609 12649 37717  3610  3605  3594]\n",
      " [ 6639  3605  9675  3645  4057  7644  4888  8262  3606 21905]\n",
      " [10158  4270  4031 11341  3638 10220  4213  3710 14523  6866]]\n"
     ]
    }
   ],
   "source": [
    "from tokenized_dataset import load_tokenized_dataset\n",
    "import numpy as np\n",
    "\n",
    "def get_batched_examples_packed(ds, batch_size, seq_len, start_tok, end_tok, pack_frac = 0.5, split=\"train\", skip_n_rows=None):\n",
    "    assert split==\"train\"\n",
    "    ds_split = ds[split].skip(skip_n_rows) if skip_n_rows is not None else ds[split]\n",
    "    \n",
    "    batch = []\n",
    "    for item in ds_split:\n",
    "        # Either append to previous batch item or create new one\n",
    "        if len(batch)>0 and (len(batch[-1][0]) < seq_len * pack_frac and len(batch[-1][1]) < seq_len * pack_frac):\n",
    "            pack_func = lambda x, y: x + [end_tok, start_tok] + y\n",
    "            batch[-1] = (pack_func(batch[-1][0], item['x']), pack_func(batch[-1][1], item['y']))\n",
    "        else:\n",
    "            batch.append((item['x'],item['y']))\n",
    "            \n",
    "        if len(batch) == batch_size:\n",
    "            def convert_batch_item(x, y):\n",
    "                x = np.array(pad_and_trunc(x, seq_len))\n",
    "                y = np.array(pad_and_trunc(y, seq_len))\n",
    "                x_mask, y_mask, yx_mask = build_masks(x,y)\n",
    "                return (x, y, x_mask, y_mask, yx_mask)\n",
    "            batch = [convert_batch_item(*it) for it in batch]\n",
    "            print(len(batch[0][0]))\n",
    "            print(len(batch[1][0]))\n",
    "            \n",
    "            yield pack_batch(batch)\n",
    "            batch = []\n",
    "            \n",
    "    if split!=\"train\" and len(batch) > 0: # Note I don't use last few rows left in train split..\n",
    "        yield pack_batch(complete_last_batch(batch, batch_size))\n",
    "\n",
    "ds, (_, _, tokenizer_vocab_size) = load_tokenized_dataset()\n",
    "START_TOK = tokenizer_vocab_size + 1\n",
    "END_TOK = tokenizer_vocab_size + 2\n",
    "\n",
    "x, y, x_mask, y_mask, yx_mask = next(get_batched_examples_packed(ds, 10, 10, START_TOK, END_TOK, pack_frac=0.75))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc6b09ee-c1ac-4592-afda-1689b6641c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT\n",
      "[1 2 3 5 6 7 0]\n",
      "[11 12 13 15 16  0  0]\n",
      "\n",
      "PACKS\n",
      "[[1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "[[1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1.]]\n",
      "[[1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "MASKS\n",
      "[[1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def build_masks(x, y, x_packs=None, y_packs=None, yx_packs=None): # x: seq_len, y: seq_len\n",
    "    # Mask padded x tokens\n",
    "    x_mask = np.ones((x.shape[0], x.shape[0]))\n",
    "    x_pad_mask = np.where(x != 0, np.ones((x.shape[0])), 0)\n",
    "    x_mask = np.multiply(np.multiply(x_mask, x_pad_mask), x_pad_mask[:, None])\n",
    "    if x_packs is not None:\n",
    "        x_mask = np.multiply(x_mask, x_packs)\n",
    "\n",
    "    # Mask padded y tokens + add \"autoregressive\" mask\n",
    "    y_pad_mask = np.where(y != 0, np.ones((y.shape[0])), 0)\n",
    "    y_mask = np.tri(y.shape[0], y.shape[0])\n",
    "    y_mask = np.multiply(np.multiply(y_mask, y_pad_mask), y_pad_mask[:, None])\n",
    "    if y_packs is not None:\n",
    "        y_mask = np.multiply(y_mask, y_packs)\n",
    "\n",
    "    # Mask padded yx tokens\n",
    "    yx_mask = np.ones((y.shape[0], x.shape[0]))\n",
    "    yx_mask = np.multiply(np.multiply(yx_mask, x_pad_mask), y_pad_mask[:, None])\n",
    "    if yx_packs is not None:\n",
    "        yx_mask = np.multiply(yx_mask, yx_packs)\n",
    "\n",
    "    return x_mask, y_mask, yx_mask\n",
    "\n",
    "def ones_block_diag(lens_dim0, lens_dim1):\n",
    "    result = np.zeros((sum(lens_dim0), sum(lens_dim1)))\n",
    "    start_ind_dim0 = 0\n",
    "    start_ind_dim1 = 0\n",
    "    for len_dim0, len_dim1 in zip(lens_dim0, lens_dim1):\n",
    "        result[start_ind_dim0:start_ind_dim0+len_dim0, start_ind_dim1:start_ind_dim1+len_dim1] = np.ones((len_dim0, len_dim1))\n",
    "        start_ind_dim0 +=len_dim0\n",
    "        start_ind_dim1 +=len_dim1\n",
    "    return result\n",
    "\n",
    "def create_packs(x_lens, y_lens):\n",
    "    x_packs = ones_block_diag(x_lens, x_lens)\n",
    "    y_packs = ones_block_diag(y_lens, y_lens)\n",
    "    yx_packs = ones_block_diag(y_lens, x_lens)\n",
    "    return x_packs, y_packs, yx_packs\n",
    "\n",
    "\n",
    "x = np.array([1, 2, 3, 5, 6, 7, 0])\n",
    "y = np.array([11, 12, 13, 15, 16, 0, 0])\n",
    "print(\"INPUT\")\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "print(\"\\nPACKS\")\n",
    "x_packs, y_packs, yx_packs = create_packs([3, 3, 1], [3, 2, 2])\n",
    "print(x_packs)\n",
    "print(y_packs)\n",
    "print(yx_packs)\n",
    "\n",
    "print(\"\\nMASKS\")\n",
    "x_mask, y_mask, yx_mask = build_masks(x, y, x_packs=x_packs, y_packs=y_packs, yx_packs = yx_packs)\n",
    "print(x_mask)\n",
    "print(y_mask)\n",
    "print(yx_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63d39f06-6d86-48a2-adf3-2cd79b5fb387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [3]]\n",
      "(7, 1)\n",
      "(1, 2)\n",
      "\n",
      "POS_ARRAY\n",
      "(7, 2)\n",
      "[[0.   0.  ]\n",
      " [1.   0.01]\n",
      " [2.   0.02]\n",
      " [0.   0.  ]\n",
      " [1.   0.01]\n",
      " [2.   0.02]\n",
      " [3.   0.03]]\n",
      "\n",
      "STACKED\n",
      "(7, 2, 2)\n",
      "[[[ 0.          1.        ]\n",
      "  [ 0.          1.        ]]\n",
      "\n",
      " [[ 0.841471    0.5403023 ]\n",
      "  [ 0.00999983  0.99995   ]]\n",
      "\n",
      " [[ 0.90929747 -0.4161468 ]\n",
      "  [ 0.01999867  0.9998    ]]\n",
      "\n",
      " [[ 0.          1.        ]\n",
      "  [ 0.          1.        ]]\n",
      "\n",
      " [[ 0.841471    0.5403023 ]\n",
      "  [ 0.00999983  0.99995   ]]\n",
      "\n",
      " [[ 0.90929747 -0.4161468 ]\n",
      "  [ 0.01999867  0.9998    ]]\n",
      "\n",
      " [[ 0.14112    -0.9899925 ]\n",
      "  [ 0.0299955   0.99955004]]]\n",
      "[[ 0.          1.          0.          1.        ]\n",
      " [ 0.841471    0.5403023   0.00999983  0.99995   ]\n",
      " [ 0.90929747 -0.4161468   0.01999867  0.9998    ]\n",
      " [ 0.          1.          0.          1.        ]\n",
      " [ 0.841471    0.5403023   0.00999983  0.99995   ]\n",
      " [ 0.90929747 -0.4161468   0.01999867  0.9998    ]\n",
      " [ 0.14112    -0.9899925   0.0299955   0.99955004]]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "def pos_encodings(x, indices): # input: seq_len x emb_dim\n",
    "    seq_len, emb_dim = x.shape\n",
    "\n",
    "    #indices = jnp.arange(seq_len)[:, None]\n",
    "    print(indices.shape)\n",
    "    div_term = jnp.fromfunction(lambda i: 1 / pow(10000, 2 * i/emb_dim), (int(emb_dim/2),), dtype=float)[None, :]\n",
    "    print(div_term.shape)\n",
    "\n",
    "    print(f'\\nPOS_ARRAY')\n",
    "    pos_array = jnp.dot(indices, div_term)\n",
    "    print(pos_array.shape)\n",
    "    print(pos_array)\n",
    "\n",
    "    print(f'\\nSTACKED')\n",
    "    stacked = jnp.stack((jnp.sin(pos_array), jnp.cos(pos_array)), axis=2)\n",
    "    print(stacked.shape)\n",
    "    print(stacked)\n",
    "    return stacked.reshape(seq_len, emb_dim)\n",
    "\n",
    "x =random.uniform(random.PRNGKey(0), (7, 4)) # 7 seq_len, 4 emb_dim\n",
    "#x_indices = jnp.arange(x.shape[0])[:, None]\n",
    "# TODO: indices need to be inferred from mask OR \n",
    "# should we get lens from data collator instead,\n",
    "# (creating indices from lens is relatively straightforward)\n",
    "# However, lens have variable size, which doesn't make it perfect\n",
    "# for JAX\n",
    "x_indices = jnp.array([0, 1, 2, 0, 1, 2, 3])[:, None] \n",
    "print(x_indices)\n",
    "print(pos_encodings(x, x_indices))\n",
    "\n",
    "#y =random.uniform(random.PRNGKey(0), (7, 4)) # 7 seq_len, 4 emb_dim\n",
    "#print(pos_encodings(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42e7f19-7def-4516-bf31-b46a9e1c2a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278277b3-1bd6-431a-8a7b-0ae54e88d857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
